{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "41e3e0ca",
   "metadata": {},
   "source": [
    "# 1. Introuction\n",
    "\n",
    " This notebook outlines the creation, compilation, and training of a Swin Tranformer network to classify 101 types of food. To this end, the **distillation technique** is applied to learn from a larger, pre-trained transformer model, especifically, a ViT-Base/16-384 transformer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6555499",
   "metadata": {},
   "source": [
    "# 2. Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ea443bb-470a-47e5-8f4d-341abf4e4d6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.backends.cudnn as cudnn\n",
    "\n",
    "from torchvision.transforms import v2\n",
    "from torchinfo import summary\n",
    "from pathlib import Path\n",
    "from torchvision import datasets\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Import custom libraries\n",
    "from utils.classification_utils import set_seeds, display_random_images\n",
    "from engines.classification import DistillationEngine, Common\n",
    "from engines.schedulers import FixedLRSchedulerWrapper\n",
    "from dataloaders.image_dataloaders import create_dataloaders\n",
    "from engines.loss_functions import DistillationLoss\n",
    "\n",
    "# Dataset\n",
    "from datasets import load_dataset\n",
    "\n",
    "import warnings\n",
    "os.environ['TORCH_USE_CUDA_DSA'] = \"1\"\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"torch.autograd.graph\")\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning, module=\"onnxscript.converter\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8403c078",
   "metadata": {},
   "source": [
    "# 3. Importing Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94336973",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define some constants\n",
    "NUM_WORKERS = os.cpu_count()\n",
    "AMOUNT_TO_GET = 1.0\n",
    "SEED = 42\n",
    "\n",
    "# Define target data directory\n",
    "TARGET_DIR_NAME = f\"data/food-101_{str(int(AMOUNT_TO_GET*100))}_percent\"\n",
    "\n",
    "# Setup training and test directories\n",
    "TARGET_DIR = Path(TARGET_DIR_NAME)\n",
    "TRAIN_DIR = TARGET_DIR / \"train\"\n",
    "TEST_DIR = TARGET_DIR / \"test\"\n",
    "TARGET_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Create target model directory\n",
    "MODEL_DIR = Path(\"outputs\")\n",
    "\n",
    "# Set seeds\n",
    "set_seeds(SEED)\n",
    "\n",
    "IMPORT_DATASET = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b18615cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "if IMPORT_DATASET:\n",
    "    # Download dataset from Hugging Face\n",
    "    ds = load_dataset(\"ethz/food101\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cc78c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "if IMPORT_DATASET:\n",
    "    # Get class names\n",
    "    class_names = ds[\"train\"].features[\"label\"].names\n",
    "\n",
    "    # Function to save images into appropriate directories\n",
    "    def save_images(split, target_dir):\n",
    "        for example in tqdm(ds[split], desc=f\"Saving {split} images\"):\n",
    "            image = example[\"image\"]\n",
    "            label = example[\"label\"]\n",
    "            class_name = class_names[label]\n",
    "\n",
    "            # Define class directory\n",
    "            class_dir = target_dir / class_name\n",
    "            class_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "            # Save image\n",
    "            img_path = class_dir / f\"{len(list(class_dir.iterdir()))}.jpg\"\n",
    "            image.save(img_path)\n",
    "\n",
    "    # Save training and test images\n",
    "    save_images(\"train\", TRAIN_DIR)\n",
    "    save_images(\"validation\", TEST_DIR)\n",
    "\n",
    "    print(\"Dataset has been saved successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c279df5f",
   "metadata": {},
   "source": [
    "# 3. Specifying Target Device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f01afed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activate cuda benchmark\n",
    "cudnn.benchmark = True\n",
    "\n",
    "# Set device\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Device: {device}\")\n",
    "\n",
    "if device == \"cuda\":\n",
    "    !nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "350701ec-c5f9-4809-884c-69a5dcf97ceb",
   "metadata": {},
   "source": [
    "# 4. Image Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d670c088",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display images\n",
    "manual_transforms = v2.Compose([\n",
    "    v2.Resize((256)),\n",
    "    v2.RandomCrop((256, 256)),    \n",
    "    v2.ToImage(),\n",
    "    v2.ToDtype(torch.float32, scale=True)\n",
    "])\n",
    "\n",
    "train_data = datasets.ImageFolder(TRAIN_DIR, transform=manual_transforms)\n",
    "display_random_images(train_data,\n",
    "                      n=25,\n",
    "                      classes=train_data.classes,\n",
    "                      rows=5,\n",
    "                      cols=5,\n",
    "                      display_shape=False,\n",
    "                      seed=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "815c28c3",
   "metadata": {},
   "source": [
    "# 5. Create Teacher - 101 Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc544d75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify transformations\n",
    "IMG_SIZE_TCH = 384\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "transform_train_tch = v2.Compose([    \n",
    "    v2.TrivialAugmentWide(),\n",
    "    v2.Resize((IMG_SIZE_TCH)),\n",
    "    v2.CenterCrop((IMG_SIZE_TCH, IMG_SIZE_TCH)),    \n",
    "    v2.ToImage(),\n",
    "    v2.ToDtype(torch.float32, scale=True),\n",
    "    v2.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                std=[0.229, 0.224, 0.225]) \n",
    "])\n",
    "\n",
    "transform_test_tch = v2.Compose([    \n",
    "    v2.Resize((IMG_SIZE_TCH)),\n",
    "    v2.CenterCrop((IMG_SIZE_TCH, IMG_SIZE_TCH)),    \n",
    "    v2.ToImage(),\n",
    "    v2.ToDtype(torch.float32, scale=True),\n",
    "    v2.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                std=[0.229, 0.224, 0.225]) \n",
    "])\n",
    "\n",
    "# Create data loaders\n",
    "train_dataloader_tch, test_dataloader_tch, class_names = create_dataloaders(\n",
    "    train_dir=TRAIN_DIR,\n",
    "    test_dir=TEST_DIR,\n",
    "    train_transform=transform_train_tch,\n",
    "    test_transform=transform_test_tch,\n",
    "    batch_size=BATCH_SIZE\n",
    ")\n",
    "\n",
    "# Load ViT-Base/16-384. Run classification_example.ipynb to genereate the teacher model. \n",
    "model_tch_type=\"teacher_model\"\n",
    "model_tch_name = model_tch_type + \".pth\"\n",
    "\n",
    "# Instantiate the model\n",
    "model_tch = torchvision.models.vit_b_16(image_size=IMG_SIZE_TCH).to(device)\n",
    "model_tch.heads = torch.nn.Linear(in_features=768, out_features=len(class_names)).to(device)\n",
    "model_tch = torch.compile(model_tch, backend=\"aot_eager\")\n",
    "\n",
    "# Load the trained weights\n",
    "model_tch = Common.load_model(\n",
    "    model=model_tch,\n",
    "    target_dir=MODEL_DIR,\n",
    "    model_name=model_tch_name)\n",
    "\n",
    "# Print summary\n",
    "#summary(model_tch,\n",
    "#        input_size=(BATCH_SIZE,3,IMG_SIZE_TCH, IMG_SIZE_TCH), # try swapping this for \"random_input_image_error\"\n",
    "#        col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
    "#        col_width=20,\n",
    "#        row_settings=[\"var_names\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77f72f2f",
   "metadata": {},
   "source": [
    "# 5. Create Student - 101 Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6eadcc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify transformations\n",
    "IMG_SIZE_STD = 256\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "transform_train_std = v2.Compose([    \n",
    "    v2.TrivialAugmentWide(),\n",
    "    v2.Resize((260)),\n",
    "    v2.RandomCrop((IMG_SIZE_STD, IMG_SIZE_STD)),    \n",
    "    v2.ToImage(),\n",
    "    v2.ToDtype(torch.float32, scale=True),\n",
    "    v2.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                std=[0.229, 0.224, 0.225]) \n",
    "])\n",
    "\n",
    "transform_test_std = v2.Compose([    \n",
    "    v2.Resize((260)),\n",
    "    v2.CenterCrop((IMG_SIZE_STD, IMG_SIZE_STD)),    \n",
    "    v2.ToImage(),\n",
    "    v2.ToDtype(torch.float32, scale=True),\n",
    "    v2.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                std=[0.229, 0.224, 0.225]) \n",
    "])\n",
    "\n",
    "# Create data loaders\n",
    "train_dataloader_std, test_dataloader_std, class_names = create_dataloaders(\n",
    "    train_dir=TRAIN_DIR,\n",
    "    test_dir=TEST_DIR,\n",
    "    train_transform=transform_train_std,\n",
    "    test_transform=transform_test_std,\n",
    "    batch_size=BATCH_SIZE\n",
    ")\n",
    "\n",
    "\n",
    "# Copy weights from torchvision.models\n",
    "set_seeds(SEED)\n",
    "\n",
    "# Instantiate the model\n",
    "model_std = torchvision.models.swin_v2_t(weights=torchvision.models.Swin_V2_T_Weights.DEFAULT)\n",
    "model_std.head = torch.nn.Linear(in_features=768, out_features=len(class_names)).to(device)\n",
    "\n",
    "# Unfreeze the base parameters\n",
    "for parameter in model_std.parameters():\n",
    "    parameter.requires_grad = True\n",
    "\n",
    "# Compile model\n",
    "model_std = torch.compile(model_std, backend=\"aot_eager\")\n",
    "\n",
    "# Print summary\n",
    "#summary(model_std,\n",
    "#        input_size=(BATCH_SIZE,3,IMG_SIZE_STD, IMG_SIZE_STD), # try swapping this for \"random_input_image_error\"\n",
    "#        col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
    "#        col_width=20,\n",
    "#        row_settings=[\"var_names\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa1218f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "model_std_type=\"student_model\"\n",
    "model_std_name = model_std_type + \".pth\"\n",
    "\n",
    "# Epochs and learning rate\n",
    "EPOCHS = 30\n",
    "LR = 0.0001\n",
    "\n",
    "# Create optimizer\n",
    "optimizer = torch.optim.AdamW(\n",
    "    params=model_std.parameters(),\n",
    "    lr=LR,\n",
    "    betas=(0.9, 0.999),\n",
    "    weight_decay=0.01\n",
    ")\n",
    "\n",
    "# Create loss function\n",
    "loss_fn = DistillationLoss(alpha=0.4, temperature=2, label_smoothing=0.1)\n",
    "\n",
    "# Initialize the scheduler\n",
    "#scheduler = CosineAnnealingLR(optimizer, T_max=EPOCHS, eta_min=eta_min)\n",
    "scheduler = FixedLRSchedulerWrapper(\n",
    "    scheduler=CosineAnnealingLR(optimizer, T_max=20, eta_min=1e-6),\n",
    "    fixed_lr=1e-6,\n",
    "    fixed_epoch=20)\n",
    "\n",
    "# Set seeds\n",
    "set_seeds(SEED)\n",
    "\n",
    "# And train...\n",
    "engine = DistillationEngine(\n",
    "    student=model_std,\n",
    "    teacher=model_tch,\n",
    "    device=device)\n",
    "\n",
    "# Configure the training method\n",
    "results = engine.train(\n",
    "    target_dir=MODEL_DIR,                       # Directory where the model will be saved\n",
    "    model_name=model_std_name,                  # Name of the student model\n",
    "    save_best_model=[\"last\", \"loss\", \"acc\"],    # Save the best models based on different criteria\n",
    "    keep_best_models_in_memory=False,           # Do not keep the models stored in memory for the sake of training time and memory efficiency\n",
    "    train_dataloader_std=train_dataloader_std,  # Train dataloader for the student\n",
    "    train_dataloader_tch=train_dataloader_tch,  # Train dataloader for the teacher\n",
    "    test_dataloader_std=test_dataloader_std,    # Test dataloader for the student\n",
    "    test_dataloader_tch=test_dataloader_tch,    # Test dataloader for the teacher\n",
    "    optimizer=optimizer,                        # Optimizer\n",
    "    loss_fn=loss_fn,                            # Loss function\n",
    "    recall_threshold=0.995,                     # False positive rate at recall_threshold recall\n",
    "    recall_threshold_pauc=0.95,                 # Partial AUC score above recall_threshold_pauc recall\n",
    "    scheduler=scheduler,                        # Scheduler\n",
    "    epochs=EPOCHS,                              # Total number of epochs\n",
    "    amp=True,                                   # Enable Automatic Mixed Precision (AMP)\n",
    "    enable_clipping=False,                      # Disable clipping on gradients, only useful if training becomes unestable\n",
    "    debug_mode=False,                           # Disable debug mode    \n",
    "    accumulation_steps=2,                       # Accumulation steps 2: effective batch size = batch_size x accumulation steps\n",
    "    apply_validation=True                       # Enable validation step\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62416085",
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv_pytorch_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
