{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook outlines the creation, compilation, and training of a deep learning network for object segmentation and detection. It is quite similar to the object_detection_default notebook, with the primary difference being how the R-CNN model is created. In this version, the model is more flexible, allowing for the use of different backbones, custom anchors, and alternative ROI pooling configurations.\n",
    "\n",
    "The notebook is fundamentally based on the PyTorch tutorial for object segmentation that is available [here](https://pytorch.org/tutorials/intermediate/torchvision_tutorial.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Links of Interest\n",
    "* https://medium.com/@soumyajitdatta123/faster-rcnns-explained-af76f96a0b70\n",
    "* https://medium.com/@RobuRishabh/understanding-and-implementing-faster-r-cnn-248f7b25ff96"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generic libraries\n",
    "import os\n",
    "import torch\n",
    "import zipfile\n",
    "import torch.backends.cudnn as cudnn\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from torchinfo import summary\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "\n",
    "# Torchvision libraries\n",
    "from torchvision import models\n",
    "from torchvision.ops import MultiScaleRoIAlign\n",
    "from torchvision import tv_tensors\n",
    "from torchvision.io import read_image\n",
    "from torchvision.transforms import v2 as T\n",
    "from torchvision.utils import draw_bounding_boxes\n",
    "from torchvision.models.detection.rpn import AnchorGenerator\n",
    "from torchvision.models.detection import FasterRCNN\n",
    "\n",
    "# Import custom libraries\n",
    "from utils.classification_utils import set_seeds\n",
    "from utils.obj_detection_utils import collate_fn, prune_predictions, display_and_save_predictions, visualize_transformed_data\n",
    "from engines.obj_detection import ObjectDetectionEngine\n",
    "from engines.schedulers import FixedLRSchedulerWrapper\n",
    "from dataloaders.obj_dect_dataloaders import ProcessDataset\n",
    "from models.faster_rcnn import StandardFasterRCNN, CustomFasterRCNN\n",
    "\n",
    "# Warnings\n",
    "import warnings\n",
    "os.environ['TORCH_USE_CUDA_DSA'] = \"1\"\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"torch.autograd.graph\")\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning, module=\"onnxscript.converter\")\n",
    "\n",
    "# Create target model directory\n",
    "MODEL_DIR = Path(\"outputs\")\n",
    "MODEL_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Set seeds\n",
    "set_seeds(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Specifying the Target Device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activate cuda benchmark\n",
    "#cudnn.benchmark = True\n",
    "\n",
    "# Set device\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Device: {device}\")\n",
    "\n",
    "if device == \"cuda\":\n",
    "    !nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Downloading the Penn-Fundan Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the dataset\n",
    "!curl -L -k https://www.cis.upenn.edu/~jshi/ped_html/PennFudanPed.zip -o data/PennFudanPed.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the downloaded zip file\n",
    "zip_file_path = \"data/PennFudanPed.zip\"\n",
    "extract_dir = \"data\"\n",
    "\n",
    "# Ensure the extraction directory exists\n",
    "os.makedirs(extract_dir, exist_ok=True)\n",
    "\n",
    "# Unzip the file\n",
    "with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
    "    zip_ref.extractall(extract_dir)\n",
    "\n",
    "print(f\"Files extracted to {extract_dir}\")\n",
    "\n",
    "zip_file = Path(zip_file_path)\n",
    "if zip_file.exists():\n",
    "    os.remove(zip_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Image and Mask Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "image = read_image(\"data/PennFudanPed/PNGImages/FudanPed00046.png\")\n",
    "mask = read_image(\"data/PennFudanPed/PedMasks/FudanPed00046_mask.png\")\n",
    "\n",
    "plt.figure(figsize=(16, 8))\n",
    "plt.subplot(121)\n",
    "plt.title(\"Image\")\n",
    "plt.imshow(image.permute(1, 2, 0))\n",
    "plt.subplot(122)\n",
    "plt.title(\"Mask\")\n",
    "plt.imshow(mask.permute(1, 2, 0))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Preparing Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Pre-processing transformations\n",
    "def get_transform(train, mean_std_norm):\n",
    "    transforms = []\n",
    "    if train:\n",
    "        transforms.append(T.RandomHorizontalFlip(p=0.5))\n",
    "        transforms.append(T.RandomVerticalFlip(p=0.5))\n",
    "        transforms.append(T.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2))\n",
    "        transforms.append(T.GaussianBlur(kernel_size=(3, 3), sigma=(0.1, 2.0)))\n",
    "        transforms.append(T.RandomPerspective(distortion_scale=0.2, p=0.5))\n",
    "        transforms.append(T.RandomGrayscale(p=0.1))\n",
    "        transforms.append(T.RandomZoomOut(fill={tv_tensors.Image: (0, 0, 0), \"others\": 0}, side_range=(1.0, 2.0), p=0.2)), #(123, 117, 104)\n",
    "    transforms.append(T.ToDtype(torch.float, scale=True))\n",
    "    if mean_std_norm:\n",
    "        transforms.append(T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]))\n",
    "    transforms.append(T.ToPureTensor())\n",
    "    return T.Compose(transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The dataset contains two classes only: background and person\n",
    "num_classes = 2\n",
    "BATCHES = 2\n",
    "\n",
    "# Use ther dataset and defined transformations\n",
    "dataset_tr = ProcessDataset(\n",
    "    root='data/PennFudanPed',\n",
    "    image_path=\"PNGImages\",\n",
    "    mask_path=\"PedMasks\",\n",
    "    transforms=get_transform(train=True, mean_std_norm=False),\n",
    "    num_classes=num_classes-1) # exclude the background\n",
    "\n",
    "dataset_ntr = ProcessDataset(\n",
    "    root='data/PennFudanPed',\n",
    "    image_path=\"PNGImages\",\n",
    "    mask_path=\"PedMasks\",\n",
    "    transforms=get_transform(train=False, mean_std_norm=False),\n",
    "    num_classes=num_classes-1) # exclude the background\n",
    "\n",
    "# Split the dataset in train and test set\n",
    "indices = torch.randperm(len(dataset_tr)).tolist()\n",
    "train_dataset = torch.utils.data.Subset(dataset_tr, indices[:-25])\n",
    "test_dataset = torch.utils.data.Subset(dataset_ntr, indices[-25:])\n",
    "test_dataset_t = torch.utils.data.Subset(dataset_tr, indices[-25:])\n",
    "\n",
    "# Define training and validation data loaders\n",
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=BATCHES,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "test_dataloader = torch.utils.data.DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=BATCHES,\n",
    "    shuffle=False,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "# Define training and validation data loaders\n",
    "test_dataloader_t = torch.utils.data.DataLoader(\n",
    "    test_dataset_t,\n",
    "    batch_size=2,\n",
    "    shuffle=False,\n",
    "    collate_fn=collate_fn\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize transformations\n",
    "for idx, ((img, target), (img_t, target_t)) in enumerate(zip(test_dataloader, test_dataloader_t)):   \n",
    "    for i in range(0, BATCHES):\n",
    "        visualize_transformed_data(img[i], target[i], img_t[i], target_t[i])\n",
    "    if idx > 4:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. Creating a Custom Object Detection Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before implementing the R-CNN, let's define some key parameters that will help us better understand the role of each stage in the model:\n",
    "\n",
    "- **Backbone**: - **Backbone**: This is the neural network used to extract features from the input image. It serves as the feature extractor for the object detection model. It produces a set of feature maps, each associated with a different layer of the neural network.\n",
    "\n",
    "- **Anchor Generator**: This defines a set of predefined bounding box shapes that are used as initial references for detecting objects in the image. These anchors are essential for region proposal.\n",
    "- **ROI Pooler** This is the algorithm responsible for mapping each Region of Interest (RoI) onto a fixed-size feature map. It ensures that the features corresponding to each RoI are of a consistent size, regardless of the original dimensions of the RoI.\n",
    "    - **featmap_names**: Specifies the source of the feature maps used for extracting RoIs. Typically, this will be the last feature map produced by the backbone (specified as ['0']), which captures the most refined features.\n",
    "    - **output_size**: Determines the size of the output RoI features. A higher output size retains more spatial detail, while a smaller size may reduce computation but lose some fine details.\n",
    "    - **sampling_ratio**: It controls how many points are sampled from the feature map to create a fixed-size output for each RoI. A higher ratio means more precise sampling, but it also increases computational cost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the backbone: https://pytorch.org/vision/0.20/models.html\n",
    "backbone = models.efficientnet_b0(weights=models.EfficientNet_B0_Weights.DEFAULT)\n",
    "#backbone = models.convnext_small(weights=models.ConvNeXt_Small_Weights.DEFAULT),\n",
    "#backbone = models.efficientnet_v2_s(weights=models.EfficientNet_V2_S_Weights.DEFAULT)\n",
    "\n",
    "# Create the R-CNN model\n",
    "model = CustomFasterRCNN(\n",
    "    num_classes=num_classes,\n",
    "    backbone=backbone,\n",
    "    anchor_generator=AnchorGenerator(\n",
    "        sizes=((32, 64, 128, 256, 512),),\n",
    "        aspect_ratios=((0.5, 1.0, 2.0),)),\n",
    "    roi_pooler=MultiScaleRoIAlign(\n",
    "        featmap_names=['0'],\n",
    "        output_size=7,\n",
    "        sampling_ratio=2),\n",
    "    device=device\n",
    ")\n",
    "\n",
    "summary(model,\n",
    "        input_size=(1,3,224, 224),\n",
    "        col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
    "        col_width=20,\n",
    "        row_settings=[\"var_names\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10. Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Model name\n",
    "model_type=\"rcnn_effnet\"\n",
    "model_name = model_type + \".pth\"\n",
    "EPOCHS = 35\n",
    "LR = 1e-4\n",
    "ETAMIN = 1e-7\n",
    "\n",
    "# Create the optimizer\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "\n",
    "# Create AdamW optimizer\n",
    "optimizer = torch.optim.AdamW(\n",
    "    params=model.parameters(),\n",
    "    lr=LR,\n",
    "    betas=(0.9, 0.999),\n",
    "    weight_decay=0.01\n",
    ")\n",
    "\n",
    "scheduler = FixedLRSchedulerWrapper(\n",
    "    scheduler=CosineAnnealingLR(optimizer, T_max=EPOCHS-5, eta_min=ETAMIN),\n",
    "    fixed_lr=ETAMIN,\n",
    "    fixed_epoch=EPOCHS-5)\n",
    "\n",
    "# Instantiate the engine with the created model and the target device\n",
    "engine = ObjectDetectionEngine(\n",
    "    model=model,\n",
    "    #color_map={'train': 'green', 'test': 'red', 'other': 'black'},\n",
    "    device=device)\n",
    "\n",
    "# Configure the training method\n",
    "results = engine.train(\n",
    "    target_dir=MODEL_DIR,                       # Directory where the model will be saved\n",
    "    model_name=model_name,                      # Name of the model\n",
    "    save_best_model=[\"last\", \"loss\"],           # Save the best models based on different criteria\n",
    "    keep_best_models_in_memory=True,            # Do not keep the models stored in memory for the sake of training time and memory efficiency\n",
    "    train_dataloader=train_dataloader,          # Train dataloader\n",
    "    test_dataloader=test_dataloader,            # Test dataloader\n",
    "    optimizer=optimizer,                        # Optimizer    \n",
    "    scheduler=scheduler,                        # Scheduler\n",
    "    epochs=EPOCHS,                              # Total number of epochs\n",
    "    amp=True,                                   # Enable Automatic Mixed Precision (AMP)\n",
    "    enable_clipping=False,                      # Disable clipping on gradients, only useful if training becomes unestable\n",
    "    debug_mode=False,                           # Disable debug mode    \n",
    "    accumulation_steps=1,                       # Accumulation steps 2: effective batch size = batch_size x accumulation steps\n",
    "    apply_validation=True                       # Enable validation step\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 11. Making Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OPTION = 1\n",
    "\n",
    "# Make predictions using the `engine` object, best model is already internally stored\n",
    "if OPTION == 1: \n",
    "    # Make predictions and plot the results\n",
    "    preds = engine.predict(\n",
    "        dataloader=test_dataloader,\n",
    "        model_state='loss', # Take the model with the lowest loss\n",
    "        prune_predictions = True,\n",
    "        #score_threshold = 0.66,\n",
    "        #mask_threshold = 0.5,    \n",
    "        #iou_threshold = 0.5\n",
    "    )\n",
    "\n",
    "# Make predictions by loading the already trained model manually\n",
    "else:\n",
    "    # Instantiate the trained model\n",
    "    # First, load the architecture\n",
    "    model = CustomFasterRCNN(\n",
    "        num_classes=num_classes,\n",
    "        backbone=models.convnext_small(\n",
    "            weights=models.ConvNeXt_Small_Weights.DEFAULT),\n",
    "        anchor_generator=AnchorGenerator(\n",
    "            sizes=((32, 64, 128, 256, 512),),\n",
    "            aspect_ratios=((0.5, 1.0, 2.0),)),\n",
    "        roi_pooler=MultiScaleRoIAlign(\n",
    "            featmap_names=['0'],\n",
    "            output_size=7,\n",
    "            sampling_ratio=2),\n",
    "        device=device\n",
    "    )\n",
    "\n",
    "    # Second, load the parameters of the best model\n",
    "    #model = load_model(model, \"outputs\", \"model_loss_epoch21.pth\")\n",
    "\n",
    "    # Instantiate the engine with the created model and the target device\n",
    "    engine2 = ObjectDetectionEngine(\n",
    "        model=model,\n",
    "        device=device)\n",
    "\n",
    "    # Make predictions and plot the results\n",
    "    preds = engine2.predict(\n",
    "        dataloader=test_dataloader,\n",
    "        prune_predictions = True,\n",
    "        #score_threshold = 0.66,\n",
    "        #mask_threshold = 0.5,    \n",
    "        #iou_threshold = 0.5\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration parameters\n",
    "MASK_COLOR = \"blue\"\n",
    "BOX_COLOR = \"white\"\n",
    "WIDTH = 3\n",
    "PRINT_LABELS = True\n",
    "\n",
    "# Display predictions\n",
    "display_and_save_predictions(\n",
    "    preds=preds,\n",
    "    dataloader=test_dataset,\n",
    "    box_color=BOX_COLOR,\n",
    "    mask_color=MASK_COLOR,\n",
    "    width=WIDTH,\n",
    "    print_classes=True,\n",
    "    print_scores=True,\n",
    "    label_to_class_dict={1: 'pedestrian'}\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load an arbitrary image from a different dataset\n",
    "image = read_image(\"images/examples/000000000674.jpg\")\n",
    "\n",
    "# And make a prediction\n",
    "eval_transform = get_transform(train=False, mean_std_norm=False)\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    x = eval_transform(image)\n",
    "    # convert RGBA -> RGB and move to device\n",
    "    x = x[:3, ...].to(device)\n",
    "    predictions = model([x, ])\n",
    "    pred = prune_predictions(predictions[0])\n",
    "    \n",
    "\n",
    "# Prepare the image for plotting\n",
    "image = (255.0 * (image - image.min()) / (image.max() - image.min())).to(torch.uint8)\n",
    "image = image[:3, ...]\n",
    "pred_labels = [f\"roi: {score:.3f}\" for label, score in zip(pred[\"labels\"], pred[\"scores\"])]\n",
    "pred_boxes = pred[\"boxes\"].long()\n",
    "output_image = draw_bounding_boxes(image, pred_boxes, pred_labels, colors=\"white\", width=3)\n",
    "\n",
    "#masks = (pred[\"masks\"] > 0.7).squeeze(1)\n",
    "#output_image = draw_segmentation_masks(output_image, masks, alpha=0.5, colors=\"blue\")\n",
    "\n",
    "# Plot the image\n",
    "fig, ax = plt.subplots(figsize=(12, 12))\n",
    "plt.imshow(output_image.permute(1, 2, 0))\n",
    "ax.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv_pytorch_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
