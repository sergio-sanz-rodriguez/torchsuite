{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "41e3e0ca",
   "metadata": {},
   "source": [
    "# 1. Introduction\n",
    "\n",
    "This notebook outlines the creation, compilation, and training of a deep learing network for audio classification using the [TorchSuite](https://github.com/sergio-sanz-rodriguez/torchsuite) framework.\n",
    " \n",
    "https://pytorch.org/tutorials/intermediate/speech_command_classification_with_torchaudio_tutorial.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dba7507",
   "metadata": {},
   "source": [
    "# 2. Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6892a15",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install torcheval\n",
    "import os\n",
    "import shutil\n",
    "import torch\n",
    "import glob\n",
    "import random\n",
    "import torch.backends.cudnn as cudnn\n",
    "import IPython.display as ipd\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from torchaudio.datasets import SPEECHCOMMANDS\n",
    "\n",
    "# Import custom libraries\n",
    "from utils.classification_utils import predict_and_play_audio\n",
    "from utils.common_utils import set_seeds, load_model\n",
    "from engines.classification import ClassificationEngine\n",
    "from engines.schedulers import FixedLRSchedulerWrapper\n",
    "from dataloaders.audio_dataloaders import load_audio, create_dataloaders_spectrogram, AudioSpectrogramTransforms\n",
    "from models.pretrained_models import build_pretrained_model\n",
    "\n",
    "import warnings\n",
    "os.environ['TORCH_USE_CUDA_DSA'] = \"1\"\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"torch.autograd.graph\")\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning, module=\"onnxscript.converter\")\n",
    "\n",
    "# Paths (modify as needed)\n",
    "TARGET_DIR_NAME = Path(\"data/SpeechCommands/speech_commands_v0.02\")\n",
    "TRAIN_DIR = Path(\"data/SpeechCommands/train\")\n",
    "TEST_DIR = Path(\"data/SpeechCommands/test\")\n",
    "\n",
    "# Define some constants\n",
    "NUM_WORKERS = os.cpu_count()\n",
    "SEED = 42\n",
    "\n",
    "# Create target model directory\n",
    "MODEL_DIR = Path(\"outputs\")\n",
    "MODEL_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Set seeds\n",
    "set_seeds(SEED)\n",
    "\n",
    "IMPORT_DATASET = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f40496fd",
   "metadata": {},
   "source": [
    "# 3. Specifying the Target Device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e897ca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activate cuda benchmark\n",
    "cudnn.benchmark = True\n",
    "\n",
    "# Set device\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Device: {device}\")\n",
    "\n",
    "#if device == \"cuda\":\n",
    "#    !nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6555499",
   "metadata": {},
   "source": [
    "# 4. Importing Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "343aa83f",
   "metadata": {},
   "source": [
    "The dataset should be organized as follows, with one subdirectory per class containing the corresponding images:\n",
    "\n",
    "```\n",
    "dataset/\n",
    "├── train/\n",
    "│   └── <class_label>/\n",
    "│       ├── img1.jpg\n",
    "│       ├── img2.png\n",
    "│       └── ...\n",
    "└── test/ (or val/)/\n",
    "    └── <class_label>/\n",
    "        ├── img1.jpg\n",
    "        ├── img2.png\n",
    "        └── ...\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d94af740",
   "metadata": {},
   "outputs": [],
   "source": [
    "if IMPORT_DATASET:\n",
    "    # Download dataset\n",
    "    os.makedirs(\"data\", exist_ok=True)\n",
    "    dataset = SPEECHCOMMANDS(\n",
    "        root=\"./data\",\n",
    "        url=\"speech_commands_v0.02\",\n",
    "        folder_in_archive=\"SpeechCommands\",\n",
    "        download=True,\n",
    "        subset=None\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e69190a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if IMPORT_DATASET:\n",
    "    # Read validation and test lists\n",
    "    val_test_files = set()\n",
    "    for filename in [\"validation_list.txt\", \"testing_list.txt\"]:\n",
    "        with open(os.path.join(TARGET_DIR_NAME, filename), \"r\") as f:\n",
    "            val_test_files.update(f.read().splitlines())\n",
    "\n",
    "    # Ensure output directories exist\n",
    "    os.makedirs(TRAIN_DIR, exist_ok=True)\n",
    "    os.makedirs(TEST_DIR, exist_ok=True)\n",
    "\n",
    "    # Loop over all class folders\n",
    "    for class_name in os.listdir(TARGET_DIR_NAME):\n",
    "        class_path = os.path.join(TARGET_DIR_NAME, class_name)\n",
    "        if not os.path.isdir(class_path):  # Skip non-folder files\n",
    "            continue\n",
    "\n",
    "        # Create class folders in train/ and test/\n",
    "        os.makedirs(os.path.join(TRAIN_DIR, class_name), exist_ok=True)\n",
    "        os.makedirs(os.path.join(TEST_DIR, class_name), exist_ok=True)\n",
    "\n",
    "        # Loop over all audio files in the class folder\n",
    "        for file_name in os.listdir(class_path):\n",
    "            # Skip non-wav-audio files\n",
    "            if not file_name.endswith(\".wav\"):  \n",
    "                continue\n",
    "            \n",
    "            # Copy file to train/ or test/\n",
    "            src_path = os.path.join(class_path, file_name)\n",
    "            dest_folder = TEST_DIR if f\"{class_name}/{file_name}\" in val_test_files else TRAIN_DIR\n",
    "            dest_path = os.path.join(dest_folder, class_name)        \n",
    "            shutil.copy(src_path, dest_path)\n",
    "\n",
    "    # Remove _background_noise_ (not needed for this notebook)\n",
    "    background_noise_train = TRAIN_DIR / \"_background_noise_\"\n",
    "    background_noise_test = TEST_DIR / \"_background_noise_\"\n",
    "\n",
    "    # Remove unnecessary folders and files\n",
    "    if background_noise_train.exists():\n",
    "        shutil.rmtree(background_noise_train)\n",
    "\n",
    "    if background_noise_test.exists():\n",
    "        shutil.rmtree(background_noise_test)\n",
    "\n",
    "    if TARGET_DIR_NAME.exists():\n",
    "        shutil.rmtree(TARGET_DIR_NAME)\n",
    "\n",
    "    zip_file = Path(\"data/speech_commands_v0.02.tar.gz\")\n",
    "    if zip_file.exists():\n",
    "        os.remove(zip_file)\n",
    "\n",
    "    print(\"Dataset restructuring completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "551c8e7d",
   "metadata": {},
   "source": [
    "# 5. Preparing Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68fdfa4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_sample_rate = 8000\n",
    "target_length = 8000 # use 1-sec length\n",
    "waveform, sample_rate = load_audio('data/SpeechCommands/train/backward/0a2b400e_nohash_0.wav')\n",
    "IMG_SIZE = 384\n",
    "BATCH_SIZE = 16\n",
    "ACCUM_STEPS = 4\n",
    "FFT_POINTS = 1024\n",
    "hop_length = round(target_length / (IMG_SIZE - 1))\n",
    "\n",
    "# Transformations for the training dataset. Creates a single spectrogram with dimensions [3, IMG_SIZE, IMG_SIZE].\n",
    "# fft_analysis_method (str): The type of FFT analysis to perform. It can include different signals in the image channels:\n",
    "#    - \"single\": One spectrogram for the entire signal. The spectrogram is replicated across the three image channels.\n",
    "#    - \"time_freq\": Three spectrograms with different time–frequency resolution trade-offs.\n",
    "#    - \"freq_band\": Three spectrograms analyzing different frequency bands (low, mid, high).\n",
    "# fft_analysis_concat (str): Applicable only to \"time_freq\" and \"freq_band\". Specifies how the spectrograms are concatenated:\n",
    "#    - \"freq\": Concatenation along the frequency axis (default for \"freq_band\"); equivalent to a vertical stack.\n",
    "#    - \"time\": Concatenation along the time axis; equivalent to a horizontal stack.\n",
    "#    - \"channel\": One spectrogram per image channel (RGB-like) (default for \"time_freq\").\n",
    "#    - \"default\": Uses \"freq\" for \"freq_band\" and \"channel\" for \"time_freq\".\n",
    "get_transform_train = AudioSpectrogramTransforms(\n",
    "    augmentation=True, # Augmentation is added: random noise + time masking + frequecy masking (see plot below)\n",
    "    mean_std_norm=True,\n",
    "    fft_analysis_method=\"time_freq\",\n",
    "    fft_analysis_concat=\"default\",\n",
    "    sample_rate=sample_rate,\n",
    "    new_sample_rate=new_sample_rate,\n",
    "    target_length=target_length,\n",
    "    n_fft=FFT_POINTS,\n",
    "    img_size=(IMG_SIZE, IMG_SIZE),\n",
    "    augment_magnitude=2\n",
    ")\n",
    "\n",
    "# Transformations for test dataset. Creates a single spectrogram with dimensions [3, IMG_SIZE, IMG_SIZE]\n",
    "get_transform_test = AudioSpectrogramTransforms(\n",
    "    augmentation=False,\n",
    "    mean_std_norm=True,\n",
    "    fft_analysis_method=\"time_freq\",\n",
    "    fft_analysis_concat=\"default\",\n",
    "    sample_rate=sample_rate,\n",
    "    new_sample_rate=new_sample_rate,\n",
    "    target_length=target_length,\n",
    "    n_fft=FFT_POINTS,\n",
    "    img_size=(IMG_SIZE, IMG_SIZE),\n",
    "    #augment_magnitude=2\n",
    ")\n",
    "\n",
    "# Create dataloaders. Each batch has dimensions [BATCH_SIZE, 3, IMG_SIZE; IMG_SIZE]\n",
    "train_dataloader, test_dataloader, class_names = create_dataloaders_spectrogram(\n",
    "    train_dir=TRAIN_DIR,\n",
    "    test_dir=TEST_DIR,\n",
    "    train_transform=get_transform_train,\n",
    "    test_transform=get_transform_test,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    random_seed=SEED\n",
    ")\n",
    "\n",
    "dataloaders = {\n",
    "    'train': train_dataloader,\n",
    "    'test':  test_dataloader\n",
    "}\n",
    "\n",
    "# Verify classes and batches\n",
    "print(f\"Classes: {class_names}\")\n",
    "print(f\"Train batches: {len(train_dataloader)}, Test batches: {len(test_dataloader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25a32cb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the number of classes\n",
    "NUM_CLASSES = len(class_names)\n",
    "print(f\"Number of classes: {NUM_CLASSES}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ac632b5",
   "metadata": {},
   "source": [
    "# 6. Audio Visualization and Reproduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5014282",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the length of the train_set\n",
    "train_set_size = len(train_dataloader.dataset)\n",
    "\n",
    "# Visualize some audio waveforms\n",
    "num_samples = 10\n",
    "fig, axs = plt.subplots(num_samples, 2, figsize=(15, num_samples*3))\n",
    "\n",
    "# Plot waveform and spectrogram\n",
    "for row in range(num_samples):\n",
    "    # Randomly select an index from the train_set\n",
    "    idx = torch.randint(0, train_set_size, (1,)).item()\n",
    "    \n",
    "    # Get waveform\n",
    "    waveform, _ = load_audio(train_dataloader.dataset.files[idx])\n",
    "\n",
    "    # Get spectrogram    \n",
    "    audio_spectrogram_transforms = AudioSpectrogramTransforms(\n",
    "        augmentation=True,\n",
    "        mean_std_norm=False,\n",
    "        fft_analysis_method=\"time_freq\", #\"time_freq\" #\"freq_band\"\n",
    "        fft_analysis_concat=\"default\",\n",
    "        sample_rate=sample_rate,\n",
    "        new_sample_rate=new_sample_rate,\n",
    "        target_length=target_length,\n",
    "        n_fft=FFT_POINTS,\n",
    "        img_size=(IMG_SIZE, IMG_SIZE),\n",
    "        augment_magnitude=2\n",
    "    )\n",
    "\n",
    "    spectrogram = audio_spectrogram_transforms(waveform)\n",
    "\n",
    "    # Get label\n",
    "    label = class_names[train_dataloader.dataset.labels[idx]]\n",
    "    \n",
    "    # Plot waveform\n",
    "    axs[row][0].plot(waveform.t().numpy())  # Ensure the waveform is transposed if necessary\n",
    "    axs[row][0].set_title(f\"Waveform - Label: {label} - Idx: {train_dataloader.dataset.labels[idx]}\")\n",
    "    axs[row][0].set_xlabel(\"Time\")\n",
    "    axs[row][0].set_ylabel(\"Amplitude\")\n",
    "    axs[row][0].set_xticks([])\n",
    "    axs[row][0].set_yticks([])\n",
    "\n",
    "    # Plot spectrogram\n",
    "    axs[row][1].imshow(spectrogram.permute(1, 2, 0).detach().numpy(), aspect='auto', origin='lower', cmap='magma')\n",
    "    axs[row][1].set_title(f\"Spectrogram - Label: {label} - Idx: {train_dataloader.dataset.labels[idx]}\")\n",
    "    axs[row][1].set_xlabel(\"Time\")\n",
    "    axs[row][1].set_ylabel(\"Frequency\")\n",
    "    axs[row][1].set_xticks([])\n",
    "    axs[row][1].set_yticks([])\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3015b2ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Play out some audio files\n",
    "try:\n",
    "    waveform_first, _ = load_audio(train_dataloader.dataset.files[0])\n",
    "except:\n",
    "    waveform_first, _ = load_audio(train_dataloader.dataset.dataset.files[0])\n",
    "ipd.Audio(waveform_first.numpy(), rate=sample_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c5e0da8",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    waveform_second, *_ = load_audio(train_dataloader.dataset.files[1])\n",
    "except:\n",
    "    waveform_second, *_ = load_audio(train_dataloader.dataset.dataset.files[1])\n",
    "ipd.Audio(waveform_second.numpy(), rate=sample_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4530ea7",
   "metadata": {},
   "source": [
    "# 7. Creating the Transformer Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a11c83a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the ViT-Base/16-384 architecture with images of 384x384 pixels\n",
    "model = build_pretrained_model(\n",
    "    model=\"vit_b_16_384\",\n",
    "    output_dim=NUM_CLASSES,\n",
    "    #dropout=0.1,\n",
    "    seed=SEED,\n",
    "    freeze_backbone=False,\n",
    "    device=device\n",
    "    )\n",
    "\n",
    "# Compile model (optional)\n",
    "#model = torch.compile(model, backend=\"aot_eager\")\n",
    "\n",
    "# Send model to device\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28a1291e",
   "metadata": {},
   "source": [
    "# 8. Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa1218f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "EPOCHS = 10\n",
    "LR = 1e-4\n",
    "ETA_MIN = 1e-6\n",
    "model_type=\"model_spectrogram\"\n",
    "model_name = model_type + \".pth\"\n",
    "\n",
    "# Create AdamW optimizer\n",
    "optimizer = torch.optim.AdamW(\n",
    "    params=model.parameters(),\n",
    "    lr=LR,\n",
    "    weight_decay=0.0001\n",
    ")\n",
    "\n",
    "# Create loss function\n",
    "loss_fn = torch.nn.CrossEntropyLoss(label_smoothing=0.1)\n",
    "\n",
    "# Set scheduler\n",
    "#scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.1)\n",
    "scheduler = CosineAnnealingLR(optimizer, T_max=EPOCHS, eta_min=ETA_MIN)\n",
    "\n",
    "# And train...\n",
    "\n",
    "# Instantiate the classification engine with the created model and the target device\n",
    "engine = ClassificationEngine(\n",
    "    model=model,                                # Model to be trained\n",
    "    optimizer=optimizer,                        # Optimizer\n",
    "    loss_fn=loss_fn,                            # Loss function\n",
    "    scheduler=scheduler,                        # Scheduler \n",
    "    use_distillation=False,                     # Optional, use_distillation is False by default    \n",
    "    log_verbose=True,                           # Verbosity    \n",
    "    #theme='dark',                               # Theme (default is 'light')\n",
    "    device=device                               # Target device\n",
    "    )\n",
    "\n",
    "# Configure the training method\n",
    "results = engine.train(\n",
    "    target_dir=MODEL_DIR,                       # Directory where the model will be saved\n",
    "    model_name=model_name,                      # Name of the model\n",
    "    resume=False,                                # Resume training from the last saved checkpoint\n",
    "    save_best_model=[\"last\", \"loss\", \"acc\"],    # Save the best models based on different criteria\n",
    "    keep_best_models_in_memory=False,           # Do not keep the models stored in memory for the sake of training time and memory efficiency    \n",
    "    dataloaders=dataloaders,                    # Dictionary with the dataloaders     \n",
    "    apply_validation=True,                      # Enable validation step\n",
    "    augmentation_strategy=\"always\",             # Augmentation strategy    \n",
    "    recall_threshold=1.0,                       # False positive rate at recall_threshold recall\n",
    "    recall_threshold_pauc=0.0,                  # Partial AUC score above recall_threshold_pauc recall\n",
    "    epochs=EPOCHS,                              # Total number of epochs\n",
    "    amp=True,                                   # Enable Automatic Mixed Precision (AMP)\n",
    "    enable_clipping=False,                      # Disable clipping on gradients, only useful if training becomes unestable\n",
    "    debug_mode=False,                           # Disable debug mode    \n",
    "    accumulation_steps=ACCUM_STEPS,             # Accumulation steps 2: effective batch size = batch_size x accumulation steps\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f53934b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions by manually loading the best model\n",
    "if device == \"cuda\":\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "model = build_pretrained_model(\n",
    "    model=\"vit_b_16_384\",\n",
    "    num_classes=NUM_CLASSES,\n",
    "    #dropout=0.1,\n",
    "    seed=SEED,\n",
    "    device=device\n",
    "    )\n",
    "\n",
    "# Compile model (optional)\n",
    "#model = torch.compile(model, backend=\"aot_eager\")\n",
    "\n",
    "# Find the file that matchs the pattern `_pauc_`\n",
    "model_file = glob.glob(os.path.join(MODEL_DIR, \"model_spectrogram_acc_*.pth\"))\n",
    "model_name = os.path.basename(model_file[0])\n",
    "\n",
    "# Instantiate engine for predictions\n",
    "engine2 = ClassificationEngine(\n",
    "        model=model,        \n",
    "        log_verbose=True,\n",
    "        device=device)\n",
    "\n",
    "# Load the model\n",
    "model = engine2.load(target_dir=MODEL_DIR, model_name=model_name, export=True)\n",
    "# or\n",
    "#model = load_model(model, MODEL_DIR, model_name)\n",
    "#model.to(device)\n",
    "\n",
    "# Make predictions on the test dataloader\n",
    "indexes2 = engine2.predict(\n",
    "    dataloader=test_dataloader,\n",
    "    output_type='argmax').tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "108f2d42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get 24 random indexes from the test dataset\n",
    "num_samples = 24\n",
    "random_indices = random.sample(range(len(test_dataloader.dataset)), num_samples)\n",
    "\n",
    "# Load audio files and get predictions\n",
    "waveform_list = []\n",
    "label_list = []\n",
    "sample_rate_list = []\n",
    "for idx in random_indices:\n",
    "    \n",
    "    # Load waveform and label\n",
    "    try:\n",
    "        waveform, sample_rate = load_audio(test_dataloader.dataset.files[idx])\n",
    "        actual_label = class_names[test_dataloader.dataset.labels[idx]]\n",
    "    except:\n",
    "        waveform, sample_rate = load_audio(test_dataloader.dataset.dataset.files[idx])\n",
    "        actual_label = class_names[test_dataloader.dataset.dataset.labels[idx]]\n",
    "\n",
    "    # Append data\n",
    "    waveform_list.append(waveform)\n",
    "    label_list.append(actual_label)\n",
    "    sample_rate_list.append(sample_rate)\n",
    "\n",
    "# Predict and play back\n",
    "predict_and_play_audio(\n",
    "    model=model,\n",
    "    waveform_list=waveform_list,\n",
    "    label_list=label_list,\n",
    "    sample_rate_list=sample_rate_list,\n",
    "    class_names=class_names,\n",
    "    transform=get_transform_test,\n",
    "    device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc38f1df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a classification report \n",
    "pred_list, classif_report = engine2.predict_and_store(\n",
    "    test_dir=TEST_DIR,\n",
    "    transform=get_transform_test,\n",
    "    class_names=class_names,\n",
    "    sample_fraction=1,\n",
    "    seed=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5528210d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(classif_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afa23c06",
   "metadata": {},
   "outputs": [],
   "source": [
    "speed = round(1.0 / pd.DataFrame(pred_list)['time_for_pred'].mean(), 2)\n",
    "print(f'GPU: Predicted Images per Sec [fps]: {speed}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92f42e31",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(classif_report)\n",
    "filtered_df = df.drop(columns=[\"accuracy\", \"macro avg\", \"weighted avg\"], errors=\"ignore\")\n",
    "f1_scores = filtered_df.loc['f1-score']\n",
    "min_f1_class = f1_scores.idxmin()\n",
    "min_f1_value = f1_scores[min_f1_class]\n",
    "max_f1_class = f1_scores.idxmax()\n",
    "max_f1_value = f1_scores[max_f1_class]\n",
    "\n",
    "print(f\"Class with lowest F1-score: {min_f1_class} (F1 = {min_f1_value:.4f})\")\n",
    "print(f\"Class with highest F1-score: {max_f1_class} (F1 = {max_f1_value:.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49df007b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv_pytorch_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
