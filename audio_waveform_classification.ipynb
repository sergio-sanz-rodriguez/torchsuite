{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "41e3e0ca",
   "metadata": {},
   "source": [
    "# 1. Introduction\n",
    "\n",
    "This notebook outlines the creation, compilation, and training of a deep learing network for audio classification using the [TorchSuite](https://github.com/sergio-sanz-rodriguez/torchsuite) framework.\n",
    " \n",
    "https://pytorch.org/tutorials/intermediate/speech_command_classification_with_torchaudio_tutorial.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dba7507",
   "metadata": {},
   "source": [
    "# 2. Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6892a15",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install torcheval\n",
    "import os\n",
    "import shutil\n",
    "import torch\n",
    "import glob\n",
    "import random\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import IPython.display as ipd\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "from pathlib import Path\n",
    "from torchinfo import summary\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR, ConstantLR, SequentialLR, StepLR\n",
    "from torchaudio.datasets import SPEECHCOMMANDS\n",
    "from torchaudio.transforms import Resample\n",
    "\n",
    "# Import custom libraries\n",
    "from utils.classification_utils import set_seeds, predict_and_play_audio, load_model\n",
    "from engines.classification import ClassificationEngine\n",
    "from engines.schedulers import FixedLRSchedulerWrapper\n",
    "from dataloaders.audio_dataloaders import load_audio, create_dataloaders_waveform, PadWaveform, AudioWaveformTransforms\n",
    "from models.wav2vec2 import Wav2Vec2Classifier\n",
    "\n",
    "import warnings\n",
    "os.environ['TORCH_USE_CUDA_DSA'] = \"1\"\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"torch.autograd.graph\")\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning, module=\"onnxscript.converter\")\n",
    "\n",
    "# Paths (modify as needed)\n",
    "TARGET_DIR_NAME = Path(\"data/SpeechCommands/speech_commands_v0.02\")\n",
    "TRAIN_DIR = Path(\"data/SpeechCommands/train\")\n",
    "TEST_DIR = Path(\"data/SpeechCommands/test\")\n",
    "\n",
    "# Define some constants\n",
    "NUM_WORKERS = os.cpu_count()\n",
    "SEED = 42\n",
    "\n",
    "# Create target model directory\n",
    "MODEL_DIR = Path(\"outputs\")\n",
    "MODEL_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Set seeds\n",
    "set_seeds(SEED)\n",
    "\n",
    "IMPORT_DATASET = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d316112",
   "metadata": {},
   "source": [
    "# 3. Specifying the Target Device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f01afed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activate cuda benchmark\n",
    "cudnn.benchmark = True\n",
    "\n",
    "# Set device\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Device: {device}\")\n",
    "\n",
    "if device == \"cuda\":\n",
    "    !nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6555499",
   "metadata": {},
   "source": [
    "# 4. Importing Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d94af740",
   "metadata": {},
   "outputs": [],
   "source": [
    "if IMPORT_DATASET:\n",
    "    # Download dataset\n",
    "    os.makedirs(\"data\", exist_ok=True)\n",
    "    dataset = SPEECHCOMMANDS(\n",
    "        root=\"./data\",\n",
    "        url=\"speech_commands_v0.02\",\n",
    "        folder_in_archive=\"SpeechCommands\",\n",
    "        download=True,\n",
    "        subset=None\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e69190a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if IMPORT_DATASET:\n",
    "    # Read validation and test lists\n",
    "    val_test_files = set()\n",
    "    for filename in [\"validation_list.txt\", \"testing_list.txt\"]:\n",
    "        with open(os.path.join(TARGET_DIR_NAME, filename), \"r\") as f:\n",
    "            val_test_files.update(f.read().splitlines())\n",
    "\n",
    "    # Ensure output directories exist\n",
    "    os.makedirs(TRAIN_DIR, exist_ok=True)\n",
    "    os.makedirs(TEST_DIR, exist_ok=True)\n",
    "\n",
    "    # Loop over all class folders\n",
    "    for class_name in os.listdir(TARGET_DIR_NAME):\n",
    "        class_path = os.path.join(TARGET_DIR_NAME, class_name)\n",
    "        if not os.path.isdir(class_path):  # Skip non-folder files\n",
    "            continue\n",
    "\n",
    "        # Create class folders in train/ and test/\n",
    "        os.makedirs(os.path.join(TRAIN_DIR, class_name), exist_ok=True)\n",
    "        os.makedirs(os.path.join(TEST_DIR, class_name), exist_ok=True)\n",
    "\n",
    "        # Loop over all audio files in the class folder\n",
    "        for file_name in os.listdir(class_path):\n",
    "            # Skip non-wav-audio files\n",
    "            if not file_name.endswith(\".wav\"):  \n",
    "                continue\n",
    "            \n",
    "            # Copy file to train/ or test/\n",
    "            src_path = os.path.join(class_path, file_name)\n",
    "            dest_folder = TEST_DIR if f\"{class_name}/{file_name}\" in val_test_files else TRAIN_DIR\n",
    "            dest_path = os.path.join(dest_folder, class_name)        \n",
    "            shutil.copy(src_path, dest_path)\n",
    "\n",
    "    # Remove _background_noise_ (not needed for this notebook)\n",
    "    background_noise_train = TRAIN_DIR / \"_background_noise_\"\n",
    "    background_noise_test = TEST_DIR / \"_background_noise_\"\n",
    "\n",
    "    # Remove unnecessary folders and files\n",
    "    if background_noise_train.exists():\n",
    "        shutil.rmtree(background_noise_train)\n",
    "\n",
    "    if background_noise_test.exists():\n",
    "        shutil.rmtree(background_noise_test)\n",
    "\n",
    "    if TARGET_DIR_NAME.exists():\n",
    "        shutil.rmtree(TARGET_DIR_NAME)\n",
    "\n",
    "    zip_file = Path(\"data/speech_commands_v0.02.tar.gz\")\n",
    "    if zip_file.exists():\n",
    "        os.remove(zip_file)\n",
    "\n",
    "    print(\"Dataset restructuring completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "551c8e7d",
   "metadata": {},
   "source": [
    "# 5. Preparing Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68fdfa4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_sample_rate = 8000\n",
    "target_length = 8000 # use 1-sec length\n",
    "waveform, sample_rate = load_audio('data/SpeechCommands/train/backward/0a2b400e_nohash_0.wav')\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "# Transformations for training dataset\n",
    "get_transform_train = AudioWaveformTransforms(\n",
    "    augmentation=True,\n",
    "    sample_rate=sample_rate,\n",
    "    new_sample_rate=new_sample_rate,\n",
    "    target_length=target_length,\n",
    "    seed=SEED,\n",
    "    augment_magnitude=2\n",
    ")\n",
    "\n",
    "# Transformations for test dataset\n",
    "get_transform_test = AudioWaveformTransforms(\n",
    "    augmentation=False,   \n",
    "    sample_rate=sample_rate,\n",
    "    new_sample_rate=new_sample_rate,\n",
    "    target_length=target_length,\n",
    "    seed=SEED,\n",
    "    augment_magnitude=2\n",
    ")\n",
    "\n",
    "train_dataloader, test_dataloader, class_names = create_dataloaders_waveform(\n",
    "    train_dir=TRAIN_DIR,\n",
    "    test_dir=TEST_DIR,\n",
    "    train_transform=get_transform_train,\n",
    "    test_transform=get_transform_test,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    random_seed=SEED\n",
    ")\n",
    "\n",
    "dataloaders = {\n",
    "    'train': train_dataloader,\n",
    "    'test':  test_dataloader\n",
    "}\n",
    "\n",
    "# Verify classes and batches\n",
    "print(f\"Classes: {class_names}\")\n",
    "print(f\"Train batches: {len(train_dataloader)}, Test batches: {len(test_dataloader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25a32cb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the number of classes\n",
    "num_classes = len(class_names)\n",
    "print(f\"Number of classes: {num_classes}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ac632b5",
   "metadata": {},
   "source": [
    "# 6. Audio Visualization and Reproduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5014282",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the length of the train_set\n",
    "train_set_size = len(train_dataloader.dataset)\n",
    "\n",
    "# Visualize some audio waveforms\n",
    "fig, axs = plt.subplots(5, 2, figsize=(15, 15))\n",
    "for col in range(2):\n",
    "    for row in range(5):\n",
    "        # Randomly select an index from the train_set\n",
    "        idx = torch.randint(0, train_set_size, (1,)).item()\n",
    "        \n",
    "        # Get the waveform, sample rate, and label for the selected index\n",
    "        try:\n",
    "            waveform, _ = load_audio(train_dataloader.dataset.files[idx])\n",
    "            label = class_names[train_dataloader.dataset.labels[idx]]\n",
    "            \n",
    "            # Plot the waveform\n",
    "            axs[row][col].plot(waveform.t().numpy())  # Ensure the waveform is transposed if necessary\n",
    "            axs[row][col].set_title(f\"Label: {label} - Idx: {train_dataloader.dataset.labels[idx]}\")  # Set the label as the title\n",
    "        except:\n",
    "            waveform, _ = load_audio(train_dataloader.dataset.dataset.files[idx])\n",
    "            label = class_names[train_dataloader.dataset.dataset.labels[idx]]        \n",
    "        \n",
    "            # Plot the waveform\n",
    "            axs[row][col].plot(waveform.t().numpy())  # Ensure the waveform is transposed if necessary\n",
    "            axs[row][col].set_title(f\"Label: {label} - Idx: {train_dataloader.dataset.dataset.labels[idx]}\")  # Set the label as the title\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3015b2ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Play out some audio files\n",
    "try:\n",
    "    waveform_first, _ = load_audio(train_dataloader.dataset.files[0])\n",
    "except:\n",
    "    waveform_first, _ = load_audio(train_dataloader.dataset.dataset.files[0])\n",
    "ipd.Audio(waveform_first.numpy(), rate=sample_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c5e0da8",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    waveform_second, *_ = load_audio(train_dataloader.dataset.files[1])\n",
    "except:\n",
    "    waveform_second, *_ = load_audio(train_dataloader.dataset.dataset.files[1])\n",
    "ipd.Audio(waveform_second.numpy(), rate=sample_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4530ea7",
   "metadata": {},
   "source": [
    "# 7. Creating the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a11c83a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a custom model\n",
    "class M5(nn.Module):\n",
    "    def __init__(self, n_input=1, n_output=35, stride=16, n_channel=32):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv1d(n_input, n_channel, kernel_size=80, stride=stride)\n",
    "        self.bn1 = nn.BatchNorm1d(n_channel)\n",
    "        self.pool1 = nn.MaxPool1d(4)\n",
    "        self.conv2 = nn.Conv1d(n_channel, n_channel, kernel_size=3)\n",
    "        self.bn2 = nn.BatchNorm1d(n_channel)\n",
    "        self.pool2 = nn.MaxPool1d(4)\n",
    "        self.conv3 = nn.Conv1d(n_channel, 2 * n_channel, kernel_size=3)\n",
    "        self.bn3 = nn.BatchNorm1d(2 * n_channel)\n",
    "        self.pool3 = nn.MaxPool1d(4)\n",
    "        self.conv4 = nn.Conv1d(2 * n_channel, 2 * n_channel, kernel_size=3)\n",
    "        self.bn4 = nn.BatchNorm1d(2 * n_channel)\n",
    "        self.pool4 = nn.MaxPool1d(4)\n",
    "        self.fc1 = nn.Linear(2 * n_channel, n_output)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(self.bn1(x))\n",
    "        x = self.pool1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(self.bn2(x))\n",
    "        x = self.pool2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = F.relu(self.bn3(x))\n",
    "        x = self.pool3(x)\n",
    "        x = self.conv4(x)\n",
    "        x = F.relu(self.bn4(x))\n",
    "        x = self.pool4(x)\n",
    "        x = F.avg_pool1d(x, x.shape[-1])\n",
    "        x = x.squeeze(-1)\n",
    "        x = self.fc1(x)\n",
    "        return F.log_softmax(x, dim=1)    \n",
    "\n",
    "CUSTOM_MODEL = 2\n",
    "\n",
    "# Instatiate the model\n",
    "if CUSTOM_MODEL == 1:\n",
    "    in_channels = load_audio(train_dataloader.dataset.files[0])[0].shape[0]\n",
    "    model = M5(n_input=in_channels, n_output=num_classes)\n",
    "    LR = 0.01\n",
    "    ETA_MIN = 1e-5\n",
    "    EPOCHS = 25\n",
    "else:\n",
    "    model = Wav2Vec2Classifier(\n",
    "        base_model_name=\"facebook/wav2vec2-base\",\n",
    "        num_classes=num_classes)\n",
    "    LR = 1e-5\n",
    "    ETA_MIN = 1e-7\n",
    "    EPOCHS = 25\n",
    "\n",
    "# Unfreeze the base parameters\n",
    "for parameter in model.parameters():\n",
    "    parameter.requires_grad = True\n",
    "\n",
    "# Compile model (optional)\n",
    "#model = torch.compile(model, backend=\"aot_eager\")\n",
    "\n",
    "# Send model to device\n",
    "model.to(device)\n",
    "\n",
    "# Print the summary of the model\n",
    "summary(model,\n",
    "        input_size=(BATCH_SIZE, 16000),\n",
    "        col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
    "        col_width=20,\n",
    "        row_settings=[\"var_names\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28a1291e",
   "metadata": {},
   "source": [
    "# 8. Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa1218f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "model_type=\"model_waveform\"\n",
    "model_name = model_type + \".pth\"\n",
    "\n",
    "# Create AdamW optimizer\n",
    "optimizer = torch.optim.AdamW(\n",
    "    params=model.parameters(),\n",
    "    lr=LR,\n",
    "    weight_decay=0.0001\n",
    ")\n",
    "\n",
    "# Create loss function\n",
    "loss_fn = torch.nn.CrossEntropyLoss(label_smoothing=0.1)\n",
    "\n",
    "# Set scheduler\n",
    "#scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.1) # Reduces LR by a constant factor\n",
    "cosine = CosineAnnealingLR(optimizer, T_max=EPOCHS-5, eta_min=ETA_MIN) # 1-20:  LR = 1e-5 -> 1e-7 (cosine)\n",
    "fixed = ConstantLR(optimizer, factor=ETA_MIN/LR, total_iters=5)        # 20-25: LR = 1e-6\n",
    "scheduler = SequentialLR(\n",
    "    optimizer,\n",
    "    schedulers=[cosine, fixed],\n",
    "    milestones=[20] \n",
    ")\n",
    "\n",
    "# Alternative is this custom class:\n",
    "#scheduler = FixedLRSchedulerWrapper(\n",
    "#    scheduler=CosineAnnealingLR(optimizer, T_max=EPOCHS-5, eta_min=ETA_MIN),\n",
    "#    fixed_lr=ETA_MIN,\n",
    "#    fixed_epoch=EPOCHS-5)\n",
    "\n",
    "# And train...\n",
    "\n",
    "# Instantiate the classification engine with the created model and the target device\n",
    "engine = ClassificationEngine(\n",
    "    model=model,                                # Model to be trained\n",
    "    optimizer=optimizer,                        # Optimizer\n",
    "    loss_fn=loss_fn,                            # Loss function\n",
    "    scheduler=scheduler,                        # Scheduler \n",
    "    use_distillation=False,                     # Optional, use_distillation is False by default    \n",
    "    log_verbose=True,                           # Verbosity\n",
    "    theme='dark',                               # Theme (default is 'light')\n",
    "    device=device                               # Target device\n",
    "    )\n",
    "\n",
    "# Configure the training method\n",
    "results = engine.train(\n",
    "    target_dir=MODEL_DIR,                       # Directory where the model will be saved\n",
    "    model_name=model_name,                      # Name of the model\n",
    "    resume=True,                                # Resume training from the last saved checkpoint\n",
    "    save_best_model=[\"last\", \"loss\", \"acc\"],    # Save the best models based on different criteria\n",
    "    keep_best_models_in_memory=False,           # Do not keep the models stored in memory for the sake of training time and memory efficiency    \n",
    "    dataloaders=dataloaders,                    # Dictionary with the dataloaders     \n",
    "    apply_validation=True,                      # Enable validation step\n",
    "    augmentation_strategy=\"always\",             # Augmentation strategy    \n",
    "    recall_threshold=1.0,                       # False positive rate at recall_threshold recall\n",
    "    recall_threshold_pauc=0.0,                  # Partial AUC score above recall_threshold_pauc recall\n",
    "    epochs=EPOCHS,                              # Total number of epochs\n",
    "    amp=True,                                   # Enable Automatic Mixed Precision (AMP)\n",
    "    enable_clipping=False,                      # Disable clipping on gradients, only useful if training becomes unestable\n",
    "    debug_mode=False,                           # Disable debug mode    \n",
    "    accumulation_steps=1,                       # Accumulation steps 2: effective batch size = batch_size x accumulation steps\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f53934b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions by manually loading the last-epoch model manually\n",
    "engine2 = ClassificationEngine(\n",
    "        model=Wav2Vec2Classifier(num_classes=num_classes),\n",
    "        log_verbose=False,\n",
    "        device=device)\n",
    "\n",
    "# Find the file that matchs the pattern `model_acc`\n",
    "model_file = glob.glob(os.path.join(MODEL_DIR, \"model_waveform_acc_*.pth\"))\n",
    "model_name = os.path.basename(model_file[0])\n",
    "engine2.load(target_dir=MODEL_DIR, model_name=model_name)\n",
    "indexes2 = engine2.predict(\n",
    "    dataloader=test_dataloader,\n",
    "    output_type='argmax').tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "610eb67e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load now the model and assign it to `model`\n",
    "model = Wav2Vec2Classifier(num_classes=num_classes)\n",
    "#model = torch.compile(model, backend=\"aot_eager\")\n",
    "model = load_model(model, MODEL_DIR, model_name)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "108f2d42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get 24 random indexes from the test dataset\n",
    "num_samples = 24\n",
    "random_indices = random.sample(range(len(test_dataloader.dataset)), num_samples)\n",
    "\n",
    "# Load audio files and get predictions\n",
    "waveform_list = []\n",
    "label_list = []\n",
    "sample_rate_list = []\n",
    "for idx in random_indices:\n",
    "    \n",
    "    # Load waveform and label\n",
    "    try:\n",
    "        waveform, sample_rate = load_audio(test_dataloader.dataset.files[idx])\n",
    "        actual_label = class_names[test_dataloader.dataset.labels[idx]]\n",
    "    except:\n",
    "        waveform, sample_rate = load_audio(test_dataloader.dataset.dataset.files[idx])\n",
    "        actual_label = class_names[test_dataloader.dataset.dataset.labels[idx]]\n",
    "\n",
    "    # Append data\n",
    "    waveform_list.append(waveform)\n",
    "    label_list.append(actual_label)\n",
    "    sample_rate_list.append(sample_rate)\n",
    "\n",
    "# Predict and play back\n",
    "predict_and_play_audio(\n",
    "    model=model,\n",
    "    waveform_list=waveform_list,\n",
    "    label_list=label_list,\n",
    "    sample_rate_list=sample_rate_list,\n",
    "    class_names=class_names,\n",
    "    transform=get_transform_test,\n",
    "    device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc38f1df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a classification report \n",
    "pred_list, classif_report = engine2.predict_and_store(\n",
    "    test_dir=TEST_DIR,\n",
    "    transform=get_transform_test,\n",
    "    class_names=class_names,\n",
    "    sample_fraction=1,\n",
    "    seed=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5528210d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(classif_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea720087",
   "metadata": {},
   "outputs": [],
   "source": [
    "speed = round(1.0 / pd.DataFrame(pred_list)['time_for_pred'].mean(), 2)\n",
    "print(f'GPU: Predicted Images per Sec [fps]: {speed}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv_pytorch_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
