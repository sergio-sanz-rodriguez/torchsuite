{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "41e3e0ca",
   "metadata": {},
   "source": [
    "# 1. Introduction\n",
    "\n",
    "This notebook outlines the creation, compilation, and training of a deep learing network for audio classification using the [TorchSuite](https://github.com/sergio-sanz-rodriguez/torchsuite) framework.\n",
    " \n",
    "https://pytorch.org/tutorials/intermediate/speech_command_classification_with_torchaudio_tutorial.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dba7507",
   "metadata": {},
   "source": [
    "# 2. Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6892a15",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install torcheval\n",
    "import os\n",
    "import shutil\n",
    "import torch\n",
    "import glob\n",
    "import random\n",
    "import librosa\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.nn as nn\n",
    "import IPython.display as ipd\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import torchaudio\n",
    "\n",
    "from pathlib import Path\n",
    "from torchinfo import summary\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from torchaudio.datasets import SPEECHCOMMANDS\n",
    "\n",
    "\n",
    "# Import custom libraries\n",
    "from utils.classification_utils import set_seeds, predict_and_play_audio, load_model\n",
    "from engines.classification import ClassificationEngine\n",
    "from engines.schedulers import FixedLRSchedulerWrapper\n",
    "from models.vision_transformer import create_vit\n",
    "from dataloaders.audio_dataloaders import load_audio, create_dataloaders_spectrogram, AudioSpectrogramTransforms\n",
    "\n",
    "import warnings\n",
    "os.environ['TORCH_USE_CUDA_DSA'] = \"1\"\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"torch.autograd.graph\")\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning, module=\"onnxscript.converter\")\n",
    "\n",
    "# Paths (modify as needed)\n",
    "TARGET_DIR_NAME = Path(\"data/SpeechCommands/speech_commands_v0.02\")\n",
    "TRAIN_DIR = Path(\"data/SpeechCommands/train\")\n",
    "TEST_DIR = Path(\"data/SpeechCommands/test\")\n",
    "\n",
    "# Define some constants\n",
    "NUM_WORKERS = os.cpu_count()\n",
    "SEED = 42\n",
    "\n",
    "# Create target model directory\n",
    "MODEL_DIR = Path(\"outputs\")\n",
    "MODEL_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Set seeds\n",
    "set_seeds(SEED)\n",
    "\n",
    "IMPORT_DATASET = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f40496fd",
   "metadata": {},
   "source": [
    "# 3. Specifying the Target Device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e897ca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activate cuda benchmark\n",
    "cudnn.benchmark = True\n",
    "\n",
    "# Set device\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Device: {device}\")\n",
    "\n",
    "if device == \"cuda\":\n",
    "    !nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6555499",
   "metadata": {},
   "source": [
    "# 4. Importing Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d94af740",
   "metadata": {},
   "outputs": [],
   "source": [
    "if IMPORT_DATASET:\n",
    "    # Download dataset\n",
    "    os.makedirs(\"data\", exist_ok=True)\n",
    "    dataset = SPEECHCOMMANDS(\n",
    "        root=\"./data\",\n",
    "        url=\"speech_commands_v0.02\",\n",
    "        folder_in_archive=\"SpeechCommands\",\n",
    "        download=True,\n",
    "        subset=None\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e69190a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if IMPORT_DATASET:\n",
    "    # Read validation and test lists\n",
    "    val_test_files = set()\n",
    "    for filename in [\"validation_list.txt\", \"testing_list.txt\"]:\n",
    "        with open(os.path.join(TARGET_DIR_NAME, filename), \"r\") as f:\n",
    "            val_test_files.update(f.read().splitlines())\n",
    "\n",
    "    # Ensure output directories exist\n",
    "    os.makedirs(TRAIN_DIR, exist_ok=True)\n",
    "    os.makedirs(TEST_DIR, exist_ok=True)\n",
    "\n",
    "    # Loop over all class folders\n",
    "    for class_name in os.listdir(TARGET_DIR_NAME):\n",
    "        class_path = os.path.join(TARGET_DIR_NAME, class_name)\n",
    "        if not os.path.isdir(class_path):  # Skip non-folder files\n",
    "            continue\n",
    "\n",
    "        # Create class folders in train/ and test/\n",
    "        os.makedirs(os.path.join(TRAIN_DIR, class_name), exist_ok=True)\n",
    "        os.makedirs(os.path.join(TEST_DIR, class_name), exist_ok=True)\n",
    "\n",
    "        # Loop over all audio files in the class folder\n",
    "        for file_name in os.listdir(class_path):\n",
    "            # Skip non-wav-audio files\n",
    "            if not file_name.endswith(\".wav\"):  \n",
    "                continue\n",
    "            \n",
    "            # Copy file to train/ or test/\n",
    "            src_path = os.path.join(class_path, file_name)\n",
    "            dest_folder = TEST_DIR if f\"{class_name}/{file_name}\" in val_test_files else TRAIN_DIR\n",
    "            dest_path = os.path.join(dest_folder, class_name)        \n",
    "            shutil.copy(src_path, dest_path)\n",
    "\n",
    "    # Remove _background_noise_ (not needed for this notebook)\n",
    "    background_noise_train = TRAIN_DIR / \"_background_noise_\"\n",
    "    background_noise_test = TEST_DIR / \"_background_noise_\"\n",
    "\n",
    "    # Remove unnecessary folders and files\n",
    "    if background_noise_train.exists():\n",
    "        shutil.rmtree(background_noise_train)\n",
    "\n",
    "    if background_noise_test.exists():\n",
    "        shutil.rmtree(background_noise_test)\n",
    "\n",
    "    if TARGET_DIR_NAME.exists():\n",
    "        shutil.rmtree(TARGET_DIR_NAME)\n",
    "\n",
    "    zip_file = Path(\"data/speech_commands_v0.02.tar.gz\")\n",
    "    if zip_file.exists():\n",
    "        os.remove(zip_file)\n",
    "\n",
    "    print(\"Dataset restructuring completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "551c8e7d",
   "metadata": {},
   "source": [
    "# 5. Preparing Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68fdfa4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_sample_rate = 8000\n",
    "target_length = 8000 # use 1-sec length\n",
    "waveform, sample_rate = load_audio('data/SpeechCommands/train/backward/0a2b400e_nohash_0.wav')\n",
    "IMG_SIZE = 384\n",
    "BATCH_SIZE = 32\n",
    "ACCUM_STEPS = 2\n",
    "FFT_POINTS = 1024\n",
    "hop_length = round(target_length / (IMG_SIZE - 1))\n",
    "\n",
    "# Transformations for training dataset\n",
    "get_transform_train = AudioSpectrogramTransforms(\n",
    "    augmentation=False,\n",
    "    mean_std_norm=True,\n",
    "    fft_analysis_method=\"single\", #\"time_freq\" #\"freq_band\"\n",
    "    sample_rate=sample_rate,\n",
    "    new_sample_rate=new_sample_rate,\n",
    "    target_length=target_length,\n",
    "    n_fft=FFT_POINTS,\n",
    "    img_size=(IMG_SIZE, IMG_SIZE),\n",
    "    augment_magnitude=2\n",
    ")\n",
    "\n",
    "# Transformations for test dataset\n",
    "get_transform_test = AudioSpectrogramTransforms(\n",
    "    augmentation=False,\n",
    "    mean_std_norm=True,\n",
    "    fft_analysis_method=\"single\", #\"time_freq\" #\"freq_band\"\n",
    "    sample_rate=sample_rate,\n",
    "    new_sample_rate=new_sample_rate,\n",
    "    target_length=target_length,\n",
    "    n_fft=FFT_POINTS,\n",
    "    img_size=(IMG_SIZE, IMG_SIZE),\n",
    "    augment_magnitude=2\n",
    ")\n",
    "\n",
    "# Create dataloaders\n",
    "train_dataloader, test_dataloader, class_names = create_dataloaders_spectrogram(\n",
    "    train_dir=TRAIN_DIR,\n",
    "    test_dir=TEST_DIR,\n",
    "    train_transform=get_transform_train,\n",
    "    test_transform=get_transform_test,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    num_workers=0,\n",
    "    random_seed=SEED\n",
    ")\n",
    "\n",
    "# Verify classes and batches\n",
    "print(f\"Classes: {class_names}\")\n",
    "print(f\"Train batches: {len(train_dataloader)}, Test batches: {len(test_dataloader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25a32cb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the number of classes\n",
    "NUM_CLASSES = len(class_names)\n",
    "print(f\"Number of classes: {NUM_CLASSES}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ac632b5",
   "metadata": {},
   "source": [
    "# 6. Audio Visualization and Reproduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5014282",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the length of the train_set\n",
    "train_set_size = len(train_dataloader.dataset)\n",
    "\n",
    "# Visualize some audio waveforms\n",
    "num_samples = 10\n",
    "fig, axs = plt.subplots(num_samples, 2, figsize=(15, num_samples*3))\n",
    "\n",
    "# Plot waveform and spectrogram\n",
    "for row in range(num_samples):\n",
    "    # Randomly select an index from the train_set\n",
    "    idx = torch.randint(0, train_set_size, (1,)).item()\n",
    "    \n",
    "    # Get waveform\n",
    "    waveform, _ = load_audio(train_dataloader.dataset.files[idx])\n",
    "\n",
    "    # Get spectrogram    \n",
    "    audio_spectrogram_transforms = AudioSpectrogramTransforms(\n",
    "        augmentation=False,\n",
    "        mean_std_norm=False,\n",
    "        fft_analysis_method=\"single\", #\"time_freq\" #\"freq_band\"\n",
    "        sample_rate=sample_rate,\n",
    "        new_sample_rate=new_sample_rate,\n",
    "        target_length=target_length,\n",
    "        n_fft=FFT_POINTS,\n",
    "        img_size=(IMG_SIZE, IMG_SIZE),\n",
    "        augment_magnitude=2\n",
    "    )\n",
    "\n",
    "    spectrogram = audio_spectrogram_transforms(waveform)\n",
    "\n",
    "    # Get label\n",
    "    label = class_names[train_dataloader.dataset.labels[idx]]\n",
    "    \n",
    "    # Plot waveform\n",
    "    axs[row][0].plot(waveform.t().numpy())  # Ensure the waveform is transposed if necessary\n",
    "    axs[row][0].set_title(f\"Waveform - Label: {label} - Idx: {train_dataloader.dataset.labels[idx]}\")\n",
    "    axs[row][0].set_xlabel(\"Time\")\n",
    "    axs[row][0].set_ylabel(\"Amplitude\")\n",
    "    axs[row][0].set_xticks([])\n",
    "    axs[row][0].set_yticks([])\n",
    "\n",
    "    # Plot spectrogram\n",
    "    axs[row][1].imshow(spectrogram.permute(1, 2, 0).detach().numpy(), aspect='auto', origin='lower', cmap='magma')\n",
    "    axs[row][1].set_title(f\"Spectrogram - Label: {label} - Idx: {train_dataloader.dataset.labels[idx]}\")\n",
    "    axs[row][1].set_xlabel(\"Time\")\n",
    "    axs[row][1].set_ylabel(\"Frequency\")\n",
    "    axs[row][1].set_xticks([])\n",
    "    axs[row][1].set_yticks([])\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3015b2ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Play out some audio files\n",
    "try:\n",
    "    waveform_first, _ = load_audio(train_dataloader.dataset.files[0])\n",
    "except:\n",
    "    waveform_first, _ = load_audio(train_dataloader.dataset.dataset.files[0])\n",
    "ipd.Audio(waveform_first.numpy(), rate=sample_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c5e0da8",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    waveform_second, *_ = load_audio(train_dataloader.dataset.files[1])\n",
    "except:\n",
    "    waveform_second, *_ = load_audio(train_dataloader.dataset.dataset.files[1])\n",
    "ipd.Audio(waveform_second.numpy(), rate=sample_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4530ea7",
   "metadata": {},
   "source": [
    "# 7. Creating the Transformer Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a11c83a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_vit(\n",
    "    vit_model=\"vitbase16_2\",\n",
    "    num_classes=NUM_CLASSES,\n",
    "    dropout=0.1,\n",
    "    seed=SEED,\n",
    "    device=device\n",
    "    )\n",
    "\n",
    "# Unfreeze the base parameters\n",
    "for parameter in model.parameters():\n",
    "    parameter.requires_grad = True\n",
    "\n",
    "# Compile model\n",
    "model = torch.compile(model, backend=\"aot_eager\")\n",
    "\n",
    "# Send model to device\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28a1291e",
   "metadata": {},
   "source": [
    "# 8. Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa1218f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "EPOCHS = 20\n",
    "LR = 1e-4\n",
    "ETA_MIN = 1e-6\n",
    "model_type=\"model_spectrogram\"\n",
    "model_name = model_type + \".pth\"\n",
    "\n",
    "# Create AdamW optimizer\n",
    "optimizer = torch.optim.AdamW(\n",
    "    params=model.parameters(),\n",
    "    lr=LR,\n",
    "    weight_decay=0.0001\n",
    ")\n",
    "\n",
    "# Create loss function\n",
    "loss_fn = torch.nn.CrossEntropyLoss(label_smoothing=0.1)\n",
    "\n",
    "# Set scheduler\n",
    "#scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.1)\n",
    "scheduler = FixedLRSchedulerWrapper(\n",
    "    scheduler=CosineAnnealingLR(optimizer, T_max=20, eta_min=ETA_MIN),\n",
    "    fixed_lr=ETA_MIN,\n",
    "    fixed_epoch=20)\n",
    "\n",
    "# Set seeds\n",
    "set_seeds(SEED)\n",
    "\n",
    "# And train...\n",
    "\n",
    "# Instantiate the classification engine with the created model and the target device\n",
    "engine = ClassificationEngine(\n",
    "    model=model,\n",
    "    #color_map={'train': 'red', 'test': 'yellow', 'other': 'black'},\n",
    "    log_verbose=True,\n",
    "    device=device)\n",
    "\n",
    "# Configure the training method\n",
    "results = engine.train(\n",
    "    target_dir=MODEL_DIR,                       # Directory where the model will be saved\n",
    "    model_name=model_name,                      # Name of the model\n",
    "    save_best_model=[\"loss\", \"acc\", \"pauc\"],    # Save the best models based on different criteria\n",
    "    keep_best_models_in_memory=False,           # Do not keep the models stored in memory for the sake of training time and memory efficiency\n",
    "    train_dataloader=train_dataloader,          # Train dataloader\n",
    "    test_dataloader=test_dataloader,            # Validation/test dataloader\n",
    "    apply_validation=True,                      # Enable validation step\n",
    "    num_classes=NUM_CLASSES,                    # Number of classes\n",
    "    optimizer=optimizer,                        # Optimizer\n",
    "    loss_fn=loss_fn,                            # Loss function\n",
    "    recall_threshold=1.0,                       # False positive rate at recall_threshold recall\n",
    "    recall_threshold_pauc=0.0,                  # Partial AUC score above recall_threshold_pauc recall\n",
    "    scheduler=scheduler,                        # Scheduler\n",
    "    epochs=EPOCHS,                              # Total number of epochs\n",
    "    amp=True,                                   # Enable Automatic Mixed Precision (AMP)\n",
    "    enable_clipping=False,                      # Disable clipping on gradients, only useful if training becomes unestable\n",
    "    debug_mode=False,                           # Disable debug mode    \n",
    "    accumulation_steps=ACCUM_STEPS              # Accumulation steps 2: effective batch size = batch_size x accumulation steps\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f53934b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions by manually loading the best model\n",
    "if device == \"cuda\":\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "model = create_vit(\n",
    "    vit_model=\"vitbase16_2\",\n",
    "    num_classes=NUM_CLASSES,\n",
    "    dropout=0.1,\n",
    "    seed=SEED,\n",
    "    device=device\n",
    "    )\n",
    "\n",
    "# Compile model\n",
    "model = torch.compile(model, backend=\"aot_eager\")\n",
    "\n",
    "# Find the file that matchs the pattern `_pauc_`\n",
    "model_file = glob.glob(os.path.join(MODEL_DIR, \"model_spectrogram_acc_*.pth\"))\n",
    "model_name = os.path.basename(model_file[0])\n",
    "\n",
    "# Instantiate engine for predictions\n",
    "engine2 = ClassificationEngine(\n",
    "        model=model,        \n",
    "        log_verbose=True,\n",
    "        device=device)\n",
    "\n",
    "engine2.load(target_dir=MODEL_DIR, model_name=model_name)\n",
    "#indexes2 = engine2.predict(\n",
    "#    dataloader=test_dataloader,\n",
    "#    output_type='argmax').tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "610eb67e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load now the model and assign it to `model`\n",
    "if device == \"cuda\":\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "# Find the file that matchs the pattern `_pauc_`\n",
    "model_file = glob.glob(os.path.join(MODEL_DIR, \"model_spectrogram_acc_*.pth\"))\n",
    "model_name = os.path.basename(model_file[0])\n",
    "    \n",
    "model = create_vit(\n",
    "    vit_model=\"vitbase16_2\",\n",
    "    num_classes=NUM_CLASSES,\n",
    "    dropout=0.1,\n",
    "    seed=42,\n",
    "    device=device\n",
    "    )\n",
    "\n",
    "model = torch.compile(model, backend=\"aot_eager\")\n",
    "model = load_model(model, MODEL_DIR, model_name)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "108f2d42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get 24 random indexes from the test dataset\n",
    "num_samples = 24\n",
    "random_indices = random.sample(range(len(test_dataloader.dataset)), num_samples)\n",
    "\n",
    "# Load audio files and get predictions\n",
    "waveform_list = []\n",
    "label_list = []\n",
    "sample_rate_list = []\n",
    "for idx in random_indices:\n",
    "    \n",
    "    # Load waveform and label\n",
    "    try:\n",
    "        waveform, sample_rate = load_audio(test_dataloader.dataset.files[idx])\n",
    "        actual_label = class_names[test_dataloader.dataset.labels[idx]]\n",
    "    except:\n",
    "        waveform, sample_rate = load_audio(test_dataloader.dataset.dataset.files[idx])\n",
    "        actual_label = class_names[test_dataloader.dataset.dataset.labels[idx]]\n",
    "\n",
    "    # Append data\n",
    "    waveform_list.append(waveform)\n",
    "    label_list.append(actual_label)\n",
    "    sample_rate_list.append(sample_rate)\n",
    "\n",
    "# Predict and play back\n",
    "predict_and_play_audio(\n",
    "    model=model,\n",
    "    waveform_list=waveform_list,\n",
    "    label_list=label_list,\n",
    "    sample_rate_list=sample_rate_list,\n",
    "    class_names=class_names,\n",
    "    transform=get_transform_test,\n",
    "    device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc38f1df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a classification report \n",
    "pred_list, classif_report = engine2.predict_and_store(\n",
    "    test_dir=TEST_DIR,\n",
    "    transform=get_transform_test,\n",
    "    class_names=class_names,\n",
    "    sample_fraction=1,\n",
    "    seed=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5528210d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(classif_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afa23c06",
   "metadata": {},
   "outputs": [],
   "source": [
    "speed = round(1.0 / pd.DataFrame(pred_list)['time_for_pred'].mean(), 2)\n",
    "print(f'GPU: Predicted Images per Sec [fps]: {speed}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fd671f6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
