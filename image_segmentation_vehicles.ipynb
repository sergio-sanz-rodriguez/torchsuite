{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Introduction\n",
    "This notebook aims to develop and train a U-Net–based deep learning model for detecting and segmenting vehicles such as cars, buses, and trucks in images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. The U-Net Architecture\n",
    "U-Net is one of the most common deep learning architectures for segmentation tasks. The block diagram of this model is depicted in the figure below (https://arxiv.org/abs/1505.04597):\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"images/u-net-architecture.png\" width=\"600\"/>\n",
    "</div>\n",
    "\n",
    "The architecture features a \"U\" shape, consisting of two main stages: the contracting (encoder) and expansive (decoder) stages.\n",
    "\n",
    "* The **encoder** captures the context and high-level features of the input image by using several convolutional layers. It gradually reduces the spatial dimensions while increasing the feature dimensions.\n",
    "\n",
    "* The **decoder** is responsible for reconstructing the output image, which, in the case of segmentation, is the mask that identifies the objects of interest. This stage involves upsampling across the same number of levels as the encoder, followed by convolutional operations to \"expand\" the contracted image.\n",
    "\n",
    "One of the unique characteristics of U-Net is its **skip connections**, which link the encoder and decoder stages at each level by merging features. While the contracting and expanding paths (\"U\" shape) capture high-level contextual information, the skip connections help preserve low-level spatial details that might be lost during downsampling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generic libraries\n",
    "import os\n",
    "import torch\n",
    "import json\n",
    "import zipfile\n",
    "import urllib\n",
    "import random\n",
    "import glob\n",
    "import torch.backends.cudnn as cudnn\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from torchinfo import summary\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR, CosineAnnealingLR, SequentialLR, ConstantLR\n",
    "\n",
    "# Import custom libraries\n",
    "from utils.obj_detection_utils import collate_fn\n",
    "from utils.segmentation_utils import display_image_with_mask, collapse_one_hot_mask, create_label_class_dict\n",
    "from engines.segmentation import SegmentationEngine\n",
    "from engines.schedulers import FixedLRSchedulerWrapper\n",
    "from engines.loss_functions import DiceCrossEntropyLoss\n",
    "from dataloaders.segmentation_dataloaders import ProcessDatasetSegmentation, SegmentationTransforms\n",
    "from models.unet import create_unet\n",
    "\n",
    "# Import custom libraries\n",
    "from utils.common_utils import set_seeds\n",
    "from utils.coco_dataset_utils import COCO_2_ImgMsk, split_dataset\n",
    "\n",
    "# Warnings\n",
    "import warnings\n",
    "os.environ['TORCH_USE_CUDA_DSA'] = \"1\"\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"torch.autograd.graph\")\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning, module=\"onnxscript.converter\")\n",
    "\n",
    "import torch._dynamo\n",
    "#torch._dynamo.config.suppress_errors = True\n",
    "torch._dynamo.config.cache_size_limit = 64\n",
    "#warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"torch._dynamo\")\n",
    "\n",
    "# Create target model directory\n",
    "MODEL_DIR = Path(\"outputs\")\n",
    "MODEL_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Set seeds\n",
    "SEED = 42\n",
    "set_seeds(SEED)\n",
    "\n",
    "DOWNLOAD_COCO = False\n",
    "PROCESS_COCO = False\n",
    "VISUALIZE_TRANSFORMED_DATA = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Specifying the Target Device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activate cuda benchmark\n",
    "#cudnn.benchmark = True\n",
    "\n",
    "# Set device\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Device: {device}\")\n",
    "\n",
    "if device == \"cuda\":\n",
    "    !nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Importing the COCO Image Dataset\n",
    "This section converts the COCO dataset for vehicle detection and segmentation into a structured format. The process involves:\n",
    "\n",
    "* Filtering the dataset to retain categories such as 'bus', 'car', and 'truck'.\n",
    "* Storing original images and corresponding segmentation masks in separate folders.\n",
    "* Splitting the dataset into three subsets: training, validation, and testing.\n",
    "\n",
    "By applying this conversion, the COCO dataset can be used alongside the PennFudanPed dataset for pedestrian detection and segmentation tasks.\n",
    "\n",
    "After this section, the dataset will be organized into three subdirectories — train, val, test — each containing two subdirectories: PNGImages and PedMasks.\n",
    "\n",
    "```\n",
    "driving/\n",
    "├── train/\n",
    "│   └── PNGImages/\n",
    "│       ├── img1.png\n",
    "│       ├── img2.png\n",
    "│       └── ...\n",
    "│   └── PedMasks/\n",
    "│       ├── msk1.png\n",
    "│       ├── msk2.png\n",
    "│       └── ...\n",
    "├── val/\n",
    "│   └── PNGImages/\n",
    "│       ├── img1.png\n",
    "│       ├── img2.png\n",
    "│       └── ...\n",
    "│   └── PedMasks/\n",
    "│       ├── msk1.png\n",
    "│       ├── msk2.png\n",
    "│       └── ...\n",
    "├── test/\n",
    "│   └── PNGImages/\n",
    "│       ├── img1.png\n",
    "│       ├── img2.png\n",
    "│       └── ...\n",
    "│   └── PedMasks/\n",
    "│       ├── msk1.png\n",
    "│       ├── msk2.png\n",
    "│       └── ...\n",
    "```\n",
    "\n",
    "**Note:** 25 GB disk is required to download the complete dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1. Downloading the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DOWNLOAD_COCO:\n",
    "    # Define download URLs\n",
    "    coco_urls = {\n",
    "        \"val_images\": \"http://images.cocodataset.org/zips/val2017.zip\",\n",
    "        \"test_images\": \"http://images.cocodataset.org/zips/test2017.zip\",\n",
    "        \"train_images\": \"http://images.cocodataset.org/zips/train2017.zip\",\n",
    "        \"annotations\": \"http://images.cocodataset.org/annotations/annotations_trainval2017.zip\"\n",
    "    }\n",
    "\n",
    "    # Create a directory to store the dataset\n",
    "    dataset_dir = \"d:/Repos/coco_dataset\"\n",
    "    os.makedirs(dataset_dir, exist_ok=True)\n",
    "\n",
    "    # Download function\n",
    "    def download_coco(url, filename):\n",
    "        filepath = os.path.join(dataset_dir, filename)\n",
    "        if not os.path.exists(filepath):\n",
    "            print(f\"Downloading {filename}...\")\n",
    "            urllib.request.urlretrieve(url, filepath)\n",
    "            print(f\"Saved to {filepath}\")\n",
    "        else:\n",
    "            print(f\"{filename} already exists.\")\n",
    "\n",
    "    # Download all files\n",
    "    for key, url in coco_urls.items():\n",
    "        filename = url.split(\"/\")[-1]\n",
    "        download_coco(url, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DOWNLOAD_COCO:\n",
    "    # Unzip the file\n",
    "    PATH = Path(dataset_dir)\n",
    "    \n",
    "    zip_file = PATH / \"val2017.zip\"\n",
    "    with zipfile.ZipFile(zip_file, 'r') as zip_ref:\n",
    "        zip_ref.extractall(dataset_dir)\n",
    "\n",
    "    if zip_file.exists():\n",
    "        os.remove(zip_file)\n",
    "\n",
    "    zip_file = PATH / \"annotations_trainval2017.zip\"\n",
    "    with zipfile.ZipFile(zip_file, 'r') as zip_ref:\n",
    "        zip_ref.extractall(dataset_dir)\n",
    "\n",
    "    if zip_file.exists():\n",
    "        os.remove(zip_file)\n",
    "\n",
    "    zip_file = PATH / \"test2017.zip\"\n",
    "    with zipfile.ZipFile(zip_file, 'r') as zip_ref:\n",
    "        zip_ref.extractall(dataset_dir)\n",
    "\n",
    "    if zip_file.exists():\n",
    "        os.remove(zip_file)\n",
    "\n",
    "    zip_file = PATH / \"train2017.zip\"\n",
    "    with zipfile.ZipFile(zip_file, 'r') as zip_ref:\n",
    "        zip_ref.extractall(dataset_dir)\n",
    "\n",
    "    if zip_file.exists():\n",
    "        os.remove(zip_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2. Processing the Dataset for Vehicle Segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if PROCESS_COCO:\n",
    "    # Path to COCO annotations file\n",
    "    ANNOTATIONS_PATH = r\"D:\\Repos\\coco_dataset\\annotations\\instances_train2017.json\"\n",
    "\n",
    "    # Load COCO annotations\n",
    "    with open(ANNOTATIONS_PATH, \"r\") as f:\n",
    "        coco_data = json.load(f)\n",
    "\n",
    "    # Extract category ID to name mapping\n",
    "    categories = {c[\"id\"]: c[\"name\"] for c in coco_data[\"categories\"]}\n",
    "\n",
    "    # Display all categories\n",
    "    for cat_id, cat_name in categories.items():\n",
    "        print(f\"{cat_id}: {cat_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the dictionary describing label -> category), say in alphabetical order\n",
    "target_categories={\n",
    "    0: 'background',\n",
    "    1: 'bus',\n",
    "    2: 'car',\n",
    "    3: 'truck'\n",
    "}\n",
    "\n",
    "# RGB\n",
    "color_map = {\n",
    "    0: (0, 0, 0),       # Background (Black)\n",
    "    1: (232, 66, 66),   # Bus (Red)\n",
    "    2: (35, 171, 75),   # Car (Green)\n",
    "    3: (28, 163, 218),  # Truck (Blue)   \n",
    "}\n",
    "\n",
    "## Display categories\n",
    "for cat_id, cat_name in target_categories.items():\n",
    "    print(f\"{cat_id}: {cat_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if PROCESS_COCO:\n",
    "    # Training dataset\n",
    "    mapping = COCO_2_ImgMsk(\n",
    "        coco_images_path=      r\"D:\\Repos\\coco_dataset\\train2017\",\n",
    "        coco_annotations_path= r\"D:\\Repos\\coco_dataset\\annotations\\instances_train2017.json\",\n",
    "        output_images_dir=     r\"D:\\Repos\\coco_dataset\\driving\\PNGImages\",\n",
    "        output_masks_dir=      r\"D:\\Repos\\coco_dataset\\driving\\PedMasks\",\n",
    "        class_dictionary=      target_categories,\n",
    "        label=                 \"train\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if PROCESS_COCO:\n",
    "    # Training dataset\n",
    "    _ = COCO_2_ImgMsk(\n",
    "        coco_images_path=      r\"D:\\Repos\\coco_dataset\\val2017\",\n",
    "        coco_annotations_path= r\"D:\\Repos\\coco_dataset\\annotations\\instances_val2017.json\",\n",
    "        output_images_dir=     r\"D:\\Repos\\coco_dataset\\driving\\PNGImages\",\n",
    "        output_masks_dir=      r\"D:\\Repos\\coco_dataset\\driving\\PedMasks\",\n",
    "        class_dictionary=      target_categories,\n",
    "        label=                 \"val\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if PROCESS_COCO:\n",
    "# Split dataset into train (80%), validation (10%), and test (10%) sets\n",
    "    split_dataset(\n",
    "        src_images=       r\"D:\\Repos\\coco_dataset\\driving\\PNGImages\",\n",
    "        src_masks=        r\"D:\\Repos\\coco_dataset\\driving\\PedMasks\",\n",
    "        dst_train_images= r\"D:\\Repos\\ML_Projects\\torchsuite\\data\\driving\\train\\PNGImages\",\n",
    "        dst_train_masks=  r\"D:\\Repos\\ML_Projects\\torchsuite\\data\\driving\\train\\PedMasks\",\n",
    "        dst_val_images=   r\"D:\\Repos\\ML_Projects\\torchsuite\\data\\driving\\val\\PNGImages\",\n",
    "        dst_val_masks=    r\"D:\\Repos\\ML_Projects\\torchsuite\\data\\driving\\val\\PedMasks\",\n",
    "        dst_test_images=  r\"D:\\Repos\\ML_Projects\\torchsuite\\data\\driving\\test\\PNGImages\",\n",
    "        dst_test_masks=   r\"D:\\Repos\\ML_Projects\\torchsuite\\data\\driving\\test\\PedMasks\",\n",
    "        train_pct=        0.80,\n",
    "        val_pct=          0.10,\n",
    "        test_pct=         0.10,\n",
    "        seed=             SEED\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3. Preparing Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The dataset contains two classes only: background and person\n",
    "NUM_CLASSES = len(target_categories)\n",
    "BATCH_SIZE = 4\n",
    "ACCUM_STEPS = 8\n",
    "IMG_SIZE = (384, 384) #(512, 512)\n",
    "AUGMENT_MAGNITUDE = 4 # 1 (low) to 5 (high)\n",
    "THEME = 'light' # or 'dark'. Default is 'light'\n",
    "\n",
    "# Define training, validation, and test data loaders\n",
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "    dataset=ProcessDatasetSegmentation(\n",
    "        root='data/driving/train',\n",
    "        image_path=\"PNGImages\",\n",
    "        mask_path=\"PedMasks\",\n",
    "        transforms=SegmentationTransforms(\n",
    "            train=True,\n",
    "            img_size=IMG_SIZE,\n",
    "            mean_std_norm=True,\n",
    "            augment_magnitude=AUGMENT_MAGNITUDE\n",
    "            ),\n",
    "        class_dictionary=target_categories\n",
    "        ),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    pin_memory=True,\n",
    ")\n",
    "\n",
    "val_dataloader = torch.utils.data.DataLoader(\n",
    "    dataset=ProcessDatasetSegmentation(\n",
    "        root='data/driving/val',\n",
    "        image_path=\"PNGImages\",\n",
    "        mask_path=\"PedMasks\",\n",
    "        transforms=SegmentationTransforms(\n",
    "            train=False, # If train if False, augmentation is not applied\n",
    "            img_size=IMG_SIZE,\n",
    "            mean_std_norm=True,\n",
    "            augment_magnitude=AUGMENT_MAGNITUDE\n",
    "            ),\n",
    "        class_dictionary=target_categories\n",
    "        ),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    pin_memory=True,\n",
    ")\n",
    "\n",
    "test_dataloader = torch.utils.data.DataLoader(\n",
    "    dataset=ProcessDatasetSegmentation(\n",
    "        root='data/driving/test',\n",
    "        image_path=\"PNGImages\",\n",
    "        mask_path=\"PedMasks\",\n",
    "        transforms=SegmentationTransforms(\n",
    "            train=False,\n",
    "            img_size=IMG_SIZE,\n",
    "            mean_std_norm=True,\n",
    "            augment_magnitude=AUGMENT_MAGNITUDE\n",
    "            ),\n",
    "        class_dictionary=target_categories\n",
    "        ),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    pin_memory=True,\n",
    ")\n",
    "\n",
    "dataloaders = {\n",
    "    'train':         train_dataloader,    \n",
    "    'test':          val_dataloader\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.4. Visualizing Images with Masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if VISUALIZE_TRANSFORMED_DATA:\n",
    "\n",
    "    # Visualize transformations\n",
    "    BATCH_SIZE = 64\n",
    "    # Train dataloader without transformations\n",
    "    dataloader_nt = torch.utils.data.DataLoader(\n",
    "        dataset=ProcessDatasetSegmentation(\n",
    "            root='data/driving/train',\n",
    "            image_path=\"PNGImages\",\n",
    "            mask_path=\"PedMasks\",\n",
    "            transforms=SegmentationTransforms(\n",
    "                train=False,\n",
    "                img_size=IMG_SIZE,\n",
    "                mean_std_norm=False,\n",
    "                augment_magnitude=AUGMENT_MAGNITUDE\n",
    "                ),\n",
    "            class_dictionary=target_categories\n",
    "            ), \n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=False,\n",
    "        pin_memory=True,\n",
    "        collate_fn=collate_fn)\n",
    "    \n",
    "    # Test dataloader with transformations\n",
    "    dataloader_t = torch.utils.data.DataLoader(\n",
    "        dataset=ProcessDatasetSegmentation(\n",
    "            root='data/driving/train',\n",
    "            image_path=\"PNGImages\",\n",
    "            mask_path=\"PedMasks\",\n",
    "            transforms=SegmentationTransforms(\n",
    "                train=True,\n",
    "                img_size=IMG_SIZE,\n",
    "                mean_std_norm=False,\n",
    "                augment_magnitude=AUGMENT_MAGNITUDE\n",
    "                ),\n",
    "            class_dictionary=target_categories\n",
    "            ), \n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=False,\n",
    "        pin_memory=True,\n",
    "        collate_fn=collate_fn)\n",
    "    \n",
    "    random.seed(SEED+1)\n",
    "\n",
    "    # Visualize images and masks with and without transformations    \n",
    "    for idx, ((img_nt, target_nt), (img_t, target_t)) in enumerate(zip(dataloader_nt, dataloader_t)):   \n",
    "\n",
    "        # Pick random images\n",
    "        random_indices = random.sample(range(BATCH_SIZE), min(10, BATCH_SIZE))\n",
    "        for i in random_indices:\n",
    "\n",
    "            # Set up the figure\n",
    "            fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "            \n",
    "            # Pass subplot axes to the function\n",
    "            mask_nt = collapse_one_hot_mask(target_nt[i])\n",
    "            mask_t = collapse_one_hot_mask(target_t[i])\n",
    "\n",
    "            # Create the label-class dictionary for the mask\n",
    "            classes_nt = create_label_class_dict(target_nt[i], target_categories)\n",
    "            classes_t = create_label_class_dict(target_t[i], target_categories)\n",
    "\n",
    "            # Remove background, as it is always there\n",
    "            classes_nt = dict(list(classes_nt.items())[1:])\n",
    "            classes_t = dict(list(classes_t.items())[1:])\n",
    "\n",
    "            # And generate the titles\n",
    "            title_nt = f\"Original: {', '.join(classes_nt.values())}\"\n",
    "            title_t = f\"Transformed: {', '.join(classes_t.values())}\"\n",
    "            \n",
    "            # Display overlaid images\n",
    "            alpha, beta = 1.0, 0.5\n",
    "            display_image_with_mask(img_nt[i], mask_nt, fig=fig, ax=axes[0], alpha=alpha, beta=beta, color_map=color_map, title=title_nt, theme=THEME)\n",
    "            display_image_with_mask(img_t[i], mask_t, fig=fig, ax=axes[1], alpha=alpha, beta=beta, color_map=color_map, title=title_t, theme=THEME)\n",
    "            \n",
    "            plt.show() \n",
    "            \n",
    "        if idx > -1:\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Creating the U-Net Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the model\n",
    "model = create_unet(\n",
    "    model_type=\"pretrained\",\n",
    "    backbone='convnext_large_384_in22ft1k', #'convnext_small_in22ft1k', #\n",
    "    in_channels=3,\n",
    "    num_classes=NUM_CLASSES,\n",
    "    print_available_models=True,\n",
    "    pretrained=True,\n",
    "    encoder_freeze=False\n",
    ")\n",
    "\n",
    "# Unfreeze the base parameters\n",
    "for parameter in model.parameters():\n",
    "    parameter.requires_grad = True\n",
    "\n",
    "#summary(model,\n",
    "#        input_size=(BATCH_SIZE,3, IMG_SIZE[0], IMG_SIZE[1]),\n",
    "#        col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
    "#        col_width=20,\n",
    "#        row_settings=[\"var_names\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "EPOCHS = 35\n",
    "LR = 1e-4\n",
    "ETAMIN = 1e-6\n",
    "model_type=\"model_seg_convnext\"\n",
    "model_name = model_type + \".pth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Create AdamW optimizer\n",
    "optimizer = torch.optim.AdamW(\n",
    "    params=model.parameters(),\n",
    "    lr=LR,\n",
    "    betas=(0.9, 0.999),\n",
    "    weight_decay=1e-4\n",
    "    )\n",
    "\n",
    "# Create loss function\n",
    "#loss_fn = torch.nn.CrossEntropyLoss(label_smoothing=0.1)\n",
    "loss_fn = DiceCrossEntropyLoss(\n",
    "    num_classes=NUM_CLASSES,\n",
    "    label_smoothing=0.1\n",
    "    )\n",
    "\n",
    "# Set scheduler\n",
    "cosine = CosineAnnealingLR(optimizer, T_max=EPOCHS-5, eta_min=ETAMIN)\n",
    "fixed = ConstantLR(optimizer, factor=ETAMIN/LR, total_iters=5)\n",
    "scheduler = SequentialLR(\n",
    "    optimizer,\n",
    "    schedulers=[cosine, fixed],\n",
    "    milestones=[EPOCHS-5]\n",
    ")\n",
    "# Or (it is equivalent)\n",
    "#scheduler = FixedLRSchedulerWrapper(\n",
    "#    scheduler=CosineAnnealingLR(optimizer, T_max=EPOCHS-5, eta_min=1e-6),\n",
    "#    fixed_lr=1e-6,\n",
    "#    fixed_epoch=EPOCHS-5\n",
    "#    )\n",
    "\n",
    "# And train...\n",
    "\n",
    "# Instantiate the classification engine with the created model and the target device\n",
    "engine = SegmentationEngine(\n",
    "    model=model,                                # Model to be trained\n",
    "    optimizer=optimizer,                        # Optimizer\n",
    "    loss_fn=loss_fn,                            # Loss function\n",
    "    scheduler=scheduler,                        # Scheduler     \n",
    "    theme=THEME,                                # Theme\n",
    "    log_verbose=True,                           # Verbosity\n",
    "    device=device                               # Target device\n",
    "    )\n",
    "\n",
    "# Configure the training method\n",
    "results = engine.train(\n",
    "    target_dir=MODEL_DIR,                       # Directory where the model will be saved\n",
    "    model_name=model_name,                      # Name of the model\n",
    "    enable_resume=True,                         # Resume training from the last saved checkpoint\n",
    "    save_best_model=[\n",
    "        \"last\", \"loss\", \"dice\", \"iou\"],         # Save the best models based on different criteria\n",
    "    keep_best_models_in_memory=False,           # If False: do not keep the models stored in memory for the sake of training time and memory efficiency\n",
    "    dataloaders=dataloaders,                    # Dictionary with the dataloaders     \n",
    "    apply_validation=True,                      # Enable validation step\n",
    "    augmentation_strategy=\"always\",             # Augmentation strategy        \n",
    "    epochs=EPOCHS,                              # Total number of epochs\n",
    "    amp=True,                                   # Enable Automatic Mixed Precision (AMP)\n",
    "    enable_clipping=False,                      # Disable clipping on gradients, only useful if training becomes unestable\n",
    "    debug_mode=False,                           # Disable debug mode    \n",
    "    accumulation_steps=ACCUM_STEPS,             # Accumulation steps: effective batch size = batch_size x accumulation steps\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Making Predictions on Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os.path import exists\n",
    "def rename_model(model_name: str, new_name: str):\n",
    "    old_name = model_name[0]\n",
    "    os.rename(old_name, new_name)\n",
    "    print(f\"Renamed {old_name} to {new_name}\")\n",
    "    \n",
    "# Find the model file with \"model_1_loss_epoch\" prefix and rename it\n",
    "new_model_name = str(MODEL_DIR / f\"{model_name}\")\n",
    "if not exists(new_model_name):\n",
    "    model_name_dice = glob.glob(str(MODEL_DIR / f\"{model_type}_dice_epoch*.pth\"))\n",
    "    rename_model(model_name_dice, new_model_name)\n",
    "\n",
    "# Instantiate the classification engine with the created model and the target device\n",
    "engine = SegmentationEngine(\n",
    "    model=model,    \n",
    "    log_verbose=True,\n",
    "    device=device\n",
    "    ).load(\n",
    "        target_dir=MODEL_DIR,\n",
    "        model_name=Path(new_model_name).name\n",
    "    )\n",
    "\n",
    "preds = engine.predict(\n",
    "    dataloader=test_dataloader,\n",
    "    num_classes=NUM_CLASSES,\n",
    "    model_state=\"last\", # This option takes the default loaded model\n",
    "    output_type=\"onehot\" #one-hot binary masks: (num_batches, batch_size, num_classes, W, H)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Shape of the prediction tensor: {preds.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize images and masks with and without transformations\n",
    "BATCH_SIZE = test_dataloader.batch_size\n",
    "\n",
    "# Create a test dataloader without mean/std transformation and augmentations\n",
    "test_dataloader_nt = torch.utils.data.DataLoader(\n",
    "    dataset=ProcessDatasetSegmentation(\n",
    "        root='data/driving/test',\n",
    "        image_path=\"PNGImages\",\n",
    "        mask_path=\"PedMasks\",\n",
    "        transforms=SegmentationTransforms(\n",
    "            train=False,\n",
    "            img_size=IMG_SIZE,\n",
    "            mean_std_norm=False,\n",
    "            augment_magnitude=AUGMENT_MAGNITUDE\n",
    "            ),\n",
    "        class_dictionary=target_categories\n",
    "        ),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    pin_memory=True,\n",
    ")\n",
    "\n",
    "for batch, (img, target) in enumerate(test_dataloader_nt):\n",
    "\n",
    "    # Pick random images\n",
    "    random_indices = random.sample(range(BATCH_SIZE), min(2, BATCH_SIZE))\n",
    "    for i in random_indices:\n",
    "\n",
    "        # Set up the figure\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "        \n",
    "        # Pass subplot axes to the function\n",
    "        mask_true = collapse_one_hot_mask(target[i])\n",
    "        mask_pred = collapse_one_hot_mask(preds[batch][i])\n",
    "\n",
    "        # Create the label-class dictionary for the mask\n",
    "        classes_true = create_label_class_dict(target[i], target_categories)\n",
    "        classes_pred = create_label_class_dict(preds[batch][i], target_categories)\n",
    "\n",
    "        # classes_true background, as it is always there\n",
    "        classes_true = dict(list(classes_true.items())[1:])\n",
    "        classes_pred = dict(list(classes_pred.items())[1:])\n",
    "\n",
    "        # And generate the titles\n",
    "        title_true = f\"Ground-Truth: {', '.join(classes_true.values())}\"\n",
    "        title_pred = f\"Predictions: {', '.join(classes_pred.values())}\"\n",
    "        \n",
    "        # Display overlaid images\n",
    "        alpha, beta = 1.0, 0.5\n",
    "        display_image_with_mask(img[i], mask_true, fig=fig, ax=axes[0], alpha=alpha, beta=beta, color_map=color_map, title=title_true, theme=THEME)\n",
    "        display_image_with_mask(img[i], mask_pred, fig=fig, ax=axes[1], alpha=alpha, beta=beta, color_map=color_map, title=title_pred, theme=THEME)\n",
    "        \n",
    "        plt.show() \n",
    "        \n",
    "    if batch > 5:\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv_pytorch_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
