{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Introduction\n",
    "This notebook provides the necessary steps to convert the COCO dataset for object detection and segmentation into a folder structure. The original images and masks are stored in separate folders, and the dataset is then split into three categories: training, validation, and testing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generic libraries\n",
    "import os\n",
    "import json\n",
    "import urllib.request\n",
    "import zipfile\n",
    "from pathlib import Path\n",
    "\n",
    "# Torchvision libraries\n",
    "from torchvision import datasets\n",
    "\n",
    "# Import custom libraries\n",
    "from utils.classification_utils import set_seeds\n",
    "from utils.coco_dataset_utils import COCO_2_ImgMsk, select_and_copy_samples, split_dataset\n",
    "\n",
    "# Warnings\n",
    "import warnings\n",
    "os.environ['TORCH_USE_CUDA_DSA'] = \"1\"\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"torch.autograd.graph\")\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning, module=\"onnxscript.converter\")\n",
    "\n",
    "# Create target model directory\n",
    "MODEL_DIR = Path(\"outputs\")\n",
    "MODEL_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Set seeds\n",
    "set_seeds(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Downloading the COCO Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define download URLs\n",
    "coco_urls = {\n",
    "    \"val_images\": \"http://images.cocodataset.org/zips/val2017.zip\",\n",
    "    \"test_images\": \"http://images.cocodataset.org/zips/test2017.zip\",\n",
    "    \"train_images\": \"http://images.cocodataset.org/zips/train2017.zip\",\n",
    "    \"annotations\": \"http://images.cocodataset.org/annotations/annotations_trainval2017.zip\"\n",
    "}\n",
    "\n",
    "# Create a directory to store the dataset\n",
    "dataset_dir = \"d:/Repos/coco_dataset\"\n",
    "os.makedirs(dataset_dir, exist_ok=True)\n",
    "\n",
    "# Download function\n",
    "def download_coco(url, filename):\n",
    "    filepath = os.path.join(dataset_dir, filename)\n",
    "    if not os.path.exists(filepath):\n",
    "        print(f\"Downloading {filename}...\")\n",
    "        urllib.request.urlretrieve(url, filepath)\n",
    "        print(f\"Saved to {filepath}\")\n",
    "    else:\n",
    "        print(f\"{filename} already exists.\")\n",
    "\n",
    "# Download all files\n",
    "for key, url in coco_urls.items():\n",
    "    filename = url.split(\"/\")[-1]\n",
    "    download_coco(url, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unzip the file\n",
    "PATH = Path(dataset_dir)\n",
    "\n",
    "zip_file = PATH / \"val2017.zip\"\n",
    "with zipfile.ZipFile(zip_file, 'r') as zip_ref:\n",
    "    zip_ref.extractall(dataset_dir)\n",
    "\n",
    "if zip_file.exists():\n",
    "    os.remove(zip_file)\n",
    "\n",
    "zip_file = PATH / \"annotations_trainval2017.zip\"\n",
    "with zipfile.ZipFile(zip_file, 'r') as zip_ref:\n",
    "    zip_ref.extractall(dataset_dir)\n",
    "\n",
    "if zip_file.exists():\n",
    "    os.remove(zip_file)\n",
    "\n",
    "zip_file = PATH / \"test2017.zip\"\n",
    "with zipfile.ZipFile(zip_file, 'r') as zip_ref:\n",
    "    zip_ref.extractall(dataset_dir)\n",
    "\n",
    "if zip_file.exists():\n",
    "    os.remove(zip_file)\n",
    "\n",
    "zip_file = PATH / \"train2017.zip\"\n",
    "with zipfile.ZipFile(zip_file, 'r') as zip_ref:\n",
    "    zip_ref.extractall(dataset_dir)\n",
    "\n",
    "if zip_file.exists():\n",
    "    os.remove(zip_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.67s)\n",
      "creating index...\n",
      "index created!\n",
      "type(img) = <class 'PIL.Image.Image'>\n",
      "type(target) = <class 'list'>\n",
      "type(target[0]) = <class 'dict'>\n",
      "target[0].keys() = dict_keys(['segmentation', 'area', 'iscrowd', 'image_id', 'bbox', 'category_id', 'id'])\n"
     ]
    }
   ],
   "source": [
    "# Define paths\n",
    "IMAGES_PATH = r\"D:\\Repos\\coco_dataset\\val2017\"  # Path to validation images\n",
    "ANNOTATIONS_PATH = r\"D:\\Repos\\coco_dataset\\annotations\\instances_val2017.json\"  # Path to annotation file\n",
    "\n",
    "# Load dataset\n",
    "dataset = datasets.CocoDetection(root=IMAGES_PATH, annFile=ANNOTATIONS_PATH)\n",
    "\n",
    "# Fetch a sample\n",
    "sample = dataset[0]\n",
    "img, target = sample\n",
    "\n",
    "# Print types and structure of target annotations\n",
    "print(f\"{type(img) = }\")          # Should be <class 'PIL.Image.Image'>\n",
    "print(f\"{type(target) = }\")       # Should be <class 'list'>\n",
    "print(f\"{type(target[0]) = }\")    # Should be <class 'dict'>\n",
    "print(f\"{target[0].keys() = }\")   # Should contain keys like 'segmentation', 'bbox', 'category_id', etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type(img) = <class 'PIL.Image.Image'>\n",
      "type(target) = <class 'dict'>\n",
      "target.keys() = dict_keys(['boxes', 'masks', 'labels'])\n",
      "type(target['boxes']) = <class 'torchvision.tv_tensors._bounding_boxes.BoundingBoxes'>\n",
      "type(target['labels']) = <class 'torch.Tensor'>\n",
      "type(target['masks']) = <class 'torchvision.tv_tensors._mask.Mask'>\n"
     ]
    }
   ],
   "source": [
    "dataset = datasets.wrap_dataset_for_transforms_v2(dataset, target_keys=(\"boxes\", \"labels\", \"masks\"))\n",
    "\n",
    "sample = dataset[0]\n",
    "img, target = sample\n",
    "print(f\"{type(img) = }\\n{type(target) = }\\n{target.keys() = }\")\n",
    "print(f\"{type(target['boxes']) = }\\n{type(target['labels']) = }\\n{type(target['masks']) = }\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Processing the COCO Dataset: Train, Validation, Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1: person\n",
      "2: bicycle\n",
      "3: car\n",
      "4: motorcycle\n",
      "5: airplane\n",
      "6: bus\n",
      "7: train\n",
      "8: truck\n",
      "9: boat\n",
      "10: traffic light\n",
      "11: fire hydrant\n",
      "13: stop sign\n",
      "14: parking meter\n",
      "15: bench\n",
      "16: bird\n",
      "17: cat\n",
      "18: dog\n",
      "19: horse\n",
      "20: sheep\n",
      "21: cow\n",
      "22: elephant\n",
      "23: bear\n",
      "24: zebra\n",
      "25: giraffe\n",
      "27: backpack\n",
      "28: umbrella\n",
      "31: handbag\n",
      "32: tie\n",
      "33: suitcase\n",
      "34: frisbee\n",
      "35: skis\n",
      "36: snowboard\n",
      "37: sports ball\n",
      "38: kite\n",
      "39: baseball bat\n",
      "40: baseball glove\n",
      "41: skateboard\n",
      "42: surfboard\n",
      "43: tennis racket\n",
      "44: bottle\n",
      "46: wine glass\n",
      "47: cup\n",
      "48: fork\n",
      "49: knife\n",
      "50: spoon\n",
      "51: bowl\n",
      "52: banana\n",
      "53: apple\n",
      "54: sandwich\n",
      "55: orange\n",
      "56: broccoli\n",
      "57: carrot\n",
      "58: hot dog\n",
      "59: pizza\n",
      "60: donut\n",
      "61: cake\n",
      "62: chair\n",
      "63: couch\n",
      "64: potted plant\n",
      "65: bed\n",
      "67: dining table\n",
      "70: toilet\n",
      "72: tv\n",
      "73: laptop\n",
      "74: mouse\n",
      "75: remote\n",
      "76: keyboard\n",
      "77: cell phone\n",
      "78: microwave\n",
      "79: oven\n",
      "80: toaster\n",
      "81: sink\n",
      "82: refrigerator\n",
      "84: book\n",
      "85: clock\n",
      "86: vase\n",
      "87: scissors\n",
      "88: teddy bear\n",
      "89: hair drier\n",
      "90: toothbrush\n"
     ]
    }
   ],
   "source": [
    "# Path to COCO annotations file\n",
    "ANNOTATIONS_PATH = r\"D:\\Repos\\coco_dataset\\annotations\\instances_val2017.json\"\n",
    "\n",
    "# Load COCO annotations\n",
    "with open(ANNOTATIONS_PATH, \"r\") as f:\n",
    "    coco_data = json.load(f)\n",
    "\n",
    "# Extract category ID to name mapping\n",
    "categories = {c[\"id\"]: c[\"name\"] for c in coco_data[\"categories\"]}\n",
    "\n",
    "# Display all categories\n",
    "for cat_id, cat_name in categories.items():\n",
    "    print(f\"{cat_id}: {cat_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n"
     ]
    }
   ],
   "source": [
    "# Training dataset\n",
    "COCO_2_ImgMsk(\n",
    "    coco_images_path=r\"D:\\Repos\\coco_dataset\\train2017\",\n",
    "    coco_annotations_path=r\"D:\\Repos\\coco_dataset\\annotations\\instances_train2017.json\",\n",
    "    output_images_dir=r\"D:\\Repos\\coco_dataset\\coco_pennfudan\\PNGImages\",\n",
    "    output_masks_dir=r\"D:\\Repos\\coco_dataset\\coco_pennfudan\\PedMasks\",\n",
    "    label=\"train\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.43s)\n",
      "creating index...\n",
      "index created!\n",
      "Dataset conversion completed: 2693 images & masks saved.\n"
     ]
    }
   ],
   "source": [
    "# Validation and testing dataset\n",
    "COCO_2_ImgMsk(\n",
    "    coco_images_path=r\"D:\\Repos\\coco_dataset\\val2017\",\n",
    "    coco_annotations_path=r\"D:\\Repos\\coco_dataset\\annotations\\instances_val2017.json\",\n",
    "    output_images_dir=r\"D:\\Repos\\coco_dataset\\coco_pennfudan\\PNGImages\",\n",
    "    output_masks_dir=r\"D:\\Repos\\coco_dataset\\coco_pennfudan\\PedMasks\",\n",
    "    label=\"val\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully copied 15000 image-mask pairs to new folders.\n"
     ]
    }
   ],
   "source": [
    "# Subset of training dataset\n",
    "select_and_copy_samples(\n",
    "    input_images_dir=r\"D:\\Repos\\coco_dataset\\coco_pennfudan\\PNGImages\",\n",
    "    input_masks_dir=r\"D:\\Repos\\coco_dataset\\coco_pennfudan\\PedMasks\",\n",
    "    output_images_dir=r\"D:\\Repos\\coco_dataset\\coco_pennfudan_sampled\\PNGImages\",\n",
    "    output_masks_dir=r\"D:\\Repos\\coco_dataset\\coco_pennfudan_sampled\\PedMasks\",\n",
    "    num_samples=15000,\n",
    "    seed=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "split_dataset(\n",
    "    src_images=r\"D:\\Repos\\coco_dataset\\coco_pennfudan_sampled\\PNGImages\",\n",
    "    src_masks=r\"D:\\Repos\\coco_dataset\\coco_pennfudan_sampled\\PedMasks\",\n",
    "    dst_train_images= r\"C:\\Users\\ssre_\\Projects\\torchsuite\\data\\coco_pennfudan_sampled_train\\PNGImages\",\n",
    "    dst_train_masks=  r\"C:\\Users\\ssre_\\Projects\\torchsuite\\data\\coco_pennfudan_sampled_train\\PedMasks\",\n",
    "    dst_val_images=   r\"C:\\Users\\ssre_\\Projects\\torchsuite\\data\\coco_pennfudan_sampled_val\\PNGImages\",\n",
    "    dst_val_masks=    r\"C:\\Users\\ssre_\\Projects\\torchsuite\\data\\coco_pennfudan_sampled_val\\PedMasks\",\n",
    "    dst_test_images=  r\"C:\\Users\\ssre_\\Projects\\torchsuite\\data\\coco_pennfudan_sampled_test\\PNGImages\",\n",
    "    dst_test_masks=   r\"C:\\Users\\ssre_\\Projects\\torchsuite\\data\\coco_pennfudan_sampled_test\\PedMasks\",\n",
    "    train_pct=0.7,  # 70% for training\n",
    "    val_pct=0.15,   # 15% for validation\n",
    "    test_pct=0.15,  # 15% for testing\n",
    "    seed=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset split successfully.\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "split_dataset(\n",
    "    src_images=r\"C:\\Users\\ssre_\\Projects\\torchsuite\\data\\PennFudanPed\\PNGImages\",\n",
    "    src_masks=r\"C:\\Users\\ssre_\\Projects\\torchsuite\\data\\PennFudanPed\\PedMasks\",\n",
    "    dst_train_images= r\"C:\\Users\\ssre_\\Projects\\torchsuite\\data\\coco_pennfudan_sampled_train\\PNGImages\",\n",
    "    dst_train_masks=  r\"C:\\Users\\ssre_\\Projects\\torchsuite\\data\\coco_pennfudan_sampled_train\\PedMasks\",\n",
    "    dst_val_images=   r\"C:\\Users\\ssre_\\Projects\\torchsuite\\data\\coco_pennfudan_sampled_val\\PNGImages\",\n",
    "    dst_val_masks=    r\"C:\\Users\\ssre_\\Projects\\torchsuite\\data\\coco_pennfudan_sampled_val\\PedMasks\",\n",
    "    dst_test_images=  r\"C:\\Users\\ssre_\\Projects\\torchsuite\\data\\coco_pennfudan_sampled_test\\PNGImages\",\n",
    "    dst_test_masks=   r\"C:\\Users\\ssre_\\Projects\\torchsuite\\data\\coco_pennfudan_sampled_test\\PedMasks\",\n",
    "    train_pct=0.7,  # 70% for training\n",
    "    val_pct=0.15,   # 15% for validation\n",
    "    test_pct=0.15,  # 15% for testing\n",
    "    seed=42\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
