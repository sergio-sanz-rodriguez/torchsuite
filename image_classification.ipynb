{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "41e3e0ca",
   "metadata": {},
   "source": [
    "# 1. Introduction\n",
    "\n",
    " This notebook outlines the creation, compilation, and training of a deep learing network using the [TorchSuite](https://github.com/sergio-sanz-rodriguez/torchsuite) framework. In particular, a Vision Transformer (ViT) will be used to classify 101 types of food from the following dataset: https://huggingface.co/datasets/ethz/food101.\n",
    " \n",
    " The following table summarizes the model:\n",
    "\n",
    "| Version | **ViT Type** | **Image Size** | **Patch Size** | **Encoding Layers** | **Hidden Size** | **Multi-layer Perceptron size** | **Attention Heads** | **Hidder Layer Units for Classification** | **Transfer Learning** | **Number of Epochs** | **Learning Rate** | **Scheduler** | **Params**\n",
    "| ----- | ----- | ----- | ----- | ----- | ----- | ----- | ----- | ----- | ----- | ----- | ----- | ----- | ----- |\n",
    "| 1 | Base | 384x384 | 16x16 | 12 | 768 | 3072 | 12 | 64 | IMAGENET1K_SWAG_E2E_V1 | 60 | 0.0001 | CosineAnnealingLR | 86.2M |\n",
    "\n",
    "The custom vision transformer architectures have been implemented from scratch based on the paper titled [\"An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale\"](https://arxiv.org/abs/2010.11929). The custom library is called **vision_transformer** where the **ViT class** can be imported."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dba7507",
   "metadata": {},
   "source": [
    "# 2. Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6892a15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.backends.cudnn as cudnn\n",
    "import pandas as pd\n",
    "\n",
    "from torchvision.transforms import v2\n",
    "from torchinfo import summary\n",
    "from pathlib import Path\n",
    "from torchvision import datasets\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR, LinearLR, SequentialLR, ConstantLR\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Import custom libraries\n",
    "from utils.classification_utils import set_seeds, display_random_images\n",
    "from engines.classification import ClassificationEngine\n",
    "from models.vision_transformer import ViT\n",
    "from engines.schedulers import FixedLRSchedulerWrapper\n",
    "from dataloaders.image_dataloaders import create_classification_dataloaders_vit, create_classification_dataloaders_swin\n",
    "from models.pretrained_classifiers import build_pretrained_classifier\n",
    "\n",
    "# Dataset\n",
    "from datasets import load_dataset\n",
    "\n",
    "import warnings\n",
    "os.environ['TORCH_USE_CUDA_DSA'] = \"1\"\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"torch.autograd.graph\")\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning, module=\"onnxscript.converter\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6555499",
   "metadata": {},
   "source": [
    "# 3. Importing Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d930b109",
   "metadata": {},
   "source": [
    "The dataset should be organized as follows, with one subdirectory per class containing the corresponding images:\n",
    "\n",
    "```\n",
    "dataset/\n",
    "├── train/\n",
    "│   └── <class_label>/\n",
    "│       ├── img1.jpg\n",
    "│       ├── img2.png\n",
    "│       └── ...\n",
    "└── test/ (or val/)/\n",
    "    └── <class_label>/\n",
    "        ├── img1.jpg\n",
    "        ├── img2.png\n",
    "        └── ...\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ea443bb-470a-47e5-8f4d-341abf4e4d6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define some constants\n",
    "NUM_WORKERS = os.cpu_count()\n",
    "AMOUNT_TO_GET = 1.0\n",
    "SEED = 42\n",
    "\n",
    "# Define target data directory\n",
    "TARGET_DIR_NAME = f\"data/food-101_{str(int(AMOUNT_TO_GET*100))}_percent\"\n",
    "\n",
    "# Setup training and test directories\n",
    "TARGET_DIR = Path(TARGET_DIR_NAME)\n",
    "TRAIN_DIR = TARGET_DIR / \"train\"\n",
    "TEST_DIR = TARGET_DIR / \"test\"\n",
    "TARGET_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Create target model directory\n",
    "MODEL_DIR = Path(\"outputs\")\n",
    "MODEL_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Set seeds\n",
    "set_seeds(SEED)\n",
    "\n",
    "IMPORT_DATASET = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99d0ac4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if IMPORT_DATASET:\n",
    "    # Download dataset from Hugging Face\n",
    "    ds = load_dataset(\"ethz/food101\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3772c570",
   "metadata": {},
   "outputs": [],
   "source": [
    "if IMPORT_DATASET:\n",
    "    # Get class names\n",
    "    class_names = ds[\"train\"].features[\"label\"].names\n",
    "\n",
    "    # Function to save images into appropriate directories\n",
    "    def save_images(split, target_dir):\n",
    "        for example in tqdm(ds[split], desc=f\"Saving {split} images\"):\n",
    "            image = example[\"image\"]\n",
    "            label = example[\"label\"]\n",
    "            class_name = class_names[label]\n",
    "\n",
    "            # Define class directory\n",
    "            class_dir = target_dir / class_name\n",
    "            class_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "            # Save image\n",
    "            img_path = class_dir / f\"{len(list(class_dir.iterdir()))}.jpg\"\n",
    "            image.save(img_path)\n",
    "\n",
    "    # Save training and test images\n",
    "    save_images(\"train\", TRAIN_DIR)\n",
    "    save_images(\"validation\", TEST_DIR)\n",
    "\n",
    "    print(\"Dataset has been saved successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c279df5f",
   "metadata": {},
   "source": [
    "# 4. Specifying the Target Device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f01afed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activate cuda benchmark\n",
    "cudnn.benchmark = True\n",
    "\n",
    "# Set device\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Device: {device}\")\n",
    "\n",
    "if device == \"cuda\":\n",
    "    !nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "350701ec-c5f9-4809-884c-69a5dcf97ceb",
   "metadata": {},
   "source": [
    "# 5. Image Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d670c088",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display images\n",
    "manual_transforms = v2.Compose([\n",
    "    v2.Resize((256)),\n",
    "    v2.RandomCrop((256, 256)),    \n",
    "    v2.ToImage(),\n",
    "    v2.ToDtype(torch.float32, scale=True)\n",
    "])\n",
    "\n",
    "train_data = datasets.ImageFolder(TRAIN_DIR, transform=manual_transforms)\n",
    "display_random_images(train_data,\n",
    "                      n=25,\n",
    "                      classes=train_data.classes,\n",
    "                      rows=5,\n",
    "                      cols=5,\n",
    "                      display_shape=False,\n",
    "                      seed=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "815c28c3",
   "metadata": {},
   "source": [
    "# 6. Preparing Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc544d75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the training and test dataloaders\n",
    "IMG_SIZE_2 = 384\n",
    "BATCH_SIZE = 32\n",
    "train_dataloader, test_dataloader, class_names = create_classification_dataloaders_vit(\n",
    "    model=f\"vit_b_16_{IMG_SIZE_2}\", # corresponds to ViT-Base/16-384\n",
    "    batch_size=BATCH_SIZE,\n",
    "    train_dir=TRAIN_DIR,\n",
    "    test_dir=TEST_DIR,\n",
    "    aug=True,\n",
    "    num_workers=NUM_WORKERS\n",
    "    )\n",
    "\n",
    "train_dataloader_no_aug, _, _ = create_classification_dataloaders_vit(\n",
    "    model=f\"vit_b_16_{IMG_SIZE_2}\", # corresponds to ViT-Base/16-384\n",
    "    batch_size=BATCH_SIZE,\n",
    "    train_dir=TRAIN_DIR,\n",
    "    test_dir=TEST_DIR,\n",
    "    aug=False,\n",
    "    num_workers=NUM_WORKERS\n",
    "    )\n",
    "\n",
    "dataloaders = {\n",
    "    'train':         train_dataloader,\n",
    "    'train_aug_off': train_dataloader_no_aug, # Optional: only if off_first/last or random augmentation is enabled.\n",
    "    'test':          test_dataloader\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4530ea7",
   "metadata": {},
   "source": [
    "# 7. Creating a Custom Vision Transformer (ViT) Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6628e81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the ViT-Base/16-384 model\n",
    "NUM_CLASSES = len(class_names)\n",
    "model = ViT(\n",
    "    img_size=IMG_SIZE_2,\n",
    "    in_channels=3,\n",
    "    patch_size=16,\n",
    "    num_transformer_layers=12,\n",
    "    emb_dim=768,\n",
    "    mlp_size=3072,\n",
    "    num_heads=12,\n",
    "    attn_dropout=0,\n",
    "    mlp_dropout=0.1,\n",
    "    emb_dropout=0.1,\n",
    "    num_classes=NUM_CLASSES\n",
    ")\n",
    "\n",
    "# Copy weights from torchvision.models\n",
    "if IMG_SIZE_2 == 384:\n",
    "    model.copy_weights(torchvision.models.ViT_B_16_Weights.IMAGENET1K_SWAG_E2E_V1) # For image size of 384x384\n",
    "else:\n",
    "    model.copy_weights(torchvision.models.ViT_B_16_Weights.IMAGENET1K_V1) # For image size of 224x224\n",
    "\n",
    "# Compile model (optional)\n",
    "#model = torch.compile(model, backend=\"aot_eager\")\n",
    "\n",
    "# Print summary\n",
    "summary(model,\n",
    "        input_size=(BATCH_SIZE,3,IMG_SIZE_2, IMG_SIZE_2),\n",
    "        col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
    "        col_width=20,\n",
    "        row_settings=[\"var_names\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f09468fe",
   "metadata": {},
   "source": [
    "# Or Using PyTorch's Default ViT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9663f31b",
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_DEFAULT_PYTORCH_VIT = False\n",
    "if USE_DEFAULT_PYTORCH_VIT:\n",
    "    # Instantiate the model\n",
    "    model = build_pretrained_classifier(\n",
    "        model=\"vit_b_16_384\" if IMG_SIZE_2 == 384 else \"vit_b_16_224\", # ViT-Base/16-384, otherwise ViT-Base/16-224\n",
    "        num_classes=NUM_CLASSES,            \n",
    "        seed=SEED,\n",
    "        freeze=False,\n",
    "        device=device\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28a1291e",
   "metadata": {},
   "source": [
    "# 8. Training the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a768e325",
   "metadata": {},
   "source": [
    "## 8.1. Allways-on Augmentation\n",
    "\n",
    "This strategy is the most commonly used approach and is also the default configuration of the training engine, maximizing the effective diversity of the training set from the outset. However, if the selected augmentation operations are too strong while the model is still untrained, it may struggle to learn the underlying structure early on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c98e473",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure training strategy: optimizer, loss function, scheduler (optional)\n",
    "EPOCHS = 20\n",
    "LR = 0.0001\n",
    "MIN_LR = 1e-6\n",
    "model_type=\"model_always\"\n",
    "model_name = model_type + \".pth\"\n",
    "\n",
    "# Create AdamW optimizer\n",
    "optimizer = torch.optim.AdamW(\n",
    "    params=model.parameters(),\n",
    "    lr=LR,\n",
    "    betas=(0.9, 0.999),\n",
    "    weight_decay=0.01\n",
    ")\n",
    "\n",
    "# Create loss function\n",
    "loss_fn = torch.nn.CrossEntropyLoss(label_smoothing=0.1)\n",
    "\n",
    "# Set scheduler\n",
    "cosine = CosineAnnealingLR(optimizer, T_max=10, eta_min=MIN_LR) # 1-10:  LR = 1e-4 -> 1e-6 (cosine)\n",
    "fixed = ConstantLR(optimizer, factor=MIN_LR/LR, total_iters=10) # 11-20: LR = 1e-6\n",
    "scheduler = SequentialLR(\n",
    "    optimizer,\n",
    "    schedulers=[cosine, fixed],\n",
    "    milestones=[10] \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d4dda4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# And train...\n",
    "\n",
    "# Instantiate the classification engine with the created model and the target device\n",
    "engine = ClassificationEngine(\n",
    "    model=model,                                # Model to be trained\n",
    "    optimizer=optimizer,                        # Optimizer\n",
    "    loss_fn=loss_fn,                            # Loss function\n",
    "    scheduler=scheduler,                        # Scheduler \n",
    "    use_distillation=False,                     # Optional, use_distillation is False by default    \n",
    "    color_map={'train': 'light_red',            # Color map for the plots\n",
    "               'test': 'light_green'},\n",
    "    log_verbose=True,                           # Verbosity\n",
    "    device=device                               # Target device\n",
    "    )\n",
    "\n",
    "# Configure the training method\n",
    "results = engine.train(\n",
    "    target_dir=MODEL_DIR,                       # Directory where the model will be saved\n",
    "    model_name=model_name,                      # Name of the model\n",
    "    resume=False,                                # Resume training from the last saved checkpoint\n",
    "    save_best_model=[\"last\", \"loss\", \"acc\"],    # Save the best models based on different criteria\n",
    "    keep_best_models_in_memory=False,           # Do not keep the models stored in memory for the sake of training time and memory efficiency    \n",
    "    dataloaders=dataloaders,                    # Dictionary with the dataloaders     \n",
    "    apply_validation=True,                      # Enable validation step\n",
    "    augmentation_strategy=\"always\",             # Augmentation strategy    \n",
    "    recall_threshold=0.995,                     # False positive rate at recall_threshold recall\n",
    "    recall_threshold_pauc=0.95,                 # Partial AUC score above recall_threshold_pauc recall\n",
    "    epochs=EPOCHS,                              # Total number of epochs\n",
    "    amp=True,                                   # Enable Automatic Mixed Precision (AMP)\n",
    "    enable_clipping=False,                      # Disable clipping on gradients, only useful if training becomes unestable\n",
    "    debug_mode=False,                           # Disable debug mode    \n",
    "    accumulation_steps=2,                       # Accumulation steps 2: effective batch size = batch_size x accumulation steps\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac7c5f8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# If trainning is interrupted, you can resume it by enabling 'resume'\n",
    "#engine = ClassificationEngine(\n",
    "#    model=model,\n",
    "#    optimizer=optimizer,\n",
    "#    loss_fn=loss_fn,\n",
    "#    scheduler=scheduler,\n",
    "#    use_distillation=False,\n",
    "#    color_map={'train': 'light_red', 'test': 'light_magenta'},\n",
    "#    log_verbose=True,\n",
    "#    device=device\n",
    "#    )\n",
    "#results = engine.train(\n",
    "#    target_dir=MODEL_DIR,                       # Directory where the model will be saved\n",
    "#    model_name=model_name,                      # Name of the model\n",
    "#    resume=True,                                # Resume training from the last saved checkpoint\n",
    "#    save_best_model=[\"last\", \"loss\", \"acc\"],    # Save the best models based on different criteria\n",
    "#    keep_best_models_in_memory=False,           # Do not keep the models stored in memory for the sake of training time and memory efficiency    \n",
    "#    dataloaders=dataloaders,                    # Dictionary with the dataloaders\n",
    "#    apply_validation=True,                      # Enable validation step\n",
    "#    augmentation_strategy=\"random\",             # Augmentation strategy    \n",
    "#    recall_threshold=0.995,                     # False positive rate at recall_threshold recall\n",
    "#    recall_threshold_pauc=0.95,                 # Partial AUC score above recall_threshold_pauc recall\n",
    "#    epochs=EPOCHS,                              # Total number of epochs\n",
    "#    amp=True,                                   # Enable Automatic Mixed Precision (AMP)\n",
    "#    enable_clipping=False,                      # Disable clipping on gradients, only useful if training becomes unestable\n",
    "#    debug_mode=False,                           # Disable debug mode    \n",
    "#    accumulation_steps=2,                       # Accumulation steps 2: effective batch size = batch_size x accumulation steps\n",
    "#    )\n",
    "\n",
    "# Or simply\n",
    "# results = engine.train(resume=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77137484",
   "metadata": {},
   "source": [
    "## 8.2. Disabling Augmentation in the First Epochs\n",
    "\n",
    "The model quickly learns the simplest structure on clean data first, then regularisation (augmentation) is introduced when it already has a decent representation.\n",
    "\n",
    "However, if training starts without augmentation and then switch it on, the model may see the augmented data as a new distribution, which can slow learning, especially if the augmentations are strong. \n",
    "\n",
    "This strategy works better if augmentations are mild."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31ee8947",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the vanilla ViT-Base model\n",
    "NUM_CLASSES = len(class_names)\n",
    "model = ViT(\n",
    "    img_size=IMG_SIZE_2,\n",
    "    in_channels=3,\n",
    "    patch_size=16,\n",
    "    num_transformer_layers=12,\n",
    "    emb_dim=768,\n",
    "    mlp_size=3072,\n",
    "    num_heads=12,\n",
    "    attn_dropout=0,\n",
    "    mlp_dropout=0.1,\n",
    "    emb_dropout=0.1,\n",
    "    num_classes=NUM_CLASSES\n",
    ")\n",
    "\n",
    "# Copy weights from torchvision.models\n",
    "if IMG_SIZE_2 == 384:\n",
    "    model.copy_weights(torchvision.models.ViT_B_16_Weights.IMAGENET1K_SWAG_E2E_V1) # For image size of 384x384\n",
    "else:\n",
    "    model.copy_weights(torchvision.models.ViT_B_16_Weights.IMAGENET1K_V1) # For image size of 224x224"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b49f2b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure training strategy: optimizer, loss function, scheduler (optional)\n",
    "EPOCHS = 20\n",
    "LR = 0.0001\n",
    "MIN_LR = 1e-6\n",
    "OFF_EPOCHS = 5\n",
    "model_type=\"model_off_first\"\n",
    "model_name = model_type + \".pth\"\n",
    "\n",
    "# Create AdamW optimizer\n",
    "optimizer = torch.optim.AdamW(\n",
    "    params=model.parameters(),\n",
    "    lr=LR,\n",
    "    betas=(0.9, 0.999),\n",
    "    weight_decay=0.01\n",
    ")\n",
    "\n",
    "# Create loss function\n",
    "loss_fn = torch.nn.CrossEntropyLoss(label_smoothing=0.1)\n",
    "\n",
    "# Set scheduler\n",
    "linear = LinearLR(optimizer, start_factor=MIN_LR/LR, total_iters=OFF_EPOCHS) # 1-5:   LR = 1e-6\n",
    "cosine = CosineAnnealingLR(optimizer, T_max=10, eta_min=MIN_LR)          # 6-15:  LR = 1e-4 -> 1e-6 (cosine)\n",
    "fixed =  ConstantLR(optimizer, factor=MIN_LR/LR, total_iters=OFF_EPOCHS) # 16-20: LR = 1e-6\n",
    "scheduler = SequentialLR(\n",
    "    optimizer,\n",
    "    schedulers=[linear, cosine, fixed],\n",
    "    milestones=[OFF_EPOCHS, EPOCHS-OFF_EPOCHS]  # switch at these epochs\n",
    ")\n",
    "\n",
    "# And train...\n",
    "\n",
    "# Instantiate the classification engine with the created model and the target device\n",
    "engine = ClassificationEngine(\n",
    "    model=model,\n",
    "    optimizer=optimizer,\n",
    "    loss_fn=loss_fn,\n",
    "    scheduler=scheduler,\n",
    "    use_distillation=False,\n",
    "    log_verbose=True,\n",
    "    device=device)\n",
    "\n",
    "# Configure the training method\n",
    "results = engine.train(\n",
    "    target_dir=MODEL_DIR,                       # Directory where the model will be saved\n",
    "    model_name=model_name,                      # Name of the model\n",
    "    save_best_model=[\"last\", \"loss\", \"acc\"],    # Save the best models based on different criteria\n",
    "    keep_best_models_in_memory=False,           # Do not keep the models stored in memory for the sake of training time and memory efficiency    \n",
    "    dataloaders=dataloaders,                    # Dictionary with the dataloaders\n",
    "    apply_validation=True,                      # Enable validation step\n",
    "    augmentation_strategy=\"off_first\",          # Augmentation strategy\n",
    "    augmentation_off_epochs=OFF_EPOCHS,         # Number of epochs without augmentation    \n",
    "    recall_threshold=0.995,                     # False positive rate at recall_threshold recall\n",
    "    recall_threshold_pauc=0.95,                 # Partial AUC score above recall_threshold_pauc recall    \n",
    "    epochs=EPOCHS,                              # Total number of epochs\n",
    "    amp=True,                                   # Enable Automatic Mixed Precision (AMP)\n",
    "    enable_clipping=False,                      # Disable clipping on gradients, only useful if training becomes unestable\n",
    "    debug_mode=False,                           # Disable debug mode    \n",
    "    accumulation_steps=2,                       # Accumulation steps 2: effective batch size = batch_size x accumulation steps\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec12f731",
   "metadata": {},
   "source": [
    "## 8.3. Disabling Augmentation in the Last Epochs\n",
    "\n",
    "By training on clean data near the end the model sees the true distribution and fine-tune its decision boundaries without the noise of augmentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c263538",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the vanilla ViT-Base model\n",
    "NUM_CLASSES = len(class_names)\n",
    "model = ViT(\n",
    "    img_size=IMG_SIZE_2,\n",
    "    in_channels=3,\n",
    "    patch_size=16,\n",
    "    num_transformer_layers=12,\n",
    "    emb_dim=768,\n",
    "    mlp_size=3072,\n",
    "    num_heads=12,\n",
    "    attn_dropout=0,\n",
    "    mlp_dropout=0.1,\n",
    "    emb_dropout=0.1,\n",
    "    num_classes=NUM_CLASSES\n",
    ")\n",
    "\n",
    "# Copy weights from torchvision.models\n",
    "if IMG_SIZE_2 == 384:\n",
    "    model.copy_weights(torchvision.models.ViT_B_16_Weights.IMAGENET1K_SWAG_E2E_V1) # For image size of 384x384\n",
    "else:\n",
    "    model.copy_weights(torchvision.models.ViT_B_16_Weights.IMAGENET1K_V1) # For image size of 224x224"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5be2266f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure training strategy: optimizer, loss function, scheduler (optional)\n",
    "EPOCHS = 20\n",
    "LR = 0.0001\n",
    "MIN_LR = 1e-6\n",
    "OFF_EPOCHS = 5\n",
    "model_type=\"model_off_last\"\n",
    "model_name = model_type + \".pth\"\n",
    "\n",
    "# Create AdamW optimizer\n",
    "optimizer = torch.optim.AdamW(\n",
    "    params=model.parameters(),\n",
    "    lr=LR,\n",
    "    betas=(0.9, 0.999),\n",
    "    weight_decay=0.01\n",
    ")\n",
    "\n",
    "# Create loss function\n",
    "loss_fn = torch.nn.CrossEntropyLoss(label_smoothing=0.1)\n",
    "\n",
    "# Set scheduler\n",
    "cosine = CosineAnnealingLR(optimizer, T_max=10, eta_min=MIN_LR) # 1-10:  LR = 1e-4 -> 1e-6 (cosine)\n",
    "fixed = ConstantLR(optimizer, factor=MIN_LR/LR, total_iters=10) # 11-20: LR = 1e-6\n",
    "scheduler = SequentialLR(\n",
    "    optimizer,\n",
    "    schedulers=[cosine, fixed],\n",
    "    milestones=[10] \n",
    ")\n",
    "\n",
    "# And train...\n",
    "\n",
    "# Instantiate the classification engine with the created model and the target device\n",
    "engine = ClassificationEngine(\n",
    "    model=model,\n",
    "    optimizer=optimizer,\n",
    "    loss_fn=loss_fn,\n",
    "    scheduler=scheduler,\n",
    "    use_distillation=False,\n",
    "    log_verbose=True,\n",
    "    color_map={'train': 'red', 'test': 'yellow'},\n",
    "    device=device)\n",
    "\n",
    "# Configure the training method\n",
    "results = engine.train(\n",
    "    target_dir=MODEL_DIR,                       # Directory where the model will be saved\n",
    "    model_name=model_name,                      # Name of the model\n",
    "    save_best_model=[\"last\", \"loss\", \"acc\"],    # Save the best models based on different criteria\n",
    "    keep_best_models_in_memory=False,           # Do not keep the models stored in memory for the sake of training time and memory efficiency    \n",
    "    dataloaders=dataloaders,                    # Dictionary with the dataloaders\n",
    "    apply_validation=True,                      # Enable validation step\n",
    "    augmentation_strategy=\"off_last\",           # Augmentation strategy\n",
    "    augmentation_off_epochs=OFF_EPOCHS,         # Number of epochs without augmentation\n",
    "    recall_threshold=0.995,                     # False positive rate at recall_threshold recall\n",
    "    recall_threshold_pauc=0.95,                 # Partial AUC score above recall_threshold_pauc recall    \n",
    "    epochs=EPOCHS,                              # Total number of epochs\n",
    "    amp=True,                                   # Enable Automatic Mixed Precision (AMP)\n",
    "    enable_clipping=False,                      # Disable clipping on gradients, only useful if training becomes unestable\n",
    "    debug_mode=False,                           # Disable debug mode    \n",
    "    accumulation_steps=2,                       # Accumulation steps 2: effective batch size = batch_size x accumulation steps\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1892dd74",
   "metadata": {},
   "source": [
    "## 8.4. Random Augmentation Schedules\n",
    "\n",
    "This strategy acts like another layer of stochastic regularisation; the model never quite knows whether input will be clean or augmented. However, it can be harder to control."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6e0a6b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the vanilla ViT-Base model\n",
    "NUM_CLASSES = len(class_names)\n",
    "model = ViT(\n",
    "    img_size=IMG_SIZE_2,\n",
    "    in_channels=3,\n",
    "    patch_size=16,\n",
    "    num_transformer_layers=12,\n",
    "    emb_dim=768,\n",
    "    mlp_size=3072,\n",
    "    num_heads=12,\n",
    "    attn_dropout=0,\n",
    "    mlp_dropout=0.1,\n",
    "    emb_dropout=0.1,\n",
    "    num_classes=NUM_CLASSES\n",
    ")\n",
    "\n",
    "# Copy weights from torchvision.models\n",
    "if IMG_SIZE_2 == 384:\n",
    "    model.copy_weights(torchvision.models.ViT_B_16_Weights.IMAGENET1K_SWAG_E2E_V1) # For image size of 384x384\n",
    "else:\n",
    "    model.copy_weights(torchvision.models.ViT_B_16_Weights.IMAGENET1K_V1) # For image size of 224x224"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d118490",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure training strategy: optimizer, loss function, scheduler (optional)\n",
    "EPOCHS = 20\n",
    "LR = 0.0001\n",
    "MIN_LR = 1e-6\n",
    "RANDOM_PROB = 0.25\n",
    "model_type=\"model_random\"\n",
    "model_name = model_type + \".pth\"\n",
    "\n",
    "# Create AdamW optimizer\n",
    "optimizer = torch.optim.AdamW(\n",
    "    params=model.parameters(),\n",
    "    lr=LR,\n",
    "    betas=(0.9, 0.999),\n",
    "    weight_decay=0.01\n",
    ")\n",
    "\n",
    "# Create loss function\n",
    "loss_fn = torch.nn.CrossEntropyLoss(label_smoothing=0.1)\n",
    "\n",
    "# Set scheduler\n",
    "cosine = CosineAnnealingLR(optimizer, T_max=10, eta_min=MIN_LR) # 1-10:  LR = 1e-4 -> 1e-6 (cosine)\n",
    "fixed = ConstantLR(optimizer, factor=MIN_LR/LR, total_iters=10) # 11-20: LR = 1e-6\n",
    "scheduler = SequentialLR(\n",
    "    optimizer,\n",
    "    schedulers=[cosine, fixed],\n",
    "    milestones=[10]\n",
    ")\n",
    "\n",
    "# And train...\n",
    "\n",
    "# Instantiate the classification engine with the created model and the target device\n",
    "engine = ClassificationEngine(\n",
    "    model=model,\n",
    "    optimizer=optimizer,\n",
    "    loss_fn=loss_fn,\n",
    "    scheduler=scheduler,\n",
    "    use_distillation=False,\n",
    "    log_verbose=True,\n",
    "    color_map={'train': 'magenta', 'test': 'green'},\n",
    "    device=device)\n",
    "\n",
    "# Configure the training method\n",
    "results = engine.train(\n",
    "    target_dir=MODEL_DIR,                       # Directory where the model will be saved\n",
    "    model_name=model_name,                      # Name of the model\n",
    "    save_best_model=[\"last\", \"loss\", \"acc\"],    # Save the best models based on different criteria\n",
    "    keep_best_models_in_memory=False,           # Do not keep the models stored in memory for the sake of training time and memory efficiency    \n",
    "    dataloaders=dataloaders,                    # Dictionary with the dataloaders\n",
    "    apply_validation=True,                      # Enable validation step\n",
    "    augmentation_strategy=\"random\",             # Augmentation strategy\n",
    "    augmentation_random_prob=RANDOM_PROB,       # Probability (0.0-1.0) of applying augmentation when agumentation_strategy is set to random    \n",
    "    recall_threshold=0.995,                     # False positive rate at recall_threshold recall\n",
    "    recall_threshold_pauc=0.95,                 # Partial AUC score above recall_threshold_pauc recall    \n",
    "    epochs=EPOCHS,                              # Total number of epochs\n",
    "    amp=True,                                   # Enable Automatic Mixed Precision (AMP)\n",
    "    enable_clipping=False,                      # Disable clipping on gradients, only useful if training becomes unestable\n",
    "    debug_mode=False,                           # Disable debug mode    \n",
    "    accumulation_steps=2,                       # Accumulation steps 2: effective batch size = batch_size x accumulation steps\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c80073a0",
   "metadata": {},
   "source": [
    "# 9. Evaluating the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c75f8757",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a classification report \n",
    "transforms = v2.Compose([\n",
    "    v2.Resize(IMG_SIZE_2),\n",
    "    v2.CenterCrop((IMG_SIZE_2, IMG_SIZE_2)),\n",
    "    v2.ToImage(),\n",
    "    v2.ToDtype(torch.float32, scale=True),\n",
    "    v2.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "pred_list_gpu, classif_report_gpu = engine.predict_and_store(\n",
    "    test_dir=TEST_DIR,\n",
    "    transform=transforms,\n",
    "    class_names=class_names,\n",
    "    sample_fraction=1,\n",
    "    seed=SEED) # make predictions on GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d4a4cc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(classif_report_gpu)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv_pytorch_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
