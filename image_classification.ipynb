{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "41e3e0ca",
   "metadata": {},
   "source": [
    "# 1. Introduction\n",
    "\n",
    " This notebook outlines the creation, compilation, and training of a deep learing network using the [TorchSuite](https://github.com/sergio-sanz-rodriguez/torchsuite) framework. In particular, a Vision Transformer (ViT) will be used to classify 101 types of food from the following dataset: https://huggingface.co/datasets/ethz/food101.\n",
    " \n",
    " The following table summarizes the model:\n",
    "\n",
    "| Version | **ViT Type** | **Image Size** | **Patch Size** | **Encoding Layers** | **Hidden Size** | **Multi-layer Perceptron size** | **Attention Heads** | **Hidder Layer Units for Classification** | **Transfer Learning** | **Number of Epochs** | **Learning Rate** | **Scheduler** | **Params**\n",
    "| ----- | ----- | ----- | ----- | ----- | ----- | ----- | ----- | ----- | ----- | ----- | ----- | ----- | ----- |\n",
    "| 1 | Base | 384x384 | 16x16 | 12 | 768 | 3072 | 12 | 64 | IMAGENET1K_SWAG_E2E_V1 | 60 | 0.0001 | CosineAnnealingLR | 86.2M |\n",
    "\n",
    "The custom vision transformer architectures have been implemented from scratch based on the paper titled [\"An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale\"](https://arxiv.org/abs/2010.11929). The custom library is called **vision_transformer** where the **ViT class** can be imported."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dba7507",
   "metadata": {},
   "source": [
    "# 2. Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6892a15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.backends.cudnn as cudnn\n",
    "import pandas as pd\n",
    "\n",
    "from torchvision.transforms import v2\n",
    "from torchinfo import summary\n",
    "from pathlib import Path\n",
    "from torchvision import datasets\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Import custom libraries\n",
    "from utils.helper_functions import set_seeds, display_random_images\n",
    "from engines.classification import ClassificationEngine\n",
    "from models.vision_transformer import ViT\n",
    "from engines.schedulers import FixedLRSchedulerWrapper\n",
    "from dataloaders.image_dataloaders import create_dataloaders_vit\n",
    "\n",
    "# Dataset\n",
    "from datasets import load_dataset\n",
    "\n",
    "import warnings\n",
    "os.environ['TORCH_USE_CUDA_DSA'] = \"1\"\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"torch.autograd.graph\")\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning, module=\"onnxscript.converter\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6555499",
   "metadata": {},
   "source": [
    "# 3. Importing Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ea443bb-470a-47e5-8f4d-341abf4e4d6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define some constants\n",
    "NUM_WORKERS = os.cpu_count()\n",
    "AMOUNT_TO_GET = 1.0\n",
    "SEED = 42\n",
    "\n",
    "# Define target data directory\n",
    "TARGET_DIR_NAME = f\"data/food-101_{str(int(AMOUNT_TO_GET*100))}_percent\"\n",
    "\n",
    "# Setup training and test directories\n",
    "TARGET_DIR = Path(TARGET_DIR_NAME)\n",
    "TRAIN_DIR = TARGET_DIR / \"train\"\n",
    "TEST_DIR = TARGET_DIR / \"test\"\n",
    "TARGET_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Create target model directory\n",
    "MODEL_DIR = Path(\"outputs\")\n",
    "MODEL_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Set seeds\n",
    "set_seeds(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99d0ac4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download dataset from Hugging Face\n",
    "ds = load_dataset(\"ethz/food101\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3772c570",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get class names\n",
    "class_names = ds[\"train\"].features[\"label\"].names\n",
    "\n",
    "# Function to save images into appropriate directories\n",
    "def save_images(split, target_dir):\n",
    "    for example in tqdm(ds[split], desc=f\"Saving {split} images\"):\n",
    "        image = example[\"image\"]\n",
    "        label = example[\"label\"]\n",
    "        class_name = class_names[label]\n",
    "\n",
    "        # Define class directory\n",
    "        class_dir = target_dir / class_name\n",
    "        class_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        # Save image\n",
    "        img_path = class_dir / f\"{len(list(class_dir.iterdir()))}.jpg\"\n",
    "        image.save(img_path)\n",
    "\n",
    "# Save training and test images\n",
    "save_images(\"train\", TRAIN_DIR)\n",
    "save_images(\"validation\", TEST_DIR)\n",
    "\n",
    "print(\"Dataset has been saved successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c279df5f",
   "metadata": {},
   "source": [
    "# 4. Specifying the Target Device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f01afed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activate cuda benchmark\n",
    "cudnn.benchmark = True\n",
    "\n",
    "# Set device\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Device: {device}\")\n",
    "\n",
    "if device == \"cuda\":\n",
    "    !nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "350701ec-c5f9-4809-884c-69a5dcf97ceb",
   "metadata": {},
   "source": [
    "# 5. Image Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d670c088",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display images\n",
    "manual_transforms = v2.Compose([\n",
    "    v2.Resize((256)),\n",
    "    v2.RandomCrop((256, 256)),    \n",
    "    v2.ToImage(),\n",
    "    v2.ToDtype(torch.float32, scale=True)\n",
    "])\n",
    "\n",
    "train_data = datasets.ImageFolder(TRAIN_DIR, transform=manual_transforms)\n",
    "display_random_images(train_data,\n",
    "                      n=25,\n",
    "                      classes=train_data.classes,\n",
    "                      rows=5,\n",
    "                      cols=5,\n",
    "                      display_shape=False,\n",
    "                      seed=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "815c28c3",
   "metadata": {},
   "source": [
    "# 6. Preparing Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc544d75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the training and test dataloaders\n",
    "IMG_SIZE_2 = 384\n",
    "BATCH_SIZE = 32\n",
    "train_dataloader, test_dataloader, class_names = create_dataloaders_vit(\n",
    "    vit_model=\"vit_b_16_384\", # corresponds to ViT-Base/16-384\n",
    "    batch_size=BATCH_SIZE,\n",
    "    train_dir=TRAIN_DIR,\n",
    "    test_dir=TEST_DIR,\n",
    "    aug=True,\n",
    "    num_workers=NUM_WORKERS\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4530ea7",
   "metadata": {},
   "source": [
    "# 7. Creating a Custom Vision Transformer (ViT) Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6628e81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the ViT-Base model\n",
    "model = ViT(\n",
    "    img_size=IMG_SIZE_2,\n",
    "    in_channels=3,\n",
    "    patch_size=16,\n",
    "    num_transformer_layers=12,\n",
    "    emb_dim=768,\n",
    "    mlp_size=3072,\n",
    "    num_heads=12,\n",
    "    attn_dropout=0,\n",
    "    mlp_dropout=0.1,\n",
    "    emb_dropout=0.1,\n",
    "    num_classes=len(class_names)\n",
    ")\n",
    "\n",
    "# Copy weights from torchvision.models.ViT_B_16_Weights.IMAGENET1K_SWAG_E2E_V1\n",
    "model.copy_weights(torchvision.models.ViT_B_16_Weights.IMAGENET1K_SWAG_E2E_V1)\n",
    "\n",
    "# Compile model\n",
    "model = torch.compile(model, backend=\"aot_eager\")\n",
    "\n",
    "# Print summary\n",
    "#summary(model,\n",
    "#        input_size=(BATCH_SIZE,3,IMG_SIZE_2, IMG_SIZE_2),\n",
    "#        col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
    "#        col_width=20,\n",
    "#        row_settings=[\"var_names\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28a1291e",
   "metadata": {},
   "source": [
    "# 8. Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa1218f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "EPOCHS = 20\n",
    "LR = 0.0001\n",
    "model_type=\"model\"\n",
    "model_name = model_type + \".pth\"\n",
    "\n",
    "# Create AdamW optimizer\n",
    "optimizer = torch.optim.AdamW(\n",
    "    params=model.parameters(),\n",
    "    lr=LR,\n",
    "    betas=(0.9, 0.999),\n",
    "    weight_decay=0.01\n",
    ")\n",
    "\n",
    "# Create loss function\n",
    "loss_fn = torch.nn.CrossEntropyLoss(label_smoothing=0.1)\n",
    "\n",
    "# Set scheduler: from epoch #1 to #10 use CosinAnnealingRL, from epoch #11 to #20 a fixed learning rate\n",
    "scheduler = FixedLRSchedulerWrapper(\n",
    "    scheduler=CosineAnnealingLR(optimizer, T_max=10, eta_min=1e-6),\n",
    "    fixed_lr=1e-6,\n",
    "    fixed_epoch=10)\n",
    "\n",
    "# Set seeds\n",
    "set_seeds(SEED)\n",
    "\n",
    "# And train...\n",
    "\n",
    "# Instantiate the classification engine with the created model and the target device\n",
    "engine = ClassificationEngine(\n",
    "    model=model,\n",
    "    device=device)\n",
    "\n",
    "# Configure the training method\n",
    "results = engine.train(\n",
    "    target_dir=MODEL_DIR,                       # Directory where the model will be saved\n",
    "    model_name=model_name,                      # Name of the model\n",
    "    save_best_model=[\"last\", \"loss\", \"acc\"],    # Save the best models based on different criteria\n",
    "    keep_best_models_in_memory=False,           # Do not keep the models stored in memory for the sake of training time and memory efficiency\n",
    "    train_dataloader=train_dataloader,          # Train dataloader\n",
    "    test_dataloader=test_dataloader,            # Test dataloader\n",
    "    optimizer=optimizer,                        # Optimizer\n",
    "    loss_fn=loss_fn,                            # Loss function\n",
    "    recall_threshold=0.995,                     # False positive rate at recall_threshold recall\n",
    "    recall_threshold_pauc=0.95,                 # Partial AUC score above recall_threshold_pauc recall\n",
    "    scheduler=scheduler,                        # Scheduler\n",
    "    epochs=EPOCHS,                              # Total number of epochs\n",
    "    amp=True,                                   # Enable Automatic Mixed Precision (AMP)\n",
    "    enable_clipping=False,                      # Disable clipping on gradients, only useful if training becomes unestable\n",
    "    debug_mode=False,                           # Disable debug mode    \n",
    "    accumulation_steps=2,                       # Accumulation steps 2: effective batch size = batch_size x accumulation steps\n",
    "    apply_validation=True                       # Enable validation step\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c75f8757",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a classification report \n",
    "transforms = v2.Compose([\n",
    "    v2.Resize(384),\n",
    "    v2.CenterCrop((384, 384)),\n",
    "    v2.ToImage(),\n",
    "    v2.ToDtype(torch.float32, scale=True),\n",
    "    v2.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "pred_list_gpu, classif_report_gpu = engine.predict_and_store(\n",
    "    test_dir=TEST_DIR,\n",
    "    transform=transforms,\n",
    "    class_names=class_names,\n",
    "    sample_fraction=1,\n",
    "    seed=SEED) # make predictions on GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d4a4cc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(classif_report_gpu)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
