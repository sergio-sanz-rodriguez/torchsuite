{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Introduction\n",
    "This notebook outlines the steps to convert the COCO dataset for pedestrian detection and segmentation into a structured format. The process involves:\n",
    "\n",
    "* Filtering the dataset to retain only the \"person\" category.\n",
    "* Storing original images and corresponding segmentation masks in separate folders.\n",
    "* Splitting the dataset into three subsets: training, validation, and testing.\n",
    "\n",
    "By applying this conversion, the COCO dataset can be used alongside the PennFudanPed dataset for pedestrian detection and segmentation tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. The U-Net Architecture\n",
    "U-Net is one of the most common deep learning architectures for segmentation tasks. The block diagram of this model is depicted in the figure below:\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"images/u-net-architecture.png\" width=\"600\"/>\n",
    "</div>\n",
    "\n",
    "The architecture features a \"U\" shape, consisting of two main stages: the contracting (encoder) and expansive (decoder) stages.\n",
    "\n",
    "* The **encoder** captures the context and high-level features of the input image by using several convolutional layers. It gradually reduces the spatial dimensions while increasing the feature dimensions.\n",
    "\n",
    "* The **decoder** is responsible for reconstructing the output image, which, in the case of segmentation, is the mask that identifies the objects of interest. This stage involves upsampling across the same number of levels as the encoder, followed by convolutional operations to \"expand\" the contracted image.\n",
    "\n",
    "One of the unique characteristics of U-Net is its **skip connections**, which link the encoder and decoder stages at each level by merging features. While the contracting and expanding paths (\"U\" shape) capture high-level contextual information, the skip connections help preserve low-level spatial details that might be lost during downsampling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generic libraries\n",
    "# Generic libraries\n",
    "import os\n",
    "import torch\n",
    "import json\n",
    "import zipfile\n",
    "import torch.backends.cudnn as cudnn\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from torchinfo import summary\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "\n",
    "# Torchvision libraries\n",
    "from torchvision import tv_tensors\n",
    "from torchvision.io import read_image\n",
    "from torchvision.transforms import v2 as T\n",
    "from torchvision.utils import draw_bounding_boxes, draw_segmentation_masks\n",
    "\n",
    "# Import custom libraries\n",
    "from utils.classification_utils import set_seeds, load_model\n",
    "from utils.obj_detection_utils import collate_fn, prune_predictions, display_and_save_predictions, visualize_transformed_data\n",
    "from utils.segmentation_utils import display_image_with_mask, collapse_one_hot_mask, create_label_class_dict\n",
    "from engines.obj_detection import ObjectDetectionEngine\n",
    "from engines.schedulers import FixedLRSchedulerWrapper\n",
    "from dataloaders.obj_dect_dataloaders import ProcessDataset\n",
    "from dataloaders.segmentation_dataloaders import ProcessDatasetSegmentation, SegmentationTransforms\n",
    "from models.faster_rcnn import StandardFasterRCNN, CustomFasterRCNN\n",
    "from models.unet import UNet, UNet2\n",
    "# Import custom libraries\n",
    "from utils.classification_utils import set_seeds\n",
    "from utils.coco_dataset_utils import COCO_2_ImgMsk, select_and_copy_samples, split_dataset\n",
    "\n",
    "# Warnings\n",
    "import warnings\n",
    "os.environ['TORCH_USE_CUDA_DSA'] = \"1\"\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"torch.autograd.graph\")\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning, module=\"onnxscript.converter\")\n",
    "\n",
    "# Create target model directory\n",
    "MODEL_DIR = Path(\"outputs\")\n",
    "MODEL_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Set seeds\n",
    "SEED = 42\n",
    "set_seeds(SEED)\n",
    "\n",
    "DOWNLOAD_COCO = False\n",
    "PROCESS_COCO = False\n",
    "VISUALIZE_TRANSFORMED_DATA = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Specifying the Target Device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "Wed Mar  5 19:09:24 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 560.94                 Driver Version: 560.94         CUDA Version: 12.6     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                  Driver-Model | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA GeForce RTX 4070      WDDM  |   00000000:01:00.0  On |                  N/A |\n",
      "|  0%   47C    P8             11W /  200W |    1674MiB /  12282MiB |      4%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|    0   N/A  N/A      5148    C+G   ...nt.CBS_cw5n1h2txyewy\\SearchHost.exe      N/A      |\n",
      "|    0   N/A  N/A      5724    C+G   ...2txyewy\\StartMenuExperienceHost.exe      N/A      |\n",
      "|    0   N/A  N/A      7828    C+G   C:\\Windows\\System32\\ShellHost.exe           N/A      |\n",
      "|    0   N/A  N/A     10464    C+G   ...crosoft\\Edge\\Application\\msedge.exe      N/A      |\n",
      "|    0   N/A  N/A     11100    C+G   ...1.0_x64__8wekyb3d8bbwe\\Video.UI.exe      N/A      |\n",
      "|    0   N/A  N/A     12760    C+G   ...Programs\\Microsoft VS Code\\Code.exe      N/A      |\n",
      "|    0   N/A  N/A     15456    C+G   ...ekyb3d8bbwe\\PhoneExperienceHost.exe      N/A      |\n",
      "|    0   N/A  N/A     15488    C+G   ...aam7r\\AcrobatNotificationClient.exe      N/A      |\n",
      "|    0   N/A  N/A     18976    C+G   ...crosoft\\Edge\\Application\\msedge.exe      N/A      |\n",
      "|    0   N/A  N/A     19492    C+G   ...GeForce Experience\\NVIDIA Share.exe      N/A      |\n",
      "|    0   N/A  N/A     21356    C+G   ... Stream\\104.0.4.0\\GoogleDriveFS.exe      N/A      |\n",
      "|    0   N/A  N/A     22336    C+G   ...on\\133.0.3065.92\\msedgewebview2.exe      N/A      |\n",
      "|    0   N/A  N/A     23288    C+G   ...\\Programs\\signal-desktop\\Signal.exe      N/A      |\n",
      "|    0   N/A  N/A     23484    C+G   ...5n1h2txyewy\\ShellExperienceHost.exe      N/A      |\n",
      "|    0   N/A  N/A     24684    C+G   ...on\\133.0.3065.92\\msedgewebview2.exe      N/A      |\n",
      "|    0   N/A  N/A     25672    C+G   ...t.LockApp_cw5n1h2txyewy\\LockApp.exe      N/A      |\n",
      "|    0   N/A  N/A     28088    C+G   ...5.0_x64__cv1g1gvanyjgm\\WhatsApp.exe      N/A      |\n",
      "|    0   N/A  N/A     28112    C+G   ...oogle\\Chrome\\Application\\chrome.exe      N/A      |\n",
      "|    0   N/A  N/A     29976    C+G   C:\\Windows\\explorer.exe                     N/A      |\n",
      "|    0   N/A  N/A     31084    C+G   ...290_x64__8wekyb3d8bbwe\\ms-teams.exe      N/A      |\n",
      "|    0   N/A  N/A     31576    C+G   ...CBS_cw5n1h2txyewy\\TextInputHost.exe      N/A      |\n",
      "|    0   N/A  N/A     35084    C+G   ...oogle\\Chrome\\Application\\chrome.exe      N/A      |\n",
      "|    0   N/A  N/A     38396    C+G   ...2.0_x64__w1wdnht996qgy\\LinkedIn.exe      N/A      |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "# Activate cuda benchmark\n",
    "#cudnn.benchmark = True\n",
    "\n",
    "# Set device\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Device: {device}\")\n",
    "\n",
    "if device == \"cuda\":\n",
    "    !nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Downloading the COCO Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DOWNLOAD_COCO:\n",
    "    # Define download URLs\n",
    "    coco_urls = {\n",
    "        \"val_images\": \"http://images.cocodataset.org/zips/val2017.zip\",\n",
    "        \"test_images\": \"http://images.cocodataset.org/zips/test2017.zip\",\n",
    "        \"train_images\": \"http://images.cocodataset.org/zips/train2017.zip\",\n",
    "        \"annotations\": \"http://images.cocodataset.org/annotations/annotations_trainval2017.zip\"\n",
    "    }\n",
    "\n",
    "    # Create a directory to store the dataset\n",
    "    dataset_dir = \"d:/Repos/coco_dataset\"\n",
    "    os.makedirs(dataset_dir, exist_ok=True)\n",
    "\n",
    "    # Download function\n",
    "    def download_coco(url, filename):\n",
    "        filepath = os.path.join(dataset_dir, filename)\n",
    "        if not os.path.exists(filepath):\n",
    "            print(f\"Downloading {filename}...\")\n",
    "            urllib.request.urlretrieve(url, filepath)\n",
    "            print(f\"Saved to {filepath}\")\n",
    "        else:\n",
    "            print(f\"{filename} already exists.\")\n",
    "\n",
    "    # Download all files\n",
    "    for key, url in coco_urls.items():\n",
    "        filename = url.split(\"/\")[-1]\n",
    "        download_coco(url, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DOWNLOAD_COCO:\n",
    "    # Unzip the file\n",
    "    PATH = Path(dataset_dir)\n",
    "    \n",
    "    zip_file = PATH / \"val2017.zip\"\n",
    "    with zipfile.ZipFile(zip_file, 'r') as zip_ref:\n",
    "        zip_ref.extractall(dataset_dir)\n",
    "\n",
    "    if zip_file.exists():\n",
    "        os.remove(zip_file)\n",
    "\n",
    "    zip_file = PATH / \"annotations_trainval2017.zip\"\n",
    "    with zipfile.ZipFile(zip_file, 'r') as zip_ref:\n",
    "        zip_ref.extractall(dataset_dir)\n",
    "\n",
    "    if zip_file.exists():\n",
    "        os.remove(zip_file)\n",
    "\n",
    "    zip_file = PATH / \"test2017.zip\"\n",
    "    with zipfile.ZipFile(zip_file, 'r') as zip_ref:\n",
    "        zip_ref.extractall(dataset_dir)\n",
    "\n",
    "    if zip_file.exists():\n",
    "        os.remove(zip_file)\n",
    "\n",
    "    zip_file = PATH / \"train2017.zip\"\n",
    "    with zipfile.ZipFile(zip_file, 'r') as zip_ref:\n",
    "        zip_ref.extractall(dataset_dir)\n",
    "\n",
    "    if zip_file.exists():\n",
    "        os.remove(zip_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Processing the COCO Dataset for Driving Segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "if PROCESS_COCO:\n",
    "    # Path to COCO annotations file\n",
    "    ANNOTATIONS_PATH = r\"D:\\Repos\\coco_dataset\\annotations\\instances_train2017.json\"\n",
    "\n",
    "    # Load COCO annotations\n",
    "    with open(ANNOTATIONS_PATH, \"r\") as f:\n",
    "        coco_data = json.load(f)\n",
    "\n",
    "    # Extract category ID to name mapping\n",
    "    categories = {c[\"id\"]: c[\"name\"] for c in coco_data[\"categories\"]}\n",
    "\n",
    "    # Display all categories\n",
    "    for cat_id, cat_name in categories.items():\n",
    "        print(f\"{cat_id}: {cat_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1: bicycle\n",
      "2: bus\n",
      "3: car\n",
      "4: motorcycle\n",
      "5: stop sign\n",
      "6: traffic light\n",
      "7: truck\n"
     ]
    }
   ],
   "source": [
    "# Create the dictionary describing label -> category), say in alphabetical order\n",
    "#target_categories=['car', 'traffic light', 'stop sign', 'motorcycle', 'bicycle', 'bus', 'truck']\n",
    "target_categories={\n",
    "    1: 'bicycle',\n",
    "    2: 'bus',\n",
    "    3: 'car',\n",
    "    4: 'motorcycle',\n",
    "    5: 'stop sign',\n",
    "    6: 'traffic light',\n",
    "    7: 'truck'}\n",
    "## Display categories\n",
    "for cat_id, cat_name in target_categories.items():\n",
    "    print(f\"{cat_id}: {cat_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "if PROCESS_COCO:\n",
    "    # Training dataset\n",
    "    mapping = COCO_2_ImgMsk(\n",
    "        coco_images_path=      r\"D:\\Repos\\coco_dataset\\train2017\",\n",
    "        coco_annotations_path= r\"D:\\Repos\\coco_dataset\\annotations\\instances_train2017.json\",\n",
    "        output_images_dir=     r\"D:\\Repos\\coco_dataset\\driving\\PNGImages\",\n",
    "        output_masks_dir=      r\"D:\\Repos\\coco_dataset\\driving\\PedMasks\",\n",
    "        class_dictionary=      target_categories,\n",
    "        label=                 \"train\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "if PROCESS_COCO:\n",
    "    # Training dataset\n",
    "    _ = COCO_2_ImgMsk(\n",
    "        coco_images_path=      r\"D:\\Repos\\coco_dataset\\val2017\",\n",
    "        coco_annotations_path= r\"D:\\Repos\\coco_dataset\\annotations\\instances_val2017.json\",\n",
    "        output_images_dir=     r\"D:\\Repos\\coco_dataset\\driving\\PNGImages\",\n",
    "        output_masks_dir=      r\"D:\\Repos\\coco_dataset\\driving\\PedMasks\",\n",
    "        class_dictionary=       target_categories,\n",
    "        label=                 \"val\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "if PROCESS_COCO:\n",
    "    # Split dataset into train (80%), validation (10%), and test (10%) sets\n",
    "    split_dataset(\n",
    "        src_images=       r\"D:\\Repos\\coco_dataset\\driving\\PNGImages\",\n",
    "        src_masks=        r\"D:\\Repos\\coco_dataset\\driving\\PedMasks\",\n",
    "        dst_train_images= r\"C:\\Users\\ssre_\\Projects\\torchsuite\\data\\driving\\train\\PNGImages\",\n",
    "        dst_train_masks=  r\"C:\\Users\\ssre_\\Projects\\torchsuite\\data\\driving\\train\\PedMasks\",\n",
    "        dst_val_images=   r\"C:\\Users\\ssre_\\Projects\\torchsuite\\data\\driving\\val\\PNGImages\",\n",
    "        dst_val_masks=    r\"C:\\Users\\ssre_\\Projects\\torchsuite\\data\\driving\\val\\PedMasks\",\n",
    "        dst_test_images=  r\"C:\\Users\\ssre_\\Projects\\torchsuite\\data\\driving\\test\\PNGImages\",\n",
    "        dst_test_masks=   r\"C:\\Users\\ssre_\\Projects\\torchsuite\\data\\driving\\test\\PedMasks\",\n",
    "        train_pct=        0.80,\n",
    "        val_pct=          0.10,\n",
    "        test_pct=         0.10,\n",
    "        seed=             SEED\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What Information Do x1 and x2 Contain?\n",
    "x1: The UpSampled Feature Map\n",
    "\n",
    "* x1 is the output from the previous layer after upsampling using ConvTranspose2d.\n",
    "* It comes from a deeper layer of the network (lower resolution, more abstract features).\n",
    "* It contains high-level, semantic information about objects and structures in the image.\n",
    "* Since it's upsampled, it lacks fine-grained spatial details (edges, textures).\n",
    "* x2: The Skip Connection from the Encoder\n",
    "\n",
    "* x2 is taken from an earlier encoder layer before downsampling.\n",
    "* It has a higher spatial resolution (more fine details).\n",
    "* It contains low-level features such as edges, textures, and shapes.\n",
    "* Since it comes from an early stage, it lacks deep semantic meaning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Preparing Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The dataset contains two classes only: background and person\n",
    "NUM_CLASSES = len(target_categories)\n",
    "BATCH_SIZE = 8\n",
    "IMG_SIZE = (512, 512)\n",
    "AUGMENT_MAGNITUDE = 3 # Max is 5\n",
    "\n",
    "# Define training, validation, and test data loaders\n",
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "    dataset=ProcessDatasetSegmentation(\n",
    "        root='data/driving/train',\n",
    "        image_path=\"PNGImages\",\n",
    "        mask_path=\"PedMasks\",\n",
    "        transforms=SegmentationTransforms(\n",
    "            train=True,\n",
    "            img_size=IMG_SIZE,\n",
    "            mean_std_norm=True,\n",
    "            augment_magnitude=AUGMENT_MAGNITUDE\n",
    "            ),\n",
    "        class_dictionary=target_categories\n",
    "        ),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "val_dataloader = torch.utils.data.DataLoader(\n",
    "    dataset=ProcessDatasetSegmentation(\n",
    "        root='data/driving/val',\n",
    "        image_path=\"PNGImages\",\n",
    "        mask_path=\"PedMasks\",\n",
    "        transforms=SegmentationTransforms(\n",
    "            train=False,\n",
    "            img_size=IMG_SIZE,\n",
    "            mean_std_norm=True,\n",
    "            augment_magnitude=AUGMENT_MAGNITUDE\n",
    "            ),\n",
    "        class_dictionary=target_categories\n",
    "        ),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "test_dataloader = torch.utils.data.DataLoader(\n",
    "    dataset=ProcessDatasetSegmentation(\n",
    "        root='data/driving/test',\n",
    "        image_path=\"PNGImages\",\n",
    "        mask_path=\"PedMasks\",\n",
    "        transforms=SegmentationTransforms(\n",
    "            train=False,\n",
    "            img_size=IMG_SIZE,\n",
    "            mean_std_norm=True,\n",
    "            augment_magnitude=AUGMENT_MAGNITUDE\n",
    "            ),\n",
    "        class_dictionary=target_categories\n",
    "        ),\n",
    "    batch_size=2,\n",
    "    shuffle=False,\n",
    "    collate_fn=collate_fn\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Visualize Images with Masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "if VISUALIZE_TRANSFORMED_DATA:\n",
    "    # Visualize transformations\n",
    "    dataloader_nt = torch.utils.data.DataLoader(\n",
    "        dataset=ProcessDatasetSegmentation(\n",
    "            root='data/driving/train',\n",
    "            image_path=\"PNGImages\",\n",
    "            mask_path=\"PedMasks\",\n",
    "            transforms=SegmentationTransforms(\n",
    "                train=False,\n",
    "                img_size=IMG_SIZE,\n",
    "                mean_std_norm=False,\n",
    "                augment_magnitude=AUGMENT_MAGNITUDE\n",
    "                ),\n",
    "            class_dictionary=target_categories\n",
    "            ), \n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=False,\n",
    "        collate_fn=collate_fn)\n",
    "    dataloader_t = torch.utils.data.DataLoader(\n",
    "        dataset=ProcessDatasetSegmentation(\n",
    "            root='data/driving/train',\n",
    "            image_path=\"PNGImages\",\n",
    "            mask_path=\"PedMasks\",\n",
    "            transforms=SegmentationTransforms(\n",
    "                train=True,\n",
    "                img_size=IMG_SIZE,\n",
    "                mean_std_norm=False,\n",
    "                augment_magnitude=AUGMENT_MAGNITUDE\n",
    "                ),\n",
    "            class_dictionary=target_categories\n",
    "            ), \n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=False,\n",
    "        collate_fn=collate_fn)\n",
    "    \n",
    "    for idx, ((img_nt, target_nt), (img_t, target_t)) in enumerate(zip(dataloader_nt, dataloader_t)):   \n",
    "        for i in range(0, BATCH_SIZE):\n",
    "\n",
    "            # Set up the figure\n",
    "            fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "            \n",
    "            # Pass subplot axes to the function\n",
    "            mask_nt = collapse_one_hot_mask(target_nt[i])\n",
    "            mask_t = collapse_one_hot_mask(target_t[i])\n",
    "\n",
    "            # Create the label-class dictionary for the mask\n",
    "            classes_nt = create_label_class_dict(target_nt[i], target_categories)\n",
    "            classes_t = create_label_class_dict(target_t[i], target_categories)\n",
    "\n",
    "            # And generate the titles\n",
    "            title_nt = f\"Original: {', '.join(classes_nt.values())}\"\n",
    "            title_t = f\"Transformed: {', '.join(classes_t.values())}\"\n",
    "            \n",
    "            # Display overlaid images\n",
    "            display_image_with_mask(img_nt[i], mask_nt, ax=axes[0], alpha=0.5, cmap='YlGnBu', title=title_nt)\n",
    "            display_image_with_mask(img_t[i], mask_t, ax=axes[1], alpha=0.5, cmap='YlOrRd', title=title_t)\n",
    "            \n",
    "            plt.show() \n",
    "            \n",
    "        if idx > 4:\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Creating the U-Net Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "=======================================================================================================================================\n",
       "Layer (type (var_name))                                 Input Shape          Output Shape         Param #              Trainable\n",
       "=======================================================================================================================================\n",
       "OptimizedModule (OptimizedModule)                       [8, 3, 512, 512]     [8, 7, 512, 512]     --                   True\n",
       "├─UNet (_orig_mod)                                      [8, 3, 512, 512]     [8, 7, 512, 512]     --                   True\n",
       "│    └─ModuleList (down_convs)                          --                   --                   --                   True\n",
       "│    │    └─DownConvert (0)                             [8, 3, 512, 512]     [8, 64, 512, 512]    38,976               True\n",
       "│    │    └─DownConvert (1)                             [8, 64, 256, 256]    [8, 128, 256, 256]   221,952              True\n",
       "│    │    └─DownConvert (2)                             [8, 128, 128, 128]   [8, 256, 128, 128]   886,272              True\n",
       "│    │    └─DownConvert (3)                             [8, 256, 64, 64]     [8, 512, 64, 64]     3,542,016            True\n",
       "│    └─DoubleConv2D (last_layer)                        [8, 512, 32, 32]     [8, 1024, 32, 32]    --                   True\n",
       "│    │    └─Sequential (double_conv2d)                  [8, 512, 32, 32]     [8, 1024, 32, 32]    14,161,920           True\n",
       "│    └─ModuleList (up_convs)                            --                   --                   --                   True\n",
       "│    │    └─UpConvert (0)                               [8, 1024, 32, 32]    [8, 512, 64, 64]     9,178,624            True\n",
       "│    │    └─UpConvert (1)                               [8, 512, 64, 64]     [8, 256, 128, 128]   2,295,552            True\n",
       "│    │    └─UpConvert (2)                               [8, 256, 128, 128]   [8, 128, 256, 256]   574,336              True\n",
       "│    │    └─UpConvert (3)                               [8, 128, 256, 256]   [8, 64, 512, 512]    143,808              True\n",
       "│    └─Conv2d (output)                                  [8, 64, 512, 512]    [8, 7, 512, 512]     455                  True\n",
       "=======================================================================================================================================\n",
       "Total params: 31,043,911\n",
       "Trainable params: 31,043,911\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.TERABYTES): 1.75\n",
       "=======================================================================================================================================\n",
       "Input size (MB): 25.17\n",
       "Forward/backward pass size (MB): 18505.27\n",
       "Params size (MB): 124.18\n",
       "Estimated Total Size (MB): 18654.61\n",
       "======================================================================================================================================="
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = UNet(\n",
    "    in_channels=3,\n",
    "    num_classes=NUM_CLASSES,\n",
    "    num_layers=5,\n",
    "    batch_norm=True,\n",
    ")\n",
    "\n",
    "# Compile model\n",
    "model = torch.compile(model, backend=\"aot_eager\")\n",
    "\n",
    "summary(model,\n",
    "        input_size=(BATCH_SIZE,3, IMG_SIZE[0], IMG_SIZE[1]),\n",
    "        col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
    "        col_width=20,\n",
    "        row_settings=[\"var_names\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-1e9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv_pytorch_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
