{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "41e3e0ca",
   "metadata": {},
   "source": [
    "# 1. Introduction\n",
    "\n",
    "This notebook outlines the creation, compilation, and training of a deep learing network using the [TorchSuite](https://github.com/sergio-sanz-rodriguez/torchsuite) framework. In particular, the ConvNeXt model will be used to make predictions on visual quality scores.\n",
    "\n",
    "## 1.1. Motivation\n",
    "\n",
    "The widespread use of aesthetic filters on social media introduces new challenges for Image Quality Assessment (IQA), as traditional distortion-based metrics often fail to capture the subjective, content-aware characteristics of these enhancements. This notebook proposes a no-reference IQA baseline model based on the ConvNeXt-Large architecture to evaluate the subjective quality of filter-manipulated images. The output of the model is a Mean Opinion Score (MOS)-like grade scale ranging from 0 to 1, where 1 represents excellent subjective quality and 0 represents poor subjective quality. More details about the proposed model can be found in the TorchSuite repository: papers/ConvNeXt_Ensemble_IQA.pdf.\"\n",
    "\n",
    "For the sake of simplicity, this notebook focuses only on the training process of a single ConvNeXt-Large model, and does not implement the entire pipeline.\n",
    "\n",
    "This work is part of the VCIP 2025 conference’s Image Manipulation Quality Assessment (IMQA) Grand Challenge: https://jiangliu5.github.io/imqac.github.io/. The image database is publicly available through the link."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dba7507",
   "metadata": {},
   "source": [
    "# 2. Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6892a15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.backends.cudnn as cudnn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "from torchinfo import summary\n",
    "from torchvision.transforms import v2\n",
    "from pathlib import Path\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR, CosineAnnealingWarmRestarts, StepLR\n",
    "from random import sample\n",
    "from torchvision.transforms import InterpolationMode\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# Import custom libraries\n",
    "from engines.regression import RegressionEngine\n",
    "from engines.schedulers import FixedLRSchedulerWrapper\n",
    "from utils.common_utils import download_data, set_seeds\n",
    "from utils.regression_utils import display_random_images_regression\n",
    "from dataloaders.image_dataloaders import RegressionDataset, create_regression_dataloaders\n",
    "from models.convnext import convnext_tiny, convnext_small, convnext_base, convnext_large, convnext_xlarge, model_urls\n",
    "from models.pretrained_models import build_pretrained_model\n",
    "\n",
    "import warnings\n",
    "os.environ['TORCH_USE_CUDA_DSA'] = \"1\"\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"torch.autograd.graph\")\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning, module=\"onnxscript.converter\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6555499",
   "metadata": {},
   "source": [
    "# 3. Importing Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1e2e08c",
   "metadata": {},
   "source": [
    "The dataset is organized as follows, with one subdirectory for the images and another subdirectory containing the CSV files with the scores:\n",
    "\n",
    "```\n",
    "dataset/\n",
    "├── all_images/   \n",
    "│   ├── img1.jpg\n",
    "│   ├── img2.png\n",
    "│   └── ...\n",
    "└── labels/\n",
    "    └── train_labels.csv\n",
    "    └── test_labels.csv\n",
    "```\n",
    "\n",
    "Alternatively:\n",
    "\n",
    "```\n",
    "dataset/\n",
    "├── all_images/   \n",
    "│   ├── img1.jpg\n",
    "│   ├── img2.png\n",
    "│   └── ...\n",
    "└── labels/\n",
    "    └── labels.csv\n",
    "```\n",
    "\n",
    "The labels.csv file can be split into two separate CSV files or dataframes: one for training and another for validation/testing.\n",
    "\n",
    "The CSV file(s) can also be placed in a different location within the dataset directory if preferred.\n",
    "\n",
    "The CSV file(s) should contain at least two columns:\n",
    "\n",
    "1. Image paths in the first column, and\n",
    "2. Scores (e.g., MOS) in the second column:\n",
    "\n",
    "```\n",
    "image_name,mos\n",
    "Act2_clahe_1,0.372379619\n",
    "Act4_toning_2,0.705094234\n",
    "```\n",
    "\n",
    "Alternatively:\n",
    "\n",
    "```\n",
    "image_name,mos\n",
    "Act2_clahe_1.jpg,0.372379619\n",
    "Act4_toning_2.jpg,0.705094234\n",
    "```\n",
    "\n",
    "Or:\n",
    "\n",
    "```\n",
    "image_name,mos\n",
    "<path>/Act2_clahe_1.jpg,0.372379619\n",
    "<path>/Act4_toning_2.jpg,0.705094234\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ea443bb-470a-47e5-8f4d-341abf4e4d6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define some constants\n",
    "NUM_WORKERS = os.cpu_count()\n",
    "AMOUNT_TO_GET = 1.0\n",
    "SEED = 42\n",
    "THEME = 'dark'\n",
    "\n",
    "# Define target data directory\n",
    "\n",
    "DONWLOAD_DATA = False\n",
    "if DONWLOAD_DATA:\n",
    "    download_data(\n",
    "        source=\"https://drive.google.com/uc?export=download&id=1d81_Lb7J1fpVU7Jw60cvXYrmYF3Qa8lU\",\n",
    "        destination=\"data/regression\")\n",
    "BASELINE_NAME = f\"data/regression/VCIP\"\n",
    "BASELINE = Path(BASELINE_NAME)\n",
    "TARGET_DIR = BASELINE / \"EQ420_image\"\n",
    "TARGET_LABEL = BASELINE / \"Labels\"\n",
    "TARGET_BASE = BASELINE / \"IMQA\"\n",
    "\n",
    "# Setup training and test directories\n",
    "TARGET_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Create target model directory\n",
    "MODEL_DIR = Path(\"outputs\")\n",
    "MODEL_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Set seeds\n",
    "def seed_torch(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    set_seeds(seed)\n",
    "    # torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "seed_torch(seed=SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c17cba38",
   "metadata": {},
   "source": [
    "# 4. Classes and Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e2a9f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def oversample_mos_ranges(df, target_count=60, bins=5, random_state=42):\n",
    "    \"\"\"\n",
    "    Oversamples underrepresented MOS ranges to reach a target count per bin.\n",
    "    Does NOT undersample overrepresented bins — all original data is kept.\n",
    "    \"\"\"\n",
    "    # Bin the MOS scores\n",
    "    df['mos_bin'] = pd.cut(df['mos'], bins=bins)\n",
    "\n",
    "    resampled_dfs = []\n",
    "\n",
    "    for bin_range, bin_df in df.groupby('mos_bin'):\n",
    "        if len(bin_df) < target_count:\n",
    "            # Oversample with replacement\n",
    "            resampled_bin = bin_df.sample(\n",
    "                n=target_count - len(bin_df),\n",
    "                replace=True,\n",
    "                random_state=random_state\n",
    "            )\n",
    "            # Combine original + oversampled\n",
    "            resampled_dfs.append(pd.concat([bin_df, resampled_bin]))\n",
    "        else:\n",
    "            # Keep as is\n",
    "            resampled_dfs.append(bin_df)\n",
    "\n",
    "    resampled_df = pd.concat(resampled_dfs).drop(columns='mos_bin').reset_index(drop=True)\n",
    "    return resampled_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c279df5f",
   "metadata": {},
   "source": [
    "# 5. Specifying the Target Device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f01afed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activate cuda benchmark\n",
    "cudnn.benchmark = True\n",
    "\n",
    "# Set device\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Device: {device}\")\n",
    "\n",
    "if device == \"cuda\":\n",
    "    !nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "350701ec-c5f9-4809-884c-69a5dcf97ceb",
   "metadata": {},
   "source": [
    "# 6. Image Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d670c088",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display images\n",
    "manual_transforms = v2.Compose([\n",
    "    v2.Resize((512)),\n",
    "    v2.ToImage(),\n",
    "    v2.ToDtype(torch.float32, scale=True)\n",
    "])\n",
    "\n",
    "csv_data = pd.read_csv(TARGET_LABEL / 'mos_fold_train.csv').sample(frac=1)\n",
    "fold_list = list(range(1, 9))\n",
    "fold_train = sample(fold_list, 7)\n",
    "train_data = csv_data[csv_data['folds'].isin(fold_train)]\n",
    "train_data.drop('folds', axis=1, inplace=True)\n",
    "train_dataset = RegressionDataset(\n",
    "    data=train_data,\n",
    "    image_folder=TARGET_DIR,\n",
    "    transform=manual_transforms,\n",
    "    )\n",
    "\n",
    "fold_test = list(set(fold_list) - set(fold_train))\n",
    "test_data = csv_data[csv_data['folds'].isin(fold_test)].copy()\n",
    "test_data.drop('folds', axis=1, inplace=True)\n",
    "\n",
    "##datasets.ImageFolder(TRAIN_DIR, transform=manual_transforms)\n",
    "display_random_images_regression(\n",
    "    train_dataset,\n",
    "    n=15,                   \n",
    "    rows=5,\n",
    "    cols=3,\n",
    "    theme=THEME,\n",
    "    display_shape=False,\n",
    "    seed=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5d87e29",
   "metadata": {},
   "source": [
    "# 7. Preparing Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ee71d8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the csv file into two dataframes: train and test\n",
    "csv_data = pd.read_csv(TARGET_LABEL / 'mos_fold_train.csv').sample(frac=1)\n",
    "fold_list = list(range(1, 9))\n",
    "fold_train = sample(fold_list, 7) # Train split: 87.5%\n",
    "fold_test = list(set(fold_list) - set(fold_train))  # Test split: 12.5%\n",
    "train_data = csv_data[csv_data['folds'].isin(fold_train)]\n",
    "test_data = csv_data[csv_data['folds'].isin(fold_test)].copy()\n",
    "train_data.drop('folds', axis=1, inplace=True)\n",
    "test_data.drop('folds', axis=1, inplace=True)\n",
    "\n",
    "# Pre-processing transformations\n",
    "IMG_SIZE = 512\n",
    "BATCH_SIZE = 2\n",
    "train_transforms = v2.Compose([\n",
    "    v2.Resize((IMG_SIZE), interpolation=InterpolationMode.BICUBIC),\n",
    "    v2.RandomHorizontalFlip(p=0.2),\n",
    "    v2.RandomResizedCrop(IMG_SIZE, scale=(0.95, 1.0), ratio=(0.95, 1.05)),\n",
    "    v2.ToImage(),\n",
    "    v2.ToDtype(torch.float32, scale=True),\n",
    "    v2.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "test_transforms = v2.Compose([\n",
    "    v2.Resize((IMG_SIZE), interpolation=InterpolationMode.BICUBIC),    \n",
    "    v2.ToImage(),\n",
    "    v2.ToDtype(torch.float32, scale=True),\n",
    "    v2.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "# Create the training and test dataloaders\n",
    "train_dataloader, test_dataloader = create_regression_dataloaders(\n",
    "        train_data=train_data,\n",
    "        test_data=test_data,\n",
    "        train_image_folder=TARGET_DIR,\n",
    "        test_image_folder=TARGET_DIR,\n",
    "        train_transform=train_transforms,\n",
    "        test_transform=test_transforms,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        num_workers=0 #NUM_WORKERS\n",
    "        )\n",
    "dataloaders = {\n",
    "    'train':         train_dataloader,\n",
    "    'test':          test_dataloader\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2d46722",
   "metadata": {},
   "source": [
    "# 8. Creating a ConvNeXT-Large Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51faeca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the model\n",
    "NUM_METRICS = 1\n",
    "DROPOUT = 0.3\n",
    "HIDDEN_DIM = 256\n",
    "MODEL_ARCH=\"convnext_large\"\n",
    "\n",
    "# Build a ConvNeXt-Large model from the convnext.py library\n",
    "model = convnext_large(\n",
    "    pretrained=True,\n",
    "    in_22k=True,\n",
    "    freeze_backbone=False,\n",
    "    mlp_hidden_dim=HIDDEN_DIM,\n",
    "    output_dim=NUM_METRICS,\n",
    "    drop_path_rate=DROPOUT,\n",
    "    dropout=DROPOUT,\n",
    "    )\n",
    "\n",
    "# Or just use PyTorch's default ConvNeXt.\n",
    "# However, for this task better performance is achieved by the prevous model\n",
    "#model = build_pretrained_model(\n",
    "#    model=\"convnext_l\",\n",
    "#    mlp_hidden_dim=HIDDEN_DIM,\n",
    "#    output_dim=NUM_METRICS,\n",
    "#    dropout=DROPOUT,\n",
    "#    freeze_backbone=False,\n",
    "#    device=device,\n",
    "#    seed=SEED)\n",
    "\n",
    "summary(model,\n",
    "        input_size=(BATCH_SIZE,3,IMG_SIZE, IMG_SIZE),\n",
    "        col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
    "        col_width=20,\n",
    "        row_settings=[\"var_names\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28a1291e",
   "metadata": {},
   "source": [
    "# 7. Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4addfb66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constant definition\n",
    "EPOCHS = 150\n",
    "LR = 1e-5\n",
    "model_type=\"model_convnext_reg\"\n",
    "model_name = model_type + \".pth\"\n",
    "\n",
    "# Create AdamW optimizer\n",
    "optimizer = torch.optim.AdamW(\n",
    "    params=model.parameters(),\n",
    "    lr=LR,\n",
    "    betas=(0.9, 0.999),\n",
    "    weight_decay=1e-4\n",
    ")\n",
    "\n",
    "# Create loss function: MSE oss\n",
    "loss_fn = torch.nn.MSELoss()\n",
    "# Or Huber loss with delta between 0.0 and 1.0:\n",
    "# Low delta → more robust to outliers: switches to Mean Absolute Error (L1 loss) sooner, good for noisy data\n",
    "# Higher delta → smoother fit: stays in Mean Squared Error (L2 loss) region longer, better for clean data\n",
    "# loss_fn = torch.nn.HuberLoss(delta=0.1)\n",
    "\n",
    "# Create scheduler\n",
    "scheduler = CosineAnnealingWarmRestarts(optimizer, T_0=10, T_mult=2, eta_min=LR/100)\n",
    "#scheduler = CosineAnnealingLR(optimizer, T_max=EPOCHS, eta_min=LR/100)\n",
    "\n",
    "# And train...\n",
    "\n",
    "# Instantiate the classification engine with the created model and the target device\n",
    "engine = RegressionEngine(\n",
    "    model=model,                                # Model to be trained\n",
    "    optimizer=optimizer,                        # Optimizer\n",
    "    loss_fn=loss_fn,                            # Loss function\n",
    "    scheduler=scheduler,                        # Scheduler \n",
    "    use_distillation=False,                     # Optional, use_distillation is False by default    \n",
    "    log_verbose=True,                           # Verbosity\n",
    "    theme=THEME,                                # Theme\n",
    "    device=device                               # Target device\n",
    "    )\n",
    "\n",
    "# Configure the training method\n",
    "results = engine.train(\n",
    "    target_dir=MODEL_DIR,                       # Directory where the model will be saved\n",
    "    model_name=model_name,                      # Name of the model\n",
    "    resume=False,                               # Resume training from the last saved checkpoint\n",
    "    save_best_model=[\"last\", \"loss\", \"r2\"],     # Save the best models based on different criteria\n",
    "    keep_best_models_in_memory=False,           # Do not keep the models stored in memory for the sake of training time and memory efficiency    \n",
    "    dataloaders=dataloaders,                    # Dictionary with the dataloaders     \n",
    "    apply_validation=True,                      # Enable validation step\n",
    "    augmentation_strategy=\"always\",             # Augmentation strategy    \n",
    "    epochs=EPOCHS,                              # Total number of epochs\n",
    "    amp=True,                                   # Enable Automatic Mixed Precision (AMP)\n",
    "    enable_clipping=False,                      # Disable clipping on gradients, only useful if training becomes unestable\n",
    "    debug_mode=False,                           # Disable debug mode    \n",
    "    accumulation_steps=2,                       # Accumulation steps 2: effective batch size = batch_size x accumulation steps\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv_pytorch_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
