{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook outlines the creation, compilation, and training of a deep learning network for object detection, in particular, for detecting pedestrians in images. It is quite similar to the `object_detection_rcnn_standard.ipynb` notebook, with the primary difference being how the R-CNN model is created. In this version, the model is more flexible, allowing for the use of different backbones, custom anchors, and alternative ROI pooling configurations.\n",
    "\n",
    "The notebook is fundamentally based on the PyTorch tutorial for object segmentation that is available [here](https://pytorch.org/tutorials/intermediate/torchvision_tutorial.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Object Detection and Segmentation\n",
    "\n",
    "Object detection is a fundamental task in computer vision that involves identifying and localizing objects within an image. Unlike image classification, which assigns a single label to an entire image, object detection provides:\n",
    "\n",
    "* Bounding boxes around detected objects.\n",
    "* Class labels to categorize each detected object.\n",
    "\n",
    "Object segmentation goes a step further by providing pixel-wise masks for objects instead of just bounding boxes. It is categorized into:\n",
    "\n",
    "* Semantic Segmentation (labels each pixel but does not distinguish between instances).\n",
    "* Instance Segmentation (detects and segments each object separately, such as Mask R-CNN).\n",
    "\n",
    "# 3. The R-CNN Model: Two-Stage Object Detection\n",
    "\n",
    "A Region-Based Convolutional Neural Network (R-CNN) is a two-stage object detection model that first generates region proposals and then classifies and refines them. The Region Proposal Network (RPN) scans the image and suggests candidate object regions, predicting bounding boxes and objectness scores. These proposals are then passed to the Region of Interest (RoI) Head, which classifies objects, refines box coordinates, and (in the case of Mask R-CNN) predicts segmentation masks. This two-stage approach delivers high accuracy but is computationally intensive compared to single-stage detectors like YOLO and SSD.\n",
    "\n",
    "**Stage 1: Region Proposal Network (RPN)**\n",
    "\n",
    "The Region Proposal Network (RPN) is responsible for generating object proposals, that is, regions of the image that are likely to contain objects. It works as follows:\n",
    "\n",
    "1. Sliding window over feature Map: The RPN slides over the feature map produced by the backbone CNN (e.g., ResNet, VGG).\n",
    "2. Anchor boxes: For each sliding window position, the RPN predicts multiple anchor boxes (default bounding boxes at various sizes and aspect ratios).\n",
    "3. Objectness score: the network outputs a score for each anchor, determining whether it contains an object (foreground) or background.\n",
    "4. Bounding box refinement: The RPN predicts adjustments to the anchor box coordinates to refine object localization.\n",
    "\n",
    "**Stage 2: Region of Interest (RoI)**\n",
    "\n",
    "The RoI Head takes the region proposals from the RPN and performs classification, bounding box refinement, and (for Mask R-CNN) segmentation mask prediction.\n",
    "\n",
    "1. RoI pooling (or RoIAlign in Mask R-CNN): The variable-sized proposed regions from RPN are resized into a fixed size to be fed into a fully connected layer.\n",
    "2. Classification & bounding box refinement: The RoI head classifies the detected object into one of the predefined categories. It further refines the bounding box predictions.\n",
    "3. (For Mask R-CNN) Segmentation Mask Prediction: A separate mask branch is used to predict per-pixel masks for each detected object."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Links of Interest\n",
    "* https://medium.com/@soumyajitdatta123/faster-rcnns-explained-af76f96a0b70\n",
    "* https://medium.com/@RobuRishabh/understanding-and-implementing-faster-r-cnn-248f7b25ff96"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generic libraries\n",
    "import os\n",
    "import torch\n",
    "import glob\n",
    "import zipfile\n",
    "import torch.backends.cudnn as cudnn\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from torchinfo import summary\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR, ConstantLR, SequentialLR\n",
    "\n",
    "# Torchvision libraries\n",
    "from torchvision import models\n",
    "from torchvision.ops import MultiScaleRoIAlign\n",
    "from torchvision import tv_tensors\n",
    "from torchvision.io import read_image\n",
    "from torchvision.transforms import v2 as T\n",
    "from torchvision.utils import draw_bounding_boxes\n",
    "from torchvision.models.detection.rpn import AnchorGenerator\n",
    "from torchvision.models.detection import FasterRCNN\n",
    "\n",
    "# Import custom libraries\n",
    "from utils.common_utils import set_seeds, load_model\n",
    "from utils.obj_detection_utils import collate_fn, prune_predictions, display_and_save_predictions, visualize_transformed_data\n",
    "from engines.obj_detection import ObjectDetectionEngine\n",
    "from engines.schedulers import FixedLRSchedulerWrapper\n",
    "from dataloaders.obj_dect_dataloaders import ProcessDataset\n",
    "from models.faster_rcnn import StandardFasterRCNN, CustomFasterRCNN\n",
    "\n",
    "# Warnings\n",
    "import warnings\n",
    "os.environ['TORCH_USE_CUDA_DSA'] = \"1\"\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"torch.autograd.graph\")\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning, module=\"onnxscript.converter\")\n",
    "\n",
    "# Create target model directory\n",
    "MODEL_DIR = Path(\"outputs\")\n",
    "MODEL_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Set seeds\n",
    "SEED = 42\n",
    "set_seeds(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Specifying the Target Device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activate cuda benchmark\n",
    "#cudnn.benchmark = True\n",
    "\n",
    "# Set device\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Device: {device}\")\n",
    "\n",
    "if device == \"cuda\":\n",
    "    !nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Downloading the Penn-Fundan Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the dataset\n",
    "!curl -L -k https://www.cis.upenn.edu/~jshi/ped_html/PennFudanPed.zip -o data/PennFudanPed.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After executing the code below, the dataset will be organized into three subdirectories: Annotation, PNGImages, and PedMasks:\n",
    "\n",
    "```\n",
    "PennFudanPed/\n",
    "├── Annotation/\n",
    "│   ├── img1.txt\n",
    "│   ├── img2.txt\n",
    "│   └── ...\n",
    "├── PNGImages/\n",
    "│   ├── img1.png\n",
    "│   ├── img2.png\n",
    "│   └── ...\n",
    "├── PedMasks/\n",
    "│   ├── img1.png\n",
    "│   ├── img2.png\n",
    "│   └── ...\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the downloaded zip file\n",
    "zip_file_path = \"data/PennFudanPed.zip\"\n",
    "extract_dir = \"data\"\n",
    "\n",
    "# Ensure the extraction directory exists\n",
    "os.makedirs(extract_dir, exist_ok=True)\n",
    "\n",
    "# Unzip the file\n",
    "with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
    "    zip_ref.extractall(extract_dir)\n",
    "\n",
    "print(f\"Files extracted to {extract_dir}\")\n",
    "\n",
    "zip_file = Path(zip_file_path)\n",
    "if zip_file.exists():\n",
    "    os.remove(zip_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Image and Mask Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "image = read_image(\"data/PennFudanPed/PNGImages/FudanPed00046.png\")\n",
    "mask = read_image(\"data/PennFudanPed/PedMasks/FudanPed00046_mask.png\")\n",
    "\n",
    "plt.figure(figsize=(16, 8))\n",
    "plt.subplot(121)\n",
    "plt.title(\"Image\")\n",
    "plt.imshow(image.permute(1, 2, 0))\n",
    "plt.subplot(122)\n",
    "plt.title(\"Mask\")\n",
    "plt.imshow(mask.permute(1, 2, 0))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Preparing Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Pre-processing transformations\n",
    "def get_transform(train, mean_std_norm):\n",
    "    transforms = []\n",
    "    if train:\n",
    "        transforms.append(T.RandomHorizontalFlip(p=0.5))\n",
    "        transforms.append(T.RandomVerticalFlip(p=0.5))\n",
    "        transforms.append(T.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2))\n",
    "        transforms.append(T.GaussianBlur(kernel_size=(3, 3), sigma=(0.1, 2.0)))\n",
    "        transforms.append(T.RandomPerspective(distortion_scale=0.2, p=0.5))\n",
    "        transforms.append(T.RandomGrayscale(p=0.1))\n",
    "        transforms.append(T.RandomZoomOut(fill={tv_tensors.Image: (0, 0, 0), \"others\": 0}, side_range=(1.0, 2.0), p=0.2)), #(123, 117, 104)\n",
    "    transforms.append(T.ToDtype(torch.float, scale=True))\n",
    "    if mean_std_norm:\n",
    "        transforms.append(T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]))\n",
    "    transforms.append(T.ToPureTensor())\n",
    "    return T.Compose(transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The dataset contains two classes only: background and person\n",
    "NUM_CLASSES = 2\n",
    "BATCHES = 2\n",
    "THEME = 'light' # or 'dark'. Default is 'light'\n",
    "\n",
    "# Use ther dataset and defined transformations\n",
    "dataset_tr = ProcessDataset(\n",
    "    root='data/PennFudanPed',\n",
    "    image_path=\"PNGImages\",\n",
    "    mask_path=\"PedMasks\",\n",
    "    transforms=get_transform(train=True, mean_std_norm=False),\n",
    "    num_classes=NUM_CLASSES-1) # exclude the background\n",
    "\n",
    "dataset_ntr = ProcessDataset(\n",
    "    root='data/PennFudanPed',\n",
    "    image_path=\"PNGImages\",\n",
    "    mask_path=\"PedMasks\",\n",
    "    transforms=get_transform(train=False, mean_std_norm=False),\n",
    "    num_classes=NUM_CLASSES-1) # exclude the background\n",
    "\n",
    "# Split the dataset in train and test set\n",
    "indices = torch.randperm(len(dataset_tr)).tolist()\n",
    "train_dataset = torch.utils.data.Subset(dataset_tr, indices[:-25])\n",
    "test_dataset = torch.utils.data.Subset(dataset_ntr, indices[-25:])\n",
    "test_dataset_t = torch.utils.data.Subset(dataset_tr, indices[-25:])\n",
    "\n",
    "# Define training and validation data loaders\n",
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=BATCHES,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "test_dataloader = torch.utils.data.DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=BATCHES,\n",
    "    shuffle=False,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "# Define training and validation data loaders\n",
    "test_dataloader_t = torch.utils.data.DataLoader(\n",
    "    test_dataset_t,\n",
    "    batch_size=2,\n",
    "    shuffle=False,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "dataloaders = {\n",
    "    'train':         train_dataloader,\n",
    "    'test':          test_dataloader\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize transformations\n",
    "for idx, ((img, target), (img_t, target_t)) in enumerate(zip(test_dataloader, test_dataloader_t)):   \n",
    "    for i in range(0, BATCHES):\n",
    "        visualize_transformed_data(img[i], target[i], img_t[i], target_t[i], theme=THEME)\n",
    "    if idx > 4:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. Creating a Custom Object Detection Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before implementing the R-CNN, let's define some key parameters that will help us better understand the role of each stage in the model:\n",
    "\n",
    "- **Backbone**: - **Backbone**: This is the neural network used to extract features from the input image. It serves as the feature extractor for the object detection model. It produces a set of feature maps, each associated with a different layer of the neural network.\n",
    "\n",
    "- **Anchor Generator**: This defines a set of predefined bounding box shapes that are used as initial references for detecting objects in the image. These anchors are essential for region proposal.\n",
    "- **ROI Pooler** This is the algorithm responsible for mapping each Region of Interest (RoI) onto a fixed-size feature map. It ensures that the features corresponding to each RoI are of a consistent size, regardless of the original dimensions of the RoI.\n",
    "    - **featmap_names**: Specifies the source of the feature maps used for extracting RoIs. Typically, this will be the last feature map produced by the backbone (specified as ['0']), which captures the most refined features.\n",
    "    - **output_size**: Determines the size of the output RoI features. A higher output size retains more spatial detail, while a smaller size may reduce computation but lose some fine details.\n",
    "    - **sampling_ratio**: It controls how many points are sampled from the feature map to create a fixed-size output for each RoI. A higher ratio means more precise sampling, but it also increases computational cost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the backbone: https://pytorch.org/vision/0.20/models.html\n",
    "backbone = models.efficientnet_b0(weights=models.EfficientNet_B0_Weights.DEFAULT)\n",
    "#backbone = models.convnext_small(weights=models.ConvNeXt_Small_Weights.DEFAULT),\n",
    "#backbone = models.efficientnet_v2_s(weights=models.EfficientNet_V2_S_Weights.DEFAULT)\n",
    "\n",
    "# Create the R-CNN model\n",
    "model = CustomFasterRCNN(\n",
    "    num_classes=NUM_CLASSES,\n",
    "    backbone=backbone,\n",
    "    anchor_generator=AnchorGenerator(\n",
    "        sizes=((32, 64, 128, 256, 512),),\n",
    "        aspect_ratios=((0.5, 1.0, 2.0),)),\n",
    "    roi_pooler=MultiScaleRoIAlign(\n",
    "        featmap_names=['0'],\n",
    "        output_size=7,\n",
    "        sampling_ratio=2),\n",
    "    device=device\n",
    ")\n",
    "\n",
    "summary(model,\n",
    "        input_size=(1,3,224, 224),\n",
    "        col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
    "        col_width=20,\n",
    "        row_settings=[\"var_names\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10. Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Model name\n",
    "model_type=\"model_rcnn_cstm_obj\"\n",
    "model_name = model_type + \".pth\"\n",
    "EPOCHS = 30\n",
    "LR = 1e-4\n",
    "ETAMIN = 1e-7\n",
    "\n",
    "# Create AdamW optimizer\n",
    "optimizer = torch.optim.AdamW(\n",
    "    params=model.parameters(),\n",
    "    lr=LR,\n",
    "    betas=(0.9, 0.999),\n",
    "    weight_decay=0.01\n",
    ")\n",
    "\n",
    "# Set scheduler\n",
    "cosine = CosineAnnealingLR(optimizer, T_max=EPOCHS-5, eta_min=ETAMIN)\n",
    "fixed = ConstantLR(optimizer, factor=ETAMIN/LR, total_iters=5)\n",
    "scheduler = SequentialLR(\n",
    "    optimizer,\n",
    "    schedulers=[cosine, fixed],\n",
    "    milestones=[EPOCHS-5]\n",
    ")\n",
    "# Or (it is equivalent)\n",
    "#scheduler = FixedLRSchedulerWrapper(\n",
    "#    scheduler=CosineAnnealingLR(optimizer, T_max=EPOCHS-5, eta_min=ETAMIN),\n",
    "#    fixed_lr=ETAMIN,\n",
    "#    fixed_epoch=EPOCHS-5)\n",
    "\n",
    "# Instantiate the classification engine with the created model and the target device\n",
    "engine = ObjectDetectionEngine(\n",
    "    model=model,                                # Model to be trained\n",
    "    optimizer=optimizer,                        # Optimizer    \n",
    "    scheduler=scheduler,                        # Scheduler     \n",
    "    log_verbose=True,                           # Verbosity    \n",
    "    theme=THEME,                                # Theme\n",
    "    device=device                               # Target device\n",
    "    )\n",
    "\n",
    "# Configure the training method\n",
    "results = engine.train(\n",
    "    target_dir=MODEL_DIR,                       # Directory where the model will be saved\n",
    "    model_name=model_name,                      # Name of the model\n",
    "    enable_resume=True,                         # Resume training from the last saved checkpoint\n",
    "    save_best_model=[\"last\", \"loss\"],           # Save the best models based on different criteria\n",
    "    keep_best_models_in_memory=True,            # If false: do not keep the models stored in memory for the sake of training time and memory efficiency    \n",
    "    dataloaders=dataloaders,                    # Dictionary with the dataloaders     \n",
    "    apply_validation=True,                      # Enable valid ation step\n",
    "    augmentation_strategy=\"always\",             # Augmentation strategy    \n",
    "    epochs=EPOCHS,                              # Total number of epochs\n",
    "    amp=True,                                   # Enable Automatic Mixed Precision (AMP)\n",
    "    enable_clipping=False,                      # Disable clipping on gradients, only useful if training becomes unestable\n",
    "    debug_mode=False,                           # Disable debug mode    \n",
    "    accumulation_steps=2,                       # Accumulation steps 2: effective batch size = batch_size x accumulation steps\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 11. Making Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OPTION = 1\n",
    "\n",
    "# Make predictions using the `engine` object, best model is already internally stored\n",
    "if OPTION == 1: \n",
    "    # Make predictions and plot the results\n",
    "    preds = engine.predict(\n",
    "        dataloader=test_dataloader,\n",
    "        model_state='loss', # Take the model with the lowest loss\n",
    "        prune_predictions = True,\n",
    "        #score_threshold = 0.66,\n",
    "        #mask_threshold = 0.5,    \n",
    "        #iou_threshold = 0.5\n",
    "    )\n",
    "\n",
    "# Make predictions by loading the already trained model manually\n",
    "else:\n",
    "    # Instantiate the trained model\n",
    "    # First, load the architecture\n",
    "    model = CustomFasterRCNN(\n",
    "        num_classes=NUM_CLASSES,\n",
    "        backbone=models.efficientnet_b0(weights=models.EfficientNet_B0_Weights.DEFAULT),\n",
    "        anchor_generator=AnchorGenerator(\n",
    "            sizes=((32, 64, 128, 256, 512),),\n",
    "            aspect_ratios=((0.5, 1.0, 2.0),)),\n",
    "        roi_pooler=MultiScaleRoIAlign(\n",
    "            featmap_names=['0'],\n",
    "            output_size=7,\n",
    "            sampling_ratio=2),\n",
    "        device=device\n",
    "    )\n",
    "\n",
    "    # Second, load the parameters of the best model    \n",
    "    model_file = glob.glob(os.path.join(MODEL_DIR, f\"{model_type}_loss_epoch*.pth\"))\n",
    "    model = load_model(model, MODEL_DIR, os.path.basename(model_file[0]))\n",
    "\n",
    "    # Instantiate the engine with the created model and the target device\n",
    "    engine2 = ObjectDetectionEngine(\n",
    "        model=model,\n",
    "        device=device)\n",
    "\n",
    "    # Make predictions and plot the results\n",
    "    preds = engine2.predict(\n",
    "        dataloader=test_dataloader,\n",
    "        prune_predictions = True,\n",
    "        #score_threshold = 0.66,\n",
    "        #mask_threshold = 0.5,    \n",
    "        #iou_threshold = 0.5\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration parameters\n",
    "MASK_COLOR = \"blue\"\n",
    "BOX_COLOR = \"white\"\n",
    "WIDTH = 3\n",
    "PRINT_LABELS = True\n",
    "\n",
    "# Display predictions\n",
    "display_and_save_predictions(\n",
    "    preds=preds,\n",
    "    dataloader=test_dataset,\n",
    "    box_color=BOX_COLOR,\n",
    "    mask_color=MASK_COLOR,\n",
    "    width=WIDTH,\n",
    "    print_classes=True,\n",
    "    print_scores=True,\n",
    "    label_to_class_dict={1: 'pedestrian'},\n",
    "    theme=THEME\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load an arbitrary image from a different dataset\n",
    "image = read_image(\"images/examples/000000000674.jpg\")\n",
    "\n",
    "# And make a prediction\n",
    "eval_transform = get_transform(train=False, mean_std_norm=False)\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    x = eval_transform(image)\n",
    "    # convert RGBA -> RGB and move to device\n",
    "    x = x[:3, ...].to(device)\n",
    "    predictions = model([x, ])\n",
    "    pred = prune_predictions(predictions[0])\n",
    "    \n",
    "\n",
    "# Prepare the image for plotting\n",
    "image = (255.0 * (image - image.min()) / (image.max() - image.min())).to(torch.uint8)\n",
    "image = image[:3, ...]\n",
    "pred_labels = [f\"roi: {score:.3f}\" for label, score in zip(pred[\"labels\"], pred[\"scores\"])]\n",
    "pred_boxes = pred[\"boxes\"].long()\n",
    "output_image = draw_bounding_boxes(image, pred_boxes, pred_labels, colors=\"white\", width=3)\n",
    "\n",
    "#masks = (pred[\"masks\"] > 0.7).squeeze(1)\n",
    "#output_image = draw_segmentation_masks(output_image, masks, alpha=0.5, colors=\"blue\")\n",
    "\n",
    "# Plot the image\n",
    "fig, ax = plt.subplots(figsize=(12, 12))\n",
    "plt.imshow(output_image.permute(1, 2, 0))\n",
    "ax.axis(\"off\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv_pytorch_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
