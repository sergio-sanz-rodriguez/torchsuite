{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Introduction\n",
    "This notebook aims to develop and train a U-Net–based deep learning model for organ detection and segmentation in computed tomography (CT) images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. The U-Net Architecture\n",
    "U-Net is one of the most common deep learning architectures for segmentation tasks. The block diagram of this model is depicted in the figure below:\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"images/u-net-architecture.png\" width=\"600\"/>\n",
    "</div>\n",
    "\n",
    "The architecture features a \"U\" shape, consisting of two main stages: the contracting (encoder) and expansive (decoder) stages.\n",
    "\n",
    "* The **encoder** captures the context and high-level features of the input image by using several convolutional layers. It gradually reduces the spatial dimensions while increasing the feature dimensions.\n",
    "\n",
    "* The **decoder** is responsible for reconstructing the output image, which, in the case of segmentation, is the mask that identifies the objects of interest. This stage involves upsampling across the same number of levels as the encoder, followed by convolutional operations to \"expand\" the contracted image.\n",
    "\n",
    "One of the unique characteristics of U-Net is its **skip connections**, which link the encoder and decoder stages at each level by merging features. While the contracting and expanding paths (\"U\" shape) capture high-level contextual information, the skip connections help preserve low-level spatial details that might be lost during downsampling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generic libraries\n",
    "import os\n",
    "import torch\n",
    "import random\n",
    "import glob\n",
    "import torch.backends.cudnn as cudnn\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from torchinfo import summary\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR, CosineAnnealingLR, SequentialLR, ConstantLR\n",
    "\n",
    "# Import custom libraries\n",
    "from utils.obj_detection_utils import collate_fn\n",
    "from utils.segmentation_utils import display_image_with_mask, collapse_one_hot_mask, create_label_class_dict\n",
    "from engines.segmentation import SegmentationEngine\n",
    "from engines.schedulers import FixedLRSchedulerWrapper\n",
    "from engines.loss_functions import DiceCrossEntropyLoss\n",
    "from dataloaders.segmentation_dataloaders import ProcessDatasetSegmentation, SegmentationTransforms\n",
    "from models.unet import create_unet\n",
    "\n",
    "# Import custom libraries\n",
    "from utils.common_utils import set_seeds\n",
    "from utils.coco_dataset_utils import COCO_2_ImgMsk, split_dataset\n",
    "\n",
    "# Warnings\n",
    "import warnings\n",
    "os.environ['TORCH_USE_CUDA_DSA'] = \"1\"\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"torch.autograd.graph\")\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning, module=\"onnxscript.converter\")\n",
    "\n",
    "import torch._dynamo\n",
    "#torch._dynamo.config.suppress_errors = True\n",
    "torch._dynamo.config.cache_size_limit = 64\n",
    "#warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"torch._dynamo\")\n",
    "\n",
    "# Create target model directory\n",
    "MODEL_DIR = Path(\"outputs\")\n",
    "MODEL_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Set seeds\n",
    "SEED = 42\n",
    "set_seeds(SEED)\n",
    "\n",
    "DOWNLOAD_COCO = False\n",
    "PROCESS_COCO = False\n",
    "VISUALIZE_TRANSFORMED_DATA = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Specifying the Target Device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activate cuda benchmark\n",
    "#cudnn.benchmark = True\n",
    "\n",
    "# Set device\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Device: {device}\")\n",
    "\n",
    "if device == \"cuda\":\n",
    "    !nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Importing the Medical Imaging Dataset\n",
    "\n",
    "This section prepares the training and validation datasets that will be used to train the U-Net model.\n",
    "\n",
    "The complete dataset is publicly available in the [Kaggle UW-Madison GI Tract Image Segmentation Challenge](https://www.kaggle.com/competitions/uw-madison-gi-tract-image-segmentation/data). \n",
    "\n",
    "You must register on Kaggle and formally accept the competition terms in order to download the data.\n",
    "\n",
    "After downloading the dataset, follow the instructions in Subsection 4.1. Once executed, the medical imaging dataset will be processed and organized into the following folder structure:\n",
    "\n",
    "```\n",
    "medical_imaging/\n",
    "├── train/\n",
    "│   └── images/\n",
    "│       ├── img1.png\n",
    "│       ├── img2.png\n",
    "│       └── ...\n",
    "│   └── masks/\n",
    "│       ├── msk1.png\n",
    "│       ├── msk2.png\n",
    "│       └── ...\n",
    "├── valid/\n",
    "│   └── images/\n",
    "│       ├── img1.png\n",
    "│       ├── img2.png\n",
    "│       └── ...\n",
    "│   └── masks/\n",
    "│       ├── msk1.png\n",
    "│       ├── msk2.png\n",
    "│       └── ...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1. Processing the Dataset\n",
    "\n",
    "The script called `dataloaders/prepare_data_segmentation.py` takes as input parameters the path of the image database and the train.csv file that includes all the information needed to generate the target masks:\n",
    "* The images are min-max normalized and saved in RGB PNG format in a specified target folder. If the image is grayscale, it is copied into each of the three channels.\n",
    "* The target masks are created from the run-length code provided in ```bash train.csv``` (column ```python segmentation```).\n",
    "* The script also allows for saving the images in a 2.5D format, where the first (R) channel is used to store the actual image 'i', the second (G) channel to store the image 'i + stride', and the third (B) channel to store the image 'i + 2*stride'.\n",
    "* Additionally, the script offers the option to remove non-segmented slices. This allows the segmentation model to be trained using the entire dataset or only a subset containing segmented images.\n",
    "\n",
    "The usage sample of this application is given next:\n",
    "\n",
    "```bash\n",
    "python .\\prepare_data_segmentation.py -dimension 2d -csv data/train.csv -input_dir images/train -output_dir segmentation_data -remove_non_seg 1\n",
    "\n",
    "```\n",
    "More information about the input parameters is provided with the help parameter as follows:\n",
    "\n",
    "```bash\n",
    "python .\\prepare_data_segmentation.py --help\n",
    "\n",
    "usage: prepare_data_segmentation.py [-h] [-dimension {2d,2.5d}] [-stride STRIDE] [-csv CSV] [-input_dir INPUT_DIR]\n",
    "                                    [-output_dir OUTPUT_DIR] [-test_patients TEST_PATIENTS]\n",
    "                                    [-remove_non_seg REMOVE_NON_SEG] [-mask_rgb MASK_RGB]\n",
    "\n",
    "options:\n",
    "-h, --help                     Show this help message and exit\n",
    "-dimension {2d,2.5d}           Choose either '2d' or '2.5d'\n",
    "-stride STRIDE                 Specify the stride as an integer (default 1) for 2.5d\n",
    "-csv CSV                       Path and file name of the csv file with rle data (default 'data/train.csv'\n",
    "-input_dir INPUT_DIR           Specify the directory where the input images reside (default 'images/train')\n",
    "-output_dir OUTPUT_DIR         Specify the directory where the images will be stored (default 'segmentation_data')\n",
    "-test_patients VALID_PATIENTS  Specify the list of validation images for inference (default \"['2', '6', '7', '9', '11', '15', '16', '140', '145', '146', '147', '148', '149', '154', '156']\")\n",
    "-remove_non_seg REMOVE_NON_SEG Remove pictures that are not segmented (default 1)\n",
    "-mask_rgb MASK_RGB             Generate masks also in RGB format (default 0)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Usage Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!python .\\prepare_data_segmentation.py -dimension 2d -csv data/train.csv -input_dir images/train -output_dir medical_imaging -remove_non_seg 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2. Preparing Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the dictionary describing label -> category), say in alphabetical order\n",
    "target_categories={\n",
    "    0: 'background',\n",
    "    1: 'stomach',\n",
    "    2: 'small_bowel',\n",
    "    3: 'large_bowel'\n",
    "}\n",
    "\n",
    "# RGB\n",
    "color_map = {\n",
    "    0: (0, 0, 0),       # Background (Black)\n",
    "    1: (232, 66, 66),   # Bus (Red)\n",
    "    2: (35, 171, 75),   # Car (Green)\n",
    "    3: (28, 163, 218),  # Truck (Blue)   \n",
    "}\n",
    "\n",
    "## Display categories\n",
    "for cat_id, cat_name in target_categories.items():\n",
    "    print(f\"{cat_id}: {cat_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The dataset contains two classes only: background and person\n",
    "NUM_CLASSES = len(target_categories)\n",
    "BATCH_SIZE = 4\n",
    "ACCUM_STEPS = 8\n",
    "IMG_SIZE = (384, 384)\n",
    "AUGMENT_MAGNITUDE = 1 # 1 (low) to 5 (high)\n",
    "THEME = 'light' # or 'dark'. Default is 'light'\n",
    "\n",
    "# Define training, validation, and test data loaders\n",
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "    dataset=ProcessDatasetSegmentation(\n",
    "        root='data/medical_imaging/train',\n",
    "        image_path=\"images\",\n",
    "        mask_path=\"masks\",\n",
    "        transforms=SegmentationTransforms(\n",
    "            train=True,\n",
    "            img_size=IMG_SIZE,\n",
    "            mean_std_norm=True,\n",
    "            augment_magnitude=AUGMENT_MAGNITUDE\n",
    "            ),\n",
    "        class_dictionary=target_categories\n",
    "        ),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    pin_memory=True,\n",
    ")\n",
    "\n",
    "val_dataloader = torch.utils.data.DataLoader(\n",
    "    dataset=ProcessDatasetSegmentation(\n",
    "        root='data/medical_imaging/valid',\n",
    "        image_path=\"images\",\n",
    "        mask_path=\"masks\",\n",
    "        transforms=SegmentationTransforms(\n",
    "            train=False,\n",
    "            img_size=IMG_SIZE,\n",
    "            mean_std_norm=True,\n",
    "            augment_magnitude=AUGMENT_MAGNITUDE\n",
    "            ),\n",
    "        class_dictionary=target_categories\n",
    "        ),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    pin_memory=True,\n",
    ")\n",
    "\n",
    "dataloaders = {\n",
    "    'train':         train_dataloader,    \n",
    "    'test':          val_dataloader\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3. Visualizing Images with Masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if VISUALIZE_TRANSFORMED_DATA:\n",
    "\n",
    "    # Visualize transformations\n",
    "    BATCH_SIZE = 64\n",
    "    # Train dataloader without transformations\n",
    "    dataloader_nt = torch.utils.data.DataLoader(\n",
    "        dataset=ProcessDatasetSegmentation(\n",
    "            root='data/medical_imaging/train',\n",
    "            image_path=\"images\",\n",
    "            mask_path=\"masks\",\n",
    "            transforms=SegmentationTransforms(\n",
    "                train=False,\n",
    "                img_size=IMG_SIZE,\n",
    "                mean_std_norm=False,\n",
    "                augment_magnitude=AUGMENT_MAGNITUDE\n",
    "                ),\n",
    "            class_dictionary=target_categories\n",
    "            ), \n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=False,\n",
    "        pin_memory=True,\n",
    "        collate_fn=collate_fn)\n",
    "    \n",
    "    # Test dataloader with transformations\n",
    "    dataloader_t = torch.utils.data.DataLoader(\n",
    "        dataset=ProcessDatasetSegmentation(\n",
    "            root='data/medical_imaging/train',\n",
    "            image_path=\"images\",\n",
    "            mask_path=\"masks\",\n",
    "            transforms=SegmentationTransforms(\n",
    "                train=True,\n",
    "                img_size=IMG_SIZE,\n",
    "                mean_std_norm=False,\n",
    "                augment_magnitude=AUGMENT_MAGNITUDE\n",
    "                ),\n",
    "            class_dictionary=target_categories\n",
    "            ), \n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=False,\n",
    "        pin_memory=True,\n",
    "        collate_fn=collate_fn)\n",
    "    \n",
    "    random.seed(SEED+1)\n",
    "\n",
    "    # Visualize images and masks with and without transformations    \n",
    "    for idx, ((img_nt, target_nt), (img_t, target_t)) in enumerate(zip(dataloader_nt, dataloader_t)):   \n",
    "\n",
    "        # Pick random images\n",
    "        random_indices = random.sample(range(BATCH_SIZE), min(10, BATCH_SIZE))\n",
    "        for i in random_indices:\n",
    "\n",
    "            # Set up the figure\n",
    "            fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "            \n",
    "            # Pass subplot axes to the function\n",
    "            mask_nt = collapse_one_hot_mask(target_nt[i])\n",
    "            mask_t = collapse_one_hot_mask(target_t[i])\n",
    "\n",
    "            # Create the label-class dictionary for the mask\n",
    "            classes_nt = create_label_class_dict(target_nt[i], target_categories)\n",
    "            classes_t = create_label_class_dict(target_t[i], target_categories)\n",
    "\n",
    "            # Remove background, as it is always there\n",
    "            classes_nt = dict(list(classes_nt.items())[1:])\n",
    "            classes_t = dict(list(classes_t.items())[1:])\n",
    "\n",
    "            # And generate the titles\n",
    "            title_nt = f\"Original: {', '.join(classes_nt.values())}\"\n",
    "            title_t = f\"Transformed: {', '.join(classes_t.values())}\"\n",
    "            \n",
    "            # Display overlaid images\n",
    "            alpha, beta = 1.0, 0.5\n",
    "            display_image_with_mask(img_nt[i], mask_nt, fig=fig, ax=axes[0], alpha=alpha, beta=beta, color_map=color_map, title=title_nt, theme=THEME)\n",
    "            display_image_with_mask(img_t[i], mask_t, fig=fig, ax=axes[1], alpha=alpha, beta=beta, color_map=color_map, title=title_t, theme=THEME)\n",
    "            \n",
    "            plt.show() \n",
    "            \n",
    "        if idx > -1:\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Creating the U-Net Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the model\n",
    "model = create_unet(\n",
    "    model_type=\"pretrained\",\n",
    "    backbone='convnext_large_384_in22ft1k', #'convnext_small_in22ft1k', #\n",
    "    in_channels=3,\n",
    "    num_classes=NUM_CLASSES,\n",
    "    print_available_models=False,\n",
    "    pretrained=True,\n",
    "    encoder_freeze=False\n",
    ")\n",
    "\n",
    "# Unfreeze the base parameters\n",
    "for parameter in model.parameters():\n",
    "    parameter.requires_grad = True\n",
    "\n",
    "#summary(model,\n",
    "#        input_size=(BATCH_SIZE,3, IMG_SIZE[0], IMG_SIZE[1]),\n",
    "#        col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
    "#        col_width=20,\n",
    "#        row_settings=[\"var_names\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "EPOCHS = 35\n",
    "LR = 1e-4\n",
    "ETAMIN = 1e-6\n",
    "model_type=\"model_seg_convnext\"\n",
    "model_name = model_type + \".pth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Create AdamW optimizer\n",
    "optimizer = torch.optim.AdamW(\n",
    "    params=model.parameters(),\n",
    "    lr=LR,\n",
    "    betas=(0.9, 0.999),\n",
    "    weight_decay=1e-4\n",
    "    )\n",
    "\n",
    "# Create loss function\n",
    "#loss_fn = torch.nn.CrossEntropyLoss(label_smoothing=0.1)\n",
    "loss_fn = DiceCrossEntropyLoss(\n",
    "    num_classes=NUM_CLASSES,\n",
    "    label_smoothing=0.1\n",
    "    )\n",
    "\n",
    "# Set scheduler\n",
    "#scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "#    optimizer, \n",
    "#    mode='min',\n",
    "#    factor=0.1,\n",
    "#    patience=3,\n",
    "#)\n",
    "scheduler = CosineAnnealingLR(optimizer, T_max=EPOCHS-5, eta_min=ETAMIN)\n",
    "\n",
    "# And train...\n",
    "\n",
    "# Instantiate the classification engine with the created model and the target device\n",
    "engine = SegmentationEngine(\n",
    "    model=model,                                # Model to be trained\n",
    "    optimizer=optimizer,                        # Optimizer\n",
    "    loss_fn=loss_fn,                            # Loss function\n",
    "    scheduler=scheduler,                        # Scheduler     \n",
    "    theme=THEME,                                # Theme\n",
    "    log_verbose=True,                           # Verbosity\n",
    "    device=device                               # Target device\n",
    "    )\n",
    "\n",
    "# Configure the training method\n",
    "results = engine.train(\n",
    "    target_dir=MODEL_DIR,                       # Directory where the model will be saved\n",
    "    model_name=model_name,                      # Name of the model\n",
    "    enable_resume=True,                         # Resume training from the last saved checkpoint\n",
    "    save_best_model=[\n",
    "        \"last\", \"loss\", \"dice\", \"iou\"],         # Save the best models based on different criteria\n",
    "    keep_best_models_in_memory=False,           # If False: do not keep the models stored in memory for the sake of training time and memory efficiency\n",
    "    dataloaders=dataloaders,                    # Dictionary with the dataloaders     \n",
    "    apply_validation=True,                      # Enable validation step\n",
    "    augmentation_strategy=\"always\",             # Augmentation strategy        \n",
    "    epochs=EPOCHS,                              # Total number of epochs\n",
    "    amp=True,                                   # Enable Automatic Mixed Precision (AMP)\n",
    "    enable_clipping=False,                      # Disable clipping on gradients, only useful if training becomes unestable\n",
    "    debug_mode=False,                           # Disable debug mode    \n",
    "    accumulation_steps=ACCUM_STEPS,             # Accumulation steps: effective batch size = batch_size x accumulation steps\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Making Predictions on Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os.path import exists\n",
    "def rename_model(model_name: str, new_name: str):\n",
    "    old_name = model_name[0]\n",
    "    os.rename(old_name, new_name)\n",
    "    print(f\"Renamed {old_name} to {new_name}\")\n",
    "    \n",
    "# Find the model file with \"model_1_loss_epoch\" prefix and rename it\n",
    "new_model_name = str(MODEL_DIR / f\"{model_name}\")\n",
    "if not exists(new_model_name):\n",
    "    model_name_dice = glob.glob(str(MODEL_DIR / f\"{model_type}_dice_epoch*.pth\"))\n",
    "    rename_model(model_name_dice, new_model_name)\n",
    "\n",
    "# Instantiate the classification engine with the created model and the target device\n",
    "engine = SegmentationEngine(\n",
    "    model=model,    \n",
    "    log_verbose=True,\n",
    "    device=device\n",
    "    ).load(\n",
    "        target_dir=MODEL_DIR,\n",
    "        model_name=Path(new_model_name).name\n",
    "    )\n",
    "\n",
    "preds = engine.predict(\n",
    "    dataloader=val_dataloader,\n",
    "    num_classes=NUM_CLASSES,\n",
    "    model_state=\"last\", # This option takes the default loaded model\n",
    "    output_type=\"onehot\" #one-hot binary masks: (num_batches, batch_size, num_classes, W, H)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Shape of the prediction tensor: {preds.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize images and masks with and without transformations\n",
    "BATCH_SIZE = val_dataloader.batch_size\n",
    "\n",
    "# Create a test dataloader without mean/std transformation and augmentations\n",
    "test_dataloader_nt = torch.utils.data.DataLoader(\n",
    "    dataset=ProcessDatasetSegmentation(\n",
    "        root='data/medical_imaging/valid',\n",
    "        image_path=\"images\",\n",
    "        mask_path=\"masks\",\n",
    "        transforms=SegmentationTransforms(\n",
    "            train=False, # If train if False, augmentation is not applied\n",
    "            img_size=IMG_SIZE,\n",
    "            mean_std_norm=False,\n",
    "            augment_magnitude=AUGMENT_MAGNITUDE\n",
    "            ),\n",
    "        class_dictionary=target_categories\n",
    "        ),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    pin_memory=True,\n",
    ")\n",
    "\n",
    "for batch, (img, target) in enumerate(test_dataloader_nt):\n",
    "\n",
    "    # Pick random images\n",
    "    random_indices = random.sample(range(BATCH_SIZE), min(2, BATCH_SIZE))\n",
    "    for i in random_indices:\n",
    "\n",
    "        # Set up the figure\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "        \n",
    "        # Pass subplot axes to the function\n",
    "        mask_true = collapse_one_hot_mask(target[i])\n",
    "        mask_pred = collapse_one_hot_mask(preds[batch][i])\n",
    "\n",
    "        # Create the label-class dictionary for the mask\n",
    "        classes_true = create_label_class_dict(target[i], target_categories)\n",
    "        classes_pred = create_label_class_dict(preds[batch][i], target_categories)\n",
    "\n",
    "        # classes_true background, as it is always there\n",
    "        classes_true = dict(list(classes_true.items())[1:])\n",
    "        classes_pred = dict(list(classes_pred.items())[1:])\n",
    "\n",
    "        # And generate the titles\n",
    "        title_true = f\"Ground-Truth: {', '.join(classes_true.values())}\"\n",
    "        title_pred = f\"Predictions: {', '.join(classes_pred.values())}\"\n",
    "        \n",
    "        # Display overlaid images\n",
    "        alpha, beta = 1.0, 1.0\n",
    "        display_image_with_mask(img[i], mask_true, fig=fig, ax=axes[0], alpha=alpha, beta=beta, color_map=color_map, title=title_true, theme=THEME)\n",
    "        display_image_with_mask(img[i], mask_pred, fig=fig, ax=axes[1], alpha=alpha, beta=beta, color_map=color_map, title=title_pred, theme=THEME)\n",
    "        \n",
    "        plt.show() \n",
    "        \n",
    "    if batch > 5:\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv_pytorch_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
