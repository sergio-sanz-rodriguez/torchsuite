{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Introduction\n",
    "This notebook outlines the steps to convert the COCO dataset for pedestrian detection and segmentation into a structured format. The process involves:\n",
    "\n",
    "* Filtering the dataset to retain only the \"person\" category.\n",
    "* Storing original images and corresponding segmentation masks in separate folders.\n",
    "* Splitting the dataset into three subsets: training, validation, and testing.\n",
    "\n",
    "By applying this conversion, the COCO dataset can be used alongside the PennFudanPed dataset for pedestrian detection and segmentation tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. The U-Net Architecture\n",
    "U-Net is one of the most common deep learning architectures for segmentation tasks. The block diagram of this model is depicted in the figure below (https://arxiv.org/abs/1505.04597):\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"images/u-net-architecture.png\" width=\"600\"/>\n",
    "</div>\n",
    "\n",
    "The architecture features a \"U\" shape, consisting of two main stages: the contracting (encoder) and expansive (decoder) stages.\n",
    "\n",
    "* The **encoder** captures the context and high-level features of the input image by using several convolutional layers. It gradually reduces the spatial dimensions while increasing the feature dimensions.\n",
    "\n",
    "* The **decoder** is responsible for reconstructing the output image, which, in the case of segmentation, is the mask that identifies the objects of interest. This stage involves upsampling across the same number of levels as the encoder, followed by convolutional operations to \"expand\" the contracted image.\n",
    "\n",
    "One of the unique characteristics of U-Net is its **skip connections**, which link the encoder and decoder stages at each level by merging features. While the contracting and expanding paths (\"U\" shape) capture high-level contextual information, the skip connections help preserve low-level spatial details that might be lost during downsampling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generic libraries\n",
    "import os\n",
    "import torch\n",
    "import json\n",
    "import zipfile\n",
    "import urllib\n",
    "import random\n",
    "import torch.backends.cudnn as cudnn\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from torchinfo import summary\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "\n",
    "# Import custom libraries\n",
    "from utils.obj_detection_utils import collate_fn\n",
    "from utils.segmentation_utils import display_image_with_mask, collapse_one_hot_mask, create_label_class_dict\n",
    "from engines.segmentation import SegmentationEngine\n",
    "from engines.schedulers import FixedLRSchedulerWrapper\n",
    "from engines.loss_functions import DiceCrossEntropyLoss\n",
    "from dataloaders.segmentation_dataloaders import ProcessDatasetSegmentation, SegmentationTransforms\n",
    "from models.unet import create_unet\n",
    "\n",
    "# Import custom libraries\n",
    "from utils.classification_utils import set_seeds\n",
    "from utils.coco_dataset_utils import COCO_2_ImgMsk, split_dataset\n",
    "\n",
    "# Warnings\n",
    "import warnings\n",
    "os.environ['TORCH_USE_CUDA_DSA'] = \"1\"\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"torch.autograd.graph\")\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning, module=\"onnxscript.converter\")\n",
    "\n",
    "import torch._dynamo\n",
    "#torch._dynamo.config.suppress_errors = True\n",
    "torch._dynamo.config.cache_size_limit = 64\n",
    "#warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"torch._dynamo\")\n",
    "\n",
    "# Create target model directory\n",
    "MODEL_DIR = Path(\"outputs\")\n",
    "MODEL_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Set seeds\n",
    "SEED = 42\n",
    "set_seeds(SEED)\n",
    "\n",
    "DOWNLOAD_COCO = False\n",
    "PROCESS_COCO = False\n",
    "VISUALIZE_TRANSFORMED_DATA = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Specifying the Target Device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activate cuda benchmark\n",
    "#cudnn.benchmark = True\n",
    "\n",
    "# Set device\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Device: {device}\")\n",
    "\n",
    "if device == \"cuda\":\n",
    "    !nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Downloading the COCO Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DOWNLOAD_COCO:\n",
    "    # Define download URLs\n",
    "    coco_urls = {\n",
    "        \"val_images\": \"http://images.cocodataset.org/zips/val2017.zip\",\n",
    "        \"test_images\": \"http://images.cocodataset.org/zips/test2017.zip\",\n",
    "        \"train_images\": \"http://images.cocodataset.org/zips/train2017.zip\",\n",
    "        \"annotations\": \"http://images.cocodataset.org/annotations/annotations_trainval2017.zip\"\n",
    "    }\n",
    "\n",
    "    # Create a directory to store the dataset\n",
    "    dataset_dir = \"d:/Repos/coco_dataset\"\n",
    "    os.makedirs(dataset_dir, exist_ok=True)\n",
    "\n",
    "    # Download function\n",
    "    def download_coco(url, filename):\n",
    "        filepath = os.path.join(dataset_dir, filename)\n",
    "        if not os.path.exists(filepath):\n",
    "            print(f\"Downloading {filename}...\")\n",
    "            urllib.request.urlretrieve(url, filepath)\n",
    "            print(f\"Saved to {filepath}\")\n",
    "        else:\n",
    "            print(f\"{filename} already exists.\")\n",
    "\n",
    "    # Download all files\n",
    "    for key, url in coco_urls.items():\n",
    "        filename = url.split(\"/\")[-1]\n",
    "        download_coco(url, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DOWNLOAD_COCO:\n",
    "    # Unzip the file\n",
    "    PATH = Path(dataset_dir)\n",
    "    \n",
    "    zip_file = PATH / \"val2017.zip\"\n",
    "    with zipfile.ZipFile(zip_file, 'r') as zip_ref:\n",
    "        zip_ref.extractall(dataset_dir)\n",
    "\n",
    "    if zip_file.exists():\n",
    "        os.remove(zip_file)\n",
    "\n",
    "    zip_file = PATH / \"annotations_trainval2017.zip\"\n",
    "    with zipfile.ZipFile(zip_file, 'r') as zip_ref:\n",
    "        zip_ref.extractall(dataset_dir)\n",
    "\n",
    "    if zip_file.exists():\n",
    "        os.remove(zip_file)\n",
    "\n",
    "    zip_file = PATH / \"test2017.zip\"\n",
    "    with zipfile.ZipFile(zip_file, 'r') as zip_ref:\n",
    "        zip_ref.extractall(dataset_dir)\n",
    "\n",
    "    if zip_file.exists():\n",
    "        os.remove(zip_file)\n",
    "\n",
    "    zip_file = PATH / \"train2017.zip\"\n",
    "    with zipfile.ZipFile(zip_file, 'r') as zip_ref:\n",
    "        zip_ref.extractall(dataset_dir)\n",
    "\n",
    "    if zip_file.exists():\n",
    "        os.remove(zip_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Processing the COCO Dataset for Driving Segmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "25 GB disk is required to download the complete dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if PROCESS_COCO:\n",
    "    # Path to COCO annotations file\n",
    "    ANNOTATIONS_PATH = r\"D:\\Repos\\coco_dataset\\annotations\\instances_train2017.json\"\n",
    "\n",
    "    # Load COCO annotations\n",
    "    with open(ANNOTATIONS_PATH, \"r\") as f:\n",
    "        coco_data = json.load(f)\n",
    "\n",
    "    # Extract category ID to name mapping\n",
    "    categories = {c[\"id\"]: c[\"name\"] for c in coco_data[\"categories\"]}\n",
    "\n",
    "    # Display all categories\n",
    "    for cat_id, cat_name in categories.items():\n",
    "        print(f\"{cat_id}: {cat_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the dictionary describing label -> category), say in alphabetical order\n",
    "target_categories={\n",
    "    0: 'background',\n",
    "    1: 'bus',\n",
    "    2: 'car',\n",
    "    3: 'truck'\n",
    "}\n",
    "\n",
    "# RGB\n",
    "color_map = {\n",
    "    0: (0, 0, 0),       # Background (Black)\n",
    "    1: (232, 66, 66),   # Bus (Red)\n",
    "    2: (35, 171, 75),   # Car (Green)\n",
    "    3: (28, 163, 218),  # Truck (Blue)   \n",
    "}\n",
    "\n",
    "## Display categories\n",
    "for cat_id, cat_name in target_categories.items():\n",
    "    print(f\"{cat_id}: {cat_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if PROCESS_COCO:\n",
    "    # Training dataset\n",
    "    mapping = COCO_2_ImgMsk(\n",
    "        coco_images_path=      r\"D:\\Repos\\coco_dataset\\train2017\",\n",
    "        coco_annotations_path= r\"D:\\Repos\\coco_dataset\\annotations\\instances_train2017.json\",\n",
    "        output_images_dir=     r\"D:\\Repos\\coco_dataset\\driving\\PNGImages\",\n",
    "        output_masks_dir=      r\"D:\\Repos\\coco_dataset\\driving\\PedMasks\",\n",
    "        class_dictionary=      target_categories,\n",
    "        label=                 \"train\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if PROCESS_COCO:\n",
    "    # Training dataset\n",
    "    _ = COCO_2_ImgMsk(\n",
    "        coco_images_path=      r\"D:\\Repos\\coco_dataset\\val2017\",\n",
    "        coco_annotations_path= r\"D:\\Repos\\coco_dataset\\annotations\\instances_val2017.json\",\n",
    "        output_images_dir=     r\"D:\\Repos\\coco_dataset\\driving\\PNGImages\",\n",
    "        output_masks_dir=      r\"D:\\Repos\\coco_dataset\\driving\\PedMasks\",\n",
    "        class_dictionary=      target_categories,\n",
    "        label=                 \"val\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if PROCESS_COCO:\n",
    "# Split dataset into train (80%), validation (10%), and test (10%) sets\n",
    "    split_dataset(\n",
    "        src_images=       r\"D:\\Repos\\coco_dataset\\driving\\PNGImages\",\n",
    "        src_masks=        r\"D:\\Repos\\coco_dataset\\driving\\PedMasks\",\n",
    "        dst_train_images= r\"C:\\Users\\ssre_\\Projects\\torchsuite\\data\\driving\\train\\PNGImages\",\n",
    "        dst_train_masks=  r\"C:\\Users\\ssre_\\Projects\\torchsuite\\data\\driving\\train\\PedMasks\",\n",
    "        dst_val_images=   r\"C:\\Users\\ssre_\\Projects\\torchsuite\\data\\driving\\val\\PNGImages\",\n",
    "        dst_val_masks=    r\"C:\\Users\\ssre_\\Projects\\torchsuite\\data\\driving\\val\\PedMasks\",\n",
    "        dst_test_images=  r\"C:\\Users\\ssre_\\Projects\\torchsuite\\data\\driving\\test\\PNGImages\",\n",
    "        dst_test_masks=   r\"C:\\Users\\ssre_\\Projects\\torchsuite\\data\\driving\\test\\PedMasks\",\n",
    "        train_pct=        0.80,\n",
    "        val_pct=          0.10,\n",
    "        test_pct=         0.10,\n",
    "        seed=             SEED\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Preparing Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The dataset contains two classes only: background and person\n",
    "NUM_CLASSES = len(target_categories)\n",
    "BATCH_SIZE = 4\n",
    "ACCUM_STEPS = 8\n",
    "IMG_SIZE = (384, 384) #(512, 512)\n",
    "AUGMENT_MAGNITUDE = 4 # 1 (low) to 5 (high)\n",
    "\n",
    "# Define training, validation, and test data loaders\n",
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "    dataset=ProcessDatasetSegmentation(\n",
    "        root='data/driving/train',\n",
    "        image_path=\"PNGImages\",\n",
    "        mask_path=\"PedMasks\",\n",
    "        transforms=SegmentationTransforms(\n",
    "            train=True,\n",
    "            img_size=IMG_SIZE,\n",
    "            mean_std_norm=True,\n",
    "            augment_magnitude=AUGMENT_MAGNITUDE\n",
    "            ),\n",
    "        class_dictionary=target_categories\n",
    "        ),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    pin_memory=True,\n",
    ")\n",
    "\n",
    "val_dataloader = torch.utils.data.DataLoader(\n",
    "    dataset=ProcessDatasetSegmentation(\n",
    "        root='data/driving/val',\n",
    "        image_path=\"PNGImages\",\n",
    "        mask_path=\"PedMasks\",\n",
    "        transforms=SegmentationTransforms(\n",
    "            train=False,\n",
    "            img_size=IMG_SIZE,\n",
    "            mean_std_norm=True,\n",
    "            augment_magnitude=AUGMENT_MAGNITUDE\n",
    "            ),\n",
    "        class_dictionary=target_categories\n",
    "        ),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    pin_memory=True,\n",
    ")\n",
    "\n",
    "test_dataloader = torch.utils.data.DataLoader(\n",
    "    dataset=ProcessDatasetSegmentation(\n",
    "        root='data/driving/test',\n",
    "        image_path=\"PNGImages\",\n",
    "        mask_path=\"PedMasks\",\n",
    "        transforms=SegmentationTransforms(\n",
    "            train=False,\n",
    "            img_size=IMG_SIZE,\n",
    "            mean_std_norm=True,\n",
    "            augment_magnitude=AUGMENT_MAGNITUDE\n",
    "            ),\n",
    "        class_dictionary=target_categories\n",
    "        ),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    pin_memory=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Visualize Images with Masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if VISUALIZE_TRANSFORMED_DATA:\n",
    "\n",
    "    # Visualize transformations\n",
    "    BATCH_SIZE = 64\n",
    "    # Train dataloader without transformations\n",
    "    dataloader_nt = torch.utils.data.DataLoader(\n",
    "        dataset=ProcessDatasetSegmentation(\n",
    "            root='data/driving/train',\n",
    "            image_path=\"PNGImages\",\n",
    "            mask_path=\"PedMasks\",\n",
    "            transforms=SegmentationTransforms(\n",
    "                train=False,\n",
    "                img_size=IMG_SIZE,\n",
    "                mean_std_norm=False,\n",
    "                augment_magnitude=AUGMENT_MAGNITUDE\n",
    "                ),\n",
    "            class_dictionary=target_categories\n",
    "            ), \n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=False,\n",
    "        pin_memory=True,\n",
    "        collate_fn=collate_fn)\n",
    "    \n",
    "    # Test dataloader with transformations\n",
    "    dataloader_t = torch.utils.data.DataLoader(\n",
    "        dataset=ProcessDatasetSegmentation(\n",
    "            root='data/driving/train',\n",
    "            image_path=\"PNGImages\",\n",
    "            mask_path=\"PedMasks\",\n",
    "            transforms=SegmentationTransforms(\n",
    "                train=True,\n",
    "                img_size=IMG_SIZE,\n",
    "                mean_std_norm=False,\n",
    "                augment_magnitude=AUGMENT_MAGNITUDE\n",
    "                ),\n",
    "            class_dictionary=target_categories\n",
    "            ), \n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=False,\n",
    "        pin_memory=True,\n",
    "        collate_fn=collate_fn)\n",
    "    \n",
    "    random.seed(SEED+1)\n",
    "\n",
    "    # Visualize images and masks with and without transformations\n",
    "    for idx, ((img_nt, target_nt), (img_t, target_t)) in enumerate(zip(dataloader_nt, dataloader_t)):   \n",
    "\n",
    "        # Pick random images\n",
    "        random_indices = random.sample(range(BATCH_SIZE), min(10, BATCH_SIZE))\n",
    "        for i in random_indices:\n",
    "\n",
    "            # Set up the figure\n",
    "            fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "            \n",
    "            # Pass subplot axes to the function\n",
    "            mask_nt = collapse_one_hot_mask(target_nt[i])\n",
    "            mask_t = collapse_one_hot_mask(target_t[i])\n",
    "\n",
    "            # Create the label-class dictionary for the mask\n",
    "            classes_nt = create_label_class_dict(target_nt[i], target_categories)\n",
    "            classes_t = create_label_class_dict(target_t[i], target_categories)\n",
    "\n",
    "            # Remove background, as it is always there\n",
    "            classes_nt = dict(list(classes_nt.items())[1:])\n",
    "            classes_t = dict(list(classes_t.items())[1:])\n",
    "\n",
    "            # And generate the titles\n",
    "            title_nt = f\"Original: {', '.join(classes_nt.values())}\"\n",
    "            title_t = f\"Transformed: {', '.join(classes_t.values())}\"\n",
    "            \n",
    "            # Display overlaid images\n",
    "            alpha, beta = 1.0, 0.5\n",
    "            display_image_with_mask(img_nt[i], mask_nt, ax=axes[0], alpha=alpha, beta=beta, color_map=color_map, title=title_nt)\n",
    "            display_image_with_mask(img_t[i], mask_t, ax=axes[1], alpha=alpha, beta=beta, color_map=color_map, title=title_t)\n",
    "            \n",
    "            plt.show() \n",
    "            \n",
    "        if idx > 4:\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Creating the U-Net Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the model\n",
    "model = create_unet(\n",
    "    model_type=\"pretrained\",\n",
    "    backbone='convnext_large_384_in22ft1k', #'convnext_base',\n",
    "    in_channels=3,\n",
    "    num_classes=NUM_CLASSES,\n",
    "    print_available_models=False,\n",
    "    pretrained=True,\n",
    "    encoder_freeze=False\n",
    ")\n",
    "\n",
    "# Unfreeze the base parameters\n",
    "for parameter in model.parameters():\n",
    "    parameter.requires_grad = True\n",
    "\n",
    "# Compile model\n",
    "model = torch.compile(model, backend=\"aot_eager\")\n",
    "\n",
    "#summary(model,\n",
    "#        input_size=(BATCH_SIZE,3, IMG_SIZE[0], IMG_SIZE[1]),\n",
    "#        col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
    "#        col_width=20,\n",
    "#        row_settings=[\"var_names\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "EPOCHS = 30\n",
    "LR = 1e-4\n",
    "model_type=\"model_seg2\"\n",
    "model_name = model_type + \".pth\"\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Create AdamW optimizer\n",
    "optimizer = torch.optim.AdamW(\n",
    "    params=model.parameters(),\n",
    "    lr=LR,\n",
    "    betas=(0.9, 0.999),\n",
    "    weight_decay=1e-4\n",
    "    )\n",
    "\n",
    "# Create loss function\n",
    "#loss_fn = torch.nn.CrossEntropyLoss(label_smoothing=0.1)\n",
    "loss_fn = DiceCrossEntropyLoss(\n",
    "    num_classes=NUM_CLASSES,\n",
    "    label_smoothing=0.1\n",
    "    )\n",
    "\n",
    "# Set scheduler\n",
    "scheduler = FixedLRSchedulerWrapper(\n",
    "    scheduler=CosineAnnealingLR(optimizer, T_max=EPOCHS, eta_min=1e-6),\n",
    "    fixed_lr=1e-6,\n",
    "    fixed_epoch=EPOCHS\n",
    "    )\n",
    "\n",
    "# Set seeds\n",
    "set_seeds(SEED)\n",
    "\n",
    "# And train...\n",
    "\n",
    "# Instantiate the classification engine with the created model and the target device\n",
    "engine = SegmentationEngine(\n",
    "    model=model,    \n",
    "    log_verbose=True,\n",
    "    device=device\n",
    "    )\n",
    "\n",
    "# Configure the training method\n",
    "results = engine.train(\n",
    "    target_dir=MODEL_DIR,                       # Directory where the model will be saved\n",
    "    model_name=model_name,                      # Name of the model\n",
    "    save_best_model=[\"last\", \"dice\"],           # Save the best models based on different criteria\n",
    "    keep_best_models_in_memory=False,           # Do not keep the models stored in memory for the sake of training time and memory efficiency\n",
    "    train_dataloader=train_dataloader,          # Train dataloader\n",
    "    test_dataloader=val_dataloader,             # Validation/test dataloader\n",
    "    num_classes=NUM_CLASSES,                    # Number of classes\n",
    "    optimizer=optimizer,                        # Optimizer\n",
    "    loss_fn=loss_fn,                            # Loss function    \n",
    "    scheduler=scheduler,                        # Scheduler\n",
    "    epochs=EPOCHS,                              # Total number of epochs\n",
    "    accumulation_steps=ACCUM_STEPS              # Accumulation steps: effective batch size = batch_size x accumulation steps\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv_pytorch_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
