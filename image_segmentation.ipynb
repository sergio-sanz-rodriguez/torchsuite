{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Introduction\n",
    "This notebook outlines the steps to convert the COCO dataset for pedestrian detection and segmentation into a structured format. The process involves:\n",
    "\n",
    "* Filtering the dataset to retain only the \"person\" category.\n",
    "* Storing original images and corresponding segmentation masks in separate folders.\n",
    "* Splitting the dataset into three subsets: training, validation, and testing.\n",
    "\n",
    "By applying this conversion, the COCO dataset can be used alongside the PennFudanPed dataset for pedestrian detection and segmentation tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generic libraries\n",
    "import os\n",
    "import json\n",
    "import urllib.request\n",
    "import zipfile\n",
    "from pathlib import Path\n",
    "\n",
    "# Torchvision libraries\n",
    "from torchvision import datasets\n",
    "\n",
    "# Import custom libraries\n",
    "from utils.classification_utils import set_seeds\n",
    "from utils.coco_dataset_utils import COCO_2_ImgMsk, select_and_copy_samples, split_dataset\n",
    "\n",
    "# Warnings\n",
    "import warnings\n",
    "os.environ['TORCH_USE_CUDA_DSA'] = \"1\"\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"torch.autograd.graph\")\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning, module=\"onnxscript.converter\")\n",
    "\n",
    "# Create target model directory\n",
    "MODEL_DIR = Path(\"outputs\")\n",
    "MODEL_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Set seeds\n",
    "SEED = 42\n",
    "set_seeds(SEED)\n",
    "\n",
    "DOWNLOAD_COCO = False\n",
    "PROCESS_COCO = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Downloading the COCO Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DOWNLOAD_COCO:\n",
    "    # Define download URLs\n",
    "    coco_urls = {\n",
    "        \"val_images\": \"http://images.cocodataset.org/zips/val2017.zip\",\n",
    "        \"test_images\": \"http://images.cocodataset.org/zips/test2017.zip\",\n",
    "        \"train_images\": \"http://images.cocodataset.org/zips/train2017.zip\",\n",
    "        \"annotations\": \"http://images.cocodataset.org/annotations/annotations_trainval2017.zip\"\n",
    "    }\n",
    "\n",
    "    # Create a directory to store the dataset\n",
    "    dataset_dir = \"d:/Repos/coco_dataset\"\n",
    "    os.makedirs(dataset_dir, exist_ok=True)\n",
    "\n",
    "    # Download function\n",
    "    def download_coco(url, filename):\n",
    "        filepath = os.path.join(dataset_dir, filename)\n",
    "        if not os.path.exists(filepath):\n",
    "            print(f\"Downloading {filename}...\")\n",
    "            urllib.request.urlretrieve(url, filepath)\n",
    "            print(f\"Saved to {filepath}\")\n",
    "        else:\n",
    "            print(f\"{filename} already exists.\")\n",
    "\n",
    "    # Download all files\n",
    "    for key, url in coco_urls.items():\n",
    "        filename = url.split(\"/\")[-1]\n",
    "        download_coco(url, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DOWNLOAD_COCO:\n",
    "    # Unzip the file\n",
    "    PATH = Path(dataset_dir)\n",
    "    \n",
    "    zip_file = PATH / \"val2017.zip\"\n",
    "    with zipfile.ZipFile(zip_file, 'r') as zip_ref:\n",
    "        zip_ref.extractall(dataset_dir)\n",
    "\n",
    "    if zip_file.exists():\n",
    "        os.remove(zip_file)\n",
    "\n",
    "    zip_file = PATH / \"annotations_trainval2017.zip\"\n",
    "    with zipfile.ZipFile(zip_file, 'r') as zip_ref:\n",
    "        zip_ref.extractall(dataset_dir)\n",
    "\n",
    "    if zip_file.exists():\n",
    "        os.remove(zip_file)\n",
    "\n",
    "    zip_file = PATH / \"test2017.zip\"\n",
    "    with zipfile.ZipFile(zip_file, 'r') as zip_ref:\n",
    "        zip_ref.extractall(dataset_dir)\n",
    "\n",
    "    if zip_file.exists():\n",
    "        os.remove(zip_file)\n",
    "\n",
    "    zip_file = PATH / \"train2017.zip\"\n",
    "    with zipfile.ZipFile(zip_file, 'r') as zip_ref:\n",
    "        zip_ref.extractall(dataset_dir)\n",
    "\n",
    "    if zip_file.exists():\n",
    "        os.remove(zip_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Processing the COCO Dataset for Driving Segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if PROCESS_COCO:\n",
    "    # Path to COCO annotations file\n",
    "    ANNOTATIONS_PATH = r\"D:\\Repos\\coco_dataset\\annotations\\instances_train2017.json\"\n",
    "\n",
    "    # Load COCO annotations\n",
    "    with open(ANNOTATIONS_PATH, \"r\") as f:\n",
    "        coco_data = json.load(f)\n",
    "\n",
    "    # Extract category ID to name mapping\n",
    "    categories = {c[\"id\"]: c[\"name\"] for c in coco_data[\"categories\"]}\n",
    "\n",
    "    # Display all categories\n",
    "    for cat_id, cat_name in categories.items():\n",
    "        print(f\"{cat_id}: {cat_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_categories=['car', 'traffic_light', 'stop_sign', 'motorcycle', 'bicycle', 'bus', 'truck']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if PROCESS_COCO:\n",
    "    # Training dataset\n",
    "    COCO_2_ImgMsk(\n",
    "        coco_images_path=      r\"D:\\Repos\\coco_dataset\\train2017\",\n",
    "        coco_annotations_path= r\"D:\\Repos\\coco_dataset\\annotations\\instances_train2017.json\",\n",
    "        output_images_dir=     r\"D:\\Repos\\coco_dataset\\driving\\PNGImages\",\n",
    "        output_masks_dir=      r\"D:\\Repos\\coco_dataset\\driving\\PedMasks\",\n",
    "        selected_categories=   target_categories,\n",
    "        reset_category_ids=    True,\n",
    "        label=                 \"train\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if PROCESS_COCO:\n",
    "    # Training dataset\n",
    "    COCO_2_ImgMsk(\n",
    "        coco_images_path=      r\"D:\\Repos\\coco_dataset\\val2017\",\n",
    "        coco_annotations_path= r\"D:\\Repos\\coco_dataset\\annotations\\instances_val2017.json\",\n",
    "        output_images_dir=     r\"D:\\Repos\\coco_dataset\\driving\\PNGImages\",\n",
    "        output_masks_dir=      r\"D:\\Repos\\coco_dataset\\driving\\PedMasks\",\n",
    "        selected_categories=   target_categories,\n",
    "        reset_category_ids=    True,\n",
    "        label=                 \"val\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if PROCESS_COCO:\n",
    "    # Split dataset into train (80%), validation (10%), and test (10%) sets\n",
    "    split_dataset(\n",
    "        src_images=       r\"D:\\Repos\\coco_dataset\\driving\\PNGImages\",\n",
    "        src_masks=        r\"D:\\Repos\\coco_dataset\\driving\\PedMasks\",\n",
    "        dst_train_images= r\"C:\\Users\\ssre_\\Projects\\torchsuite\\data\\driving\\train\\PNGImages\",\n",
    "        dst_train_masks=  r\"C:\\Users\\ssre_\\Projects\\torchsuite\\data\\driving\\train\\PedMasks\",\n",
    "        dst_val_images=   r\"C:\\Users\\ssre_\\Projects\\torchsuite\\data\\driving\\val\\PNGImages\",\n",
    "        dst_val_masks=    r\"C:\\Users\\ssre_\\Projects\\torchsuite\\data\\driving\\val\\PedMasks\",\n",
    "        dst_test_images=  r\"C:\\Users\\ssre_\\Projects\\torchsuite\\data\\driving\\test\\PNGImages\",\n",
    "        dst_test_masks=   r\"C:\\Users\\ssre_\\Projects\\torchsuite\\data\\driving\\test\\PedMasks\",\n",
    "        train_pct=        0.80,\n",
    "        val_pct=          0.10,\n",
    "        test_pct=         0.10,\n",
    "        seed=             SEED\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What Information Do x1 and x2 Contain?\n",
    "x1: The UpSampled Feature Map\n",
    "\n",
    "* x1 is the output from the previous layer after upsampling using ConvTranspose2d.\n",
    "* It comes from a deeper layer of the network (lower resolution, more abstract features).\n",
    "* It contains high-level, semantic information about objects and structures in the image.\n",
    "* Since it's upsampled, it lacks fine-grained spatial details (edges, textures).\n",
    "* x2: The Skip Connection from the Encoder\n",
    "\n",
    "* x2 is taken from an earlier encoder layer before downsampling.\n",
    "* It has a higher spatial resolution (more fine details).\n",
    "* It contains low-level features such as edges, textures, and shapes.\n",
    "* Since it comes from an early stage, it lacks deep semantic meaning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv_pytorch_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
