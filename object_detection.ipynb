{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook outlines the creation, compilation, and training of a deep learing network for object segmentation and detection. The notebook is fundamentally based on the PyTorch tutorial for object segmentation that is available [here](https://pytorch.org/tutorials/intermediate/torchvision_tutorial.html).\n",
    "\n",
    "The training and inference framework created for object segmentation has been validated with the pre-trained CNN models described in this [link](https://pytorch.org/vision/master/models/faster_rcnn.html). The support for a custom backbone will be available in a future verstionof TorchSuite."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Object Detection and Segmentation\n",
    "\n",
    "Object detection is a fundamental task in computer vision that involves identifying and localizing objects within an image. Unlike image classification, which assigns a single label to an entire image, object detection provides:\n",
    "\n",
    "* Bounding boxes around detected objects.\n",
    "* Class labels to categorize each detected object.\n",
    "\n",
    "Object segmentation goes a step further by providing pixel-wise masks for objects instead of just bounding boxes. It is categorized into:\n",
    "\n",
    "* Semantic Segmentation (labels each pixel but does not distinguish between instances).\n",
    "* Instance Segmentation (detects and segments each object separately, such as Mask R-CNN).\n",
    "\n",
    "The challegens in object detection are listed below:\n",
    "\n",
    "* Scale variability: Objects can appear at different sizes, requiring models to detect both large and small objects.\n",
    "* Occlusion:  Objects may be partially hidden behind others.\n",
    "* Class imbalance: Some object classes may appear more frequently than others.\n",
    "* Background clutter: Distracting elements in images can mislead detection models.\n",
    "* Real-time processing: Faster models are needed for applications like autonomous driving and surveillance.\n",
    "\n",
    "# 3. The R-CNN Model: Two-Stage Object Detection\n",
    "\n",
    "A Region-Based Convolutional Neural Network (R-CNN) is a two-stage object detection model that first generates region proposals and then classifies and refines them. The Region Proposal Network (RPN) scans the image and suggests candidate object regions, predicting bounding boxes and objectness scores. These proposals are then passed to the Region of Interest (RoI) Head, which classifies objects, refines box coordinates, and (in the case of Mask R-CNN) predicts segmentation masks. This two-stage approach delivers high accuracy but is computationally intensive compared to single-stage detectors like YOLO and SSD.\n",
    "\n",
    "**Stage 1: Region Proposal Network (RPN)**\n",
    "\n",
    "The Region Proposal Network (RPN) is responsible for generating object proposals, that is, regions of the image that are likely to contain objects. It works as follows:\n",
    "\n",
    "1. Sliding window over feature Map: The RPN slides over the feature map produced by the backbone CNN (e.g., ResNet, VGG).\n",
    "2. Anchor boxes: For each sliding window position, the RPN predicts multiple anchor boxes (default bounding boxes at various sizes and aspect ratios).\n",
    "3. Objectness score: the network outputs a score for each anchor, determining whether it contains an object (foreground) or background.\n",
    "4. Bounding box refinement: The RPN predicts adjustments to the anchor box coordinates to refine object localization.\n",
    "\n",
    "**Stage 2: Region of Interest (RoI)**\n",
    "\n",
    "The RoI Head takes the region proposals from the RPN and performs classification, bounding box refinement, and (for Mask R-CNN) segmentation mask prediction.\n",
    "\n",
    "1. RoI pooling (or RoIAlign in Mask R-CNN): The variable-sized proposed regions from RPN are resized into a fixed size to be fed into a fully connected layer.\n",
    "2. Classification & bounding box refinement: The RoI head classifies the detected object into one of the predefined categories. It further refines the bounding box predictions.\n",
    "3. (For Mask R-CNN) Segmentation Mask Prediction: A separate mask branch is used to predict per-pixel masks for each detected object."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Links of Interest\n",
    "* https://medium.com/@soumyajitdatta123/faster-rcnns-explained-af76f96a0b70\n",
    "* https://medium.com/@RobuRishabh/understanding-and-implementing-faster-r-cnn-248f7b25ff96"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generic libraries\n",
    "import os\n",
    "import torch\n",
    "import zipfile\n",
    "import torch.backends.cudnn as cudnn\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from torchinfo import summary\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "\n",
    "# Torchvision libraries\n",
    "from torchvision import tv_tensors\n",
    "from torchvision.io import read_image\n",
    "from torchvision.transforms import v2 as T\n",
    "from torchvision.utils import draw_bounding_boxes, draw_segmentation_masks\n",
    "\n",
    "# Import custom libraries\n",
    "from utils.classification_utils import set_seeds, load_model\n",
    "from utils.obj_detection_utils import collate_fn, prune_predictions, diplay_predictions, visualize_transformed_data\n",
    "from engines.obj_detection import ObjectDetectionEngine\n",
    "from engines.schedulers import FixedLRSchedulerWrapper\n",
    "from dataloaders.obj_dect_dataloaders import ProcessDataset\n",
    "from models.faster_rcnn import FasterRCNN\n",
    "\n",
    "# Warnings\n",
    "import warnings\n",
    "os.environ['TORCH_USE_CUDA_DSA'] = \"1\"\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"torch.autograd.graph\")\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning, module=\"onnxscript.converter\")\n",
    "\n",
    "# Create target model directory\n",
    "MODEL_DIR = Path(\"outputs\")\n",
    "MODEL_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Set seeds\n",
    "set_seeds(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Specifying the Target Device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activate cuda benchmark\n",
    "#cudnn.benchmark = True\n",
    "\n",
    "# Set device\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Device: {device}\")\n",
    "\n",
    "if device == \"cuda\":\n",
    "    !nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Downloading the Penn-Fundan Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the dataset\n",
    "!curl -L -k https://www.cis.upenn.edu/~jshi/ped_html/PennFudanPed.zip -o data/PennFudanPed.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the downloaded zip file\n",
    "zip_file_path = \"data/PennFudanPed.zip\"\n",
    "extract_dir = \"data\"\n",
    "\n",
    "# Ensure the extraction directory exists\n",
    "os.makedirs(extract_dir, exist_ok=True)\n",
    "\n",
    "# Unzip the file\n",
    "with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
    "    zip_ref.extractall(extract_dir)\n",
    "\n",
    "print(f\"Files extracted to {extract_dir}\")\n",
    "\n",
    "zip_file = Path(zip_file_path)\n",
    "if zip_file.exists():\n",
    "    os.remove(zip_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Image and Mask Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "image = read_image(\"data/PennFudanPed/PNGImages/FudanPed00046.png\")\n",
    "mask = read_image(\"data/PennFudanPed/PedMasks/FudanPed00046_mask.png\")\n",
    "\n",
    "plt.figure(figsize=(16, 8))\n",
    "plt.subplot(121)\n",
    "plt.title(\"Image\")\n",
    "plt.imshow(image.permute(1, 2, 0))\n",
    "plt.subplot(122)\n",
    "plt.title(\"Mask\")\n",
    "plt.imshow(mask.permute(1, 2, 0))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Preparing Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Pre-processing transformations\n",
    "def get_transform(train, mean_std_norm):\n",
    "    transforms = []\n",
    "    if train:\n",
    "        transforms.append(T.RandomHorizontalFlip(p=0.5))\n",
    "        transforms.append(T.RandomVerticalFlip(p=0.5))\n",
    "        transforms.append(T.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2))\n",
    "        transforms.append(T.GaussianBlur(kernel_size=(3, 3), sigma=(0.1, 2.0)))\n",
    "        transforms.append(T.RandomPerspective(distortion_scale=0.2, p=0.5))\n",
    "        transforms.append(T.RandomGrayscale(p=0.1))\n",
    "        transforms.append(T.RandomZoomOut(fill={tv_tensors.Image: (0, 0, 0), \"others\": 0}, side_range=(1.0, 2.0), p=0.2)), #(123, 117, 104)\n",
    "    transforms.append(T.ToDtype(torch.float, scale=True))\n",
    "    if mean_std_norm:\n",
    "        transforms.append(T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]))\n",
    "    transforms.append(T.ToPureTensor())\n",
    "    return T.Compose(transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The dataset contains two classes only: background and person\n",
    "num_classes = 2\n",
    "BATCHES = 2\n",
    "\n",
    "# Use ther dataset and defined transformations\n",
    "dataset_tr = ProcessDataset(\n",
    "    root='data/PennFudanPed',\n",
    "    image_path=\"PNGImages\",\n",
    "    mask_path=\"PedMasks\",\n",
    "    transforms=get_transform(train=True, mean_std_norm=False),\n",
    "    num_classes=num_classes-1) # exclude the background\n",
    "\n",
    "dataset_ntr = ProcessDataset(\n",
    "    root='data/PennFudanPed',\n",
    "    image_path=\"PNGImages\",\n",
    "    mask_path=\"PedMasks\",\n",
    "    transforms=get_transform(train=False, mean_std_norm=False),\n",
    "    num_classes=num_classes-1) # exclude the background\n",
    "\n",
    "# Split the dataset in train and test set\n",
    "indices = torch.randperm(len(dataset_tr)).tolist()\n",
    "train_dataset = torch.utils.data.Subset(dataset_tr, indices[:-25])\n",
    "test_dataset = torch.utils.data.Subset(dataset_ntr, indices[-25:])\n",
    "test_dataset_t = torch.utils.data.Subset(dataset_tr, indices[-25:])\n",
    "\n",
    "# Define training and validation data loaders\n",
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=BATCHES,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "test_dataloader = torch.utils.data.DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=BATCHES,\n",
    "    shuffle=False,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "# Define training and validation data loaders\n",
    "test_dataloader_t = torch.utils.data.DataLoader(\n",
    "    test_dataset_t,\n",
    "    batch_size=2,\n",
    "    shuffle=False,\n",
    "    collate_fn=collate_fn\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize transformations\n",
    "for idx, ((img, target), (img_t, target_t)) in enumerate(zip(test_dataloader, test_dataloader_t)):   \n",
    "    for i in range(0, BATCHES):\n",
    "        visualize_transformed_data(img[i], target[i], img_t[i], target_t[i])\n",
    "    if idx > 4:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Creating the Object Detection Model Based on Faster R-CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FasterRCNN(\n",
    "    backbone=\"resnet50_v2\",\n",
    "    num_classes=num_classes,\n",
    "    device=device\n",
    "    )\n",
    "\n",
    "#model = torch.compile(model, backend=\"aot_eager\")\n",
    "\n",
    "#summary(model,\n",
    "#        input_size=(64, 3, 224, 224),\n",
    "#        col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
    "#        col_width=20,\n",
    "#        row_settings=[\"var_names\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Model name\n",
    "model_type=\"model\"\n",
    "model_name = model_type + \".pth\"\n",
    "EPOCHS = 30\n",
    "LR = 0.001\n",
    "\n",
    "# Create the optimizer\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.SGD(\n",
    "    params,\n",
    "    lr=LR,\n",
    "    momentum=0.9,\n",
    "    weight_decay=0.0005\n",
    ")\n",
    "\n",
    "# Create AdamW optimizer\n",
    "#optimizer = torch.optim.AdamW(\n",
    "#    params=model.parameters(),\n",
    "#    lr=LR,\n",
    "#    betas=(0.9, 0.999),\n",
    "#    weight_decay=0.01\n",
    "#)\n",
    "\n",
    "\n",
    "# Create the scheduler\n",
    "#scheduler = torch.optim.lr_scheduler.StepLR(\n",
    "#    optimizer,\n",
    "#    step_size=5,\n",
    "#    gamma=0.1\n",
    "#)\n",
    "\n",
    "scheduler = FixedLRSchedulerWrapper(\n",
    "    scheduler=CosineAnnealingLR(optimizer, T_max=25, eta_min=1e-6),\n",
    "    fixed_lr=1e-6,\n",
    "    fixed_epoch=25)\n",
    "\n",
    "# Instantiate the engine with the created model and the target device\n",
    "engine = ObjectDetectionEngine(\n",
    "    model=model,\n",
    "    device=device)\n",
    "\n",
    "# Configure the training method\n",
    "results = engine.train(\n",
    "    target_dir=MODEL_DIR,                       # Directory where the model will be saved\n",
    "    model_name=model_name,                      # Name of the model\n",
    "    save_best_model=[\"last\", \"loss\"],           # Save the best models based on different criteria\n",
    "    keep_best_models_in_memory=True,            # Do not keep the models stored in memory for the sake of training time and memory efficiency\n",
    "    train_dataloader=train_dataloader,          # Train dataloader\n",
    "    test_dataloader=test_dataloader,            # Test dataloader\n",
    "    optimizer=optimizer,                        # Optimizer    \n",
    "    scheduler=scheduler,                        # Scheduler\n",
    "    epochs=EPOCHS,                              # Total number of epochs\n",
    "    amp=True,                                   # Enable Automatic Mixed Precision (AMP)\n",
    "    enable_clipping=False,                      # Disable clipping on gradients, only useful if training becomes unestable\n",
    "    debug_mode=False,                           # Disable debug mode    \n",
    "    accumulation_steps=1,                       # Accumulation steps 2: effective batch size = batch_size x accumulation steps\n",
    "    apply_validation=True                       # Enable validation step\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10. Making Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OPTION = 1\n",
    "\n",
    "# Make predictions using the `engine` object, best model is already internally stored\n",
    "if OPTION == 1:\n",
    "    # Make predictions and plot the results\n",
    "    preds = engine.predict(\n",
    "        dataloader=test_dataloader,\n",
    "        model_state='loss', # Take the model with the lowest loss\n",
    "        prune_predictions = True,\n",
    "        #score_threshold = 0.66,\n",
    "        #mask_threshold = 0.5,    \n",
    "        #iou_threshold = 0.5\n",
    "    )\n",
    "\n",
    "# Make predictions by loading the already trained model manually\n",
    "else:\n",
    "    # Instantiate the trained model\n",
    "    # First, load the architecture\n",
    "    model = FasterRCNN(\n",
    "        backbone=\"resnet50_v2\",\n",
    "        num_classes=num_classes,\n",
    "        device=device\n",
    "        )\n",
    "\n",
    "    # Second, load the parameters of the best model\n",
    "    #model = load_model(model, \"outputs\", \"model_loss_epoch21.pth\")\n",
    "\n",
    "    # Instantiate the engine with the created model and the target device\n",
    "    engine2 = ObjectDetectionEngine(\n",
    "        model=model,\n",
    "        device=device)\n",
    "\n",
    "    # Make predictions and plot the results\n",
    "    preds = engine2.predict(\n",
    "        dataloader=test_dataloader,\n",
    "        prune_predictions = True,\n",
    "        #score_threshold = 0.66,\n",
    "        #mask_threshold = 0.5,    \n",
    "        #iou_threshold = 0.5\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration parameters\n",
    "MASK_COLOR = \"blue\"\n",
    "BOX_COLOR = \"white\"\n",
    "WIDTH = 3\n",
    "PRINT_LABELS = True\n",
    "\n",
    "# Display predictions\n",
    "diplay_predictions(\n",
    "    preds=preds,\n",
    "    dataset=test_dataset,\n",
    "    box_color=BOX_COLOR,\n",
    "    mask_color=MASK_COLOR,\n",
    "    width=WIDTH,\n",
    "    print_classes=True,\n",
    "    print_scores=True,\n",
    "    label_to_class_dict={1: 'pedestrian'}\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load an arbitrary image from a different dataset\n",
    "image = read_image(\"images/examples/000000000674.jpg\")\n",
    "\n",
    "# And make a prediction\n",
    "eval_transform = get_transform(train=False, mean_std_norm=False)\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    x = eval_transform(image)\n",
    "    # convert RGBA -> RGB and move to device\n",
    "    x = x[:3, ...].to(device)\n",
    "    predictions = model([x, ])\n",
    "    pred = prune_predictions(predictions[0])\n",
    "    \n",
    "\n",
    "# Prepare the image for plotting\n",
    "image = (255.0 * (image - image.min()) / (image.max() - image.min())).to(torch.uint8)\n",
    "image = image[:3, ...]\n",
    "pred_labels = [f\"roi: {score:.3f}\" for label, score in zip(pred[\"labels\"], pred[\"scores\"])]\n",
    "pred_boxes = pred[\"boxes\"].long()\n",
    "output_image = draw_bounding_boxes(image, pred_boxes, pred_labels, colors=\"white\", width=3)\n",
    "\n",
    "#masks = (pred[\"masks\"] > 0.7).squeeze(1)\n",
    "#output_image = draw_segmentation_masks(output_image, masks, alpha=0.5, colors=\"blue\")\n",
    "\n",
    "# Plot the image\n",
    "fig, ax = plt.subplots(figsize=(12, 12))\n",
    "plt.imshow(output_image.permute(1, 2, 0))\n",
    "ax.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv_pytorch_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
